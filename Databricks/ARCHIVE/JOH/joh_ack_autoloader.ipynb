{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cca025a-40b3-4b62-bb36-0cb685b30097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# time.sleep(300) # Await for 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9e722a5-603c-4f03-af75-ac3854aa8e45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, LongType\n",
    "from pyspark.sql.functions import col,from_json\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3da9d2fc-1216-46a5-ba5f-35ca91338044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ack_schema = StructType([\n",
    "    StructField(\"filename\", StringType(), True),\n",
    "    StructField(\"http_response\", IntegerType(),True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"http_message\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2b3b952-10b1-4ef0-b7b5-d1cfa7b10157",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## set up the configuration to allow the autoloader to connect to the source system\n",
    "\n",
    "## Eventhub details\n",
    "connection_string = dbutils.secrets.get(\"ingest00-meta002-sbox\", \"evh-joh-ack-dev-uks-dlrm-01-key\")\n",
    "consumer_group = \"$Default\"\n",
    "# Encrypt the connection string using the EventHubsUtils.encrypt method\n",
    "encrypted_conn_str = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connection_string)\n",
    "# Create a JSON string for starting positions for 10 partitions (0 through 9)\n",
    "starting_positions = {str(i): \"-1\" for i in range(10)}\n",
    "starting_positions_json = json.dumps(starting_positions)\n",
    "\n",
    "ehConf = {\n",
    "    \"eventhubs.connectionString\": encrypted_conn_str,\n",
    "    \"eventhubs.consumerGroup\": consumer_group,\n",
    "    # \"eventhubs.startingPositions\": starting_positions_json\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "eventhubdf = spark.readStream.format(\"eventhubs\")\\\n",
    "    .options(**ehConf)\\\n",
    "        .load()\n",
    "\n",
    "# eventhubdf.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa487277-ea6a-4c08-9eff-dc1a4902bc51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parsed_df = (\n",
    "    eventhubdf\n",
    "    # 'body' is binary, so we cast to string (assuming UTF-8)\n",
    "    .select(col(\"body\").cast(\"string\").alias(\"json_str\"))\n",
    "    .select(from_json(col(\"json_str\"), ack_schema).alias(\"json_obj\"))\n",
    "    .select(\"json_obj.*\")\n",
    ")\n",
    "\n",
    "\n",
    "# parsed_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8471c989-6e68-41f2-ac65-179ae0ad72e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ack_path = \"/mnt/ingest00curatedsboxsilver/ARIADM/ARM/AUDIT/JOH/\"\n",
    "\n",
    "ack_file_name = f\"{ack_path}joh_ack_audit\"\n",
    "\n",
    "parsed_df.writeStream \\\n",
    "    .format(\"delta\")\\\n",
    "        .option(\"checkpointLocation\", \"/mnt/autoLoaderSchema/ARMJOH/ACK/ack\")\\\n",
    "            .outputMode(\"append\")\\\n",
    "                .trigger(availableNow=True)\\\n",
    "                .start(ack_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81fdf9d1-12ce-4494-99de-a73fbf117b8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(ack_file_name)\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ff2696d-632b-414e-867f-a7c1b41fc85c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Notebook completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1010b179-88e3-45fb-967d-d7d69187502a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0bbc322-3c44-48a6-9908-de85bde093d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.rm(\"/mnt/autoLoaderSchema/\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8637f3bf-1ea0-445f-9739-21256ba4d02a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# # Initialize Spark session (if needed)\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# # Define schema\n",
    "# schema = StructType([\n",
    "#     StructField(\"EmployeeID\", IntegerType(), True),\n",
    "#     StructField(\"Name\", StringType(), True),\n",
    "#     StructField(\"Department\", StringType(), True),\n",
    "#     StructField(\"Salary\", IntegerType(), True)\n",
    "# ])\n",
    "\n",
    "# # Create dummy data\n",
    "# data = [\n",
    "#     (101, \"Alice\", \"Engineering\", 80000),\n",
    "#     (102, \"Bob\", \"HR\", 50000),\n",
    "#     (103, \"Charlie\", \"Finance\", 60000),\n",
    "#     (104, \"Diana\", \"Marketing\", 70000)\n",
    "# ]\n",
    "\n",
    "# # Create DataFrame\n",
    "# df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# # Show the DataFrame\n",
    "# df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18ed5881-62b5-47c7-a699-27a85274d711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# file_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/testtable\"\n",
    "# file_path"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "joh_ack_autoloader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
