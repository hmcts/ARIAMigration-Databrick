{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca145479-3655-4bf2-bafa-8c2894258a3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ARM Acknowledgment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81ef4be3-3c60-450c-91c7-9d09d193aea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Autoloader set up**  \n",
    "This Notebook sets up an Autoloader job that runs on a manual trigger to collect ack messages from the ack eventhubs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4cd7c49-4cda-4468-adb3-e41e68b1ddee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "#time.sleep(300) # Await for 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ab6f686-9bdf-4e7c-8537-52a7097d6569",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, LongType\n",
    "from pyspark.sql.functions import col,from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c41be175-1d89-4a6f-9135-82e7f162705e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ack_schema = StructType([\n",
    "    StructField(\"filename\", StringType(), True),\n",
    "    StructField(\"http_response\", IntegerType(),True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"http_message\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a02a91d2-82a2-4d79-9022-2a988caa9b74",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Extract Environment Details and Generate KeyVault Name"
    }
   },
   "outputs": [],
   "source": [
    "config = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/configs/config.json\")\n",
    "env_name = config.first()[\"env\"].strip().lower()\n",
    "lz_key = config.first()[\"lz_key\"].strip().lower()\n",
    "\n",
    "print(f\"env_code: {lz_key}\")  # This won't be redacted\n",
    "print(f\"env_name: {env_name}\")  # This won't be redacted\n",
    "\n",
    "KeyVault_name = f\"ingest{lz_key}-meta002-{env_name}\"\n",
    "print(f\"KeyVault_name: {KeyVault_name}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "792f37ec-d68c-4dfe-a30c-6611057f56d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "EH_NAMESPACE = f\"ingest{lz_key}-integration-eventHubNamespace001-{env_name}\"\n",
    "EH_NAME = f\"evh-aplfta-ack-{lz_key}-uks-dlrm-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f759f72c-951e-4048-a5d5-a5bcaa34fe3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## set up the configuration to allow the autoloader to connect to the source system\n",
    "\n",
    "## Eventhub details\n",
    "# connection_string = dbutils.secrets.get(\"ingest00-meta002-sbox\", \"evh-joh-ack-dev-uks-dlrm-01-key\")\n",
    "connection_string = dbutils.secrets.get(KeyVault_name, \"RootManageSharedAccessKey\")\n",
    "# Encrypt the connection string using the EventHubsUtils.encrypt method\n",
    "#encrypted_conn_str = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connection_string)\n",
    "\n",
    "\n",
    "# eventhubdf = spark.readStream.format(\"eventhubs\")\\\n",
    "#     .options(**ehConf)\\\n",
    "#         .load()\n",
    "\n",
    "# eventhubdf.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a59192cd-e151-4c3e-a03e-0556d408a42e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "KAFKA_OPTIONS = {\n",
    "    \"kafka.bootstrap.servers\": f\"{EH_NAMESPACE}.servicebus.windows.net:9093\",\n",
    "    \"subscribe\": EH_NAME,\n",
    "    \"startingOffsets\": \"earliest\",\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"failOnDataLoss\": \"false\",\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.sasl.jaas.config\": f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{connection_string}\";'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98a499af-0f3c-4d7f-aab2-a7f52382d045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Service principal credentials\n",
    "client_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-ID\")\n",
    "client_secret = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-SECRET\")\n",
    "tenant_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-TENANT-ID\")\n",
    "\n",
    "# Storage account names\n",
    "curated_storage = f\"ingest{lz_key}curated{env_name}\"\n",
    "checkpoint_storage = f\"ingest{lz_key}xcutting{env_name}\"\n",
    "\n",
    "# Spark config for curated storage (Delta table)\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{curated_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{curated_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{curated_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{curated_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{curated_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{checkpoint_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{checkpoint_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{checkpoint_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{checkpoint_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{checkpoint_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96315c6f-9074-4afd-ba12-5c7026a03f9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Container and path for storing Delta table (in curated storage account)\n",
    "data_path = f\"abfss://silver@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ARM/AUDIT/APPEALS/ARIAFTA/fta_ack_audit\"\n",
    "\n",
    "# Container and path for checkpoint (in xcuttings storage account)\n",
    "checkpoint_path = f\"abfss://db-ack-checkpoint@ingest{lz_key}xcutting{env_name}.dfs.core.windows.net/APPEALS/ARIAFTA/ACK/ack\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "922598c2-1858-4201-8f85-0a9fc30051c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eventhubdf = spark.readStream.format(\"kafka\")\\\n",
    "    .options(**KAFKA_OPTIONS)\\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b4875ca-6c32-476e-935c-4b52b6dbfc4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parsed_df = (\n",
    "    eventhubdf\n",
    "    # 'body' is binary, so we cast to string (assuming UTF-8)\n",
    "    .select(col(\"value\").cast(\"string\").alias(\"json_str\"))\n",
    "    .select(from_json(col(\"json_str\"), ack_schema).alias(\"json_obj\"))\n",
    "    .select(\"json_obj.*\")\n",
    ")\n",
    "\n",
    "\n",
    "# parsed_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb11c310-00a7-457a-a93e-af392c2a5c8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "query = parsed_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start(data_path)\n",
    "\n",
    "time.sleep(30)\n",
    "\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19dfa679-816a-47c5-8666-2ff9c192d488",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(data_path)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3524853-c6ae-4ff5-9958-31f5f4ada5a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Notebook completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6a1d49c-b75b-40a0-8dc8-2bab330533b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af26c404-8b9b-4e26-9d60-d391668fde54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.ls(\"/mnt/ingest00curatedsboxsilver/ARIADM/ARM/AUDIT/APPEALS/ARIAFTA\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f25fe43-5d2d-4e41-9f56-0e3ddfc78d75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbutils.fs.ls(\"/mnt/autoLoaderSchema/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a65df38-d3fb-4624-8237-722f68462c00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.rm(\"/mnt/autoLoaderSchema/APPEALS/\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a93fc0cb-53a0-430f-b7e9-9b78b7539d99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbutils.fs.ls(\"/mnt/dropzoneariafta/ARIAFTA/submission/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee456a9a-e9e4-4739-bceb-b525c75ea6d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, from_unixtime\n",
    "\n",
    "# files_df = spark.createDataFrame(dbutils.fs.ls(\"/mnt/dropzoneariafta/ARIAFTA/submission/\"))\n",
    "# files_df = files_df.withColumn(\"modificationTime\", from_unixtime(col(\"modificationTime\") / 1000).cast(\"timestamp\"))\n",
    "\n",
    "# display(files_df.orderBy(col(\"modificationTime\").desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b45c229-8dd3-4f75-8ec0-d091bb4138ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, from_unixtime\n",
    "\n",
    "# files_df = spark.createDataFrame(dbutils.fs.ls(\"/mnt/dropzoneariafta/ARIAFTA/response/\"))\n",
    "# files_df = files_df.withColumn(\"modificationTime\", from_unixtime(col(\"modificationTime\") / 1000).cast(\"timestamp\"))\n",
    "\n",
    "# display(files_df.orderBy(col(\"modificationTime\").desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1474670-b974-4514-bd26-6128cf089dd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# files_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc593029-fbd2-4731-ad42-3b91891d034b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, from_unixtime\n",
    "\n",
    "# files_df = spark.createDataFrame(dbutils.fs.ls(\"/mnt/dropzoneariafta/ARIAFTA/response/\"))\n",
    "# files_df = files_df.withColumn(\"modificationTime\", from_unixtime(col(\"modificationTime\") / 1000).cast(\"timestamp\"))\n",
    "# files_df = files_df.filter(col(\"path\").contains(\"_0_\"))\n",
    "\n",
    "# display(files_df.orderBy(col(\"modificationTime\").desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0836283a-54b9-4160-a2b4-6712f780c80e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# https://ingest00curatedsbox.blob.core.windows.net/silver/ARIADM/ARM/AUDIT/APPEALS/ARIAFTA/fta_ack_audit/part-00000-2c1cd471-0147-4905-b09b-ead0278c9552-c000.snappy.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2979be7-057d-4101-b5b3-88b9d5db14ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Define the paths to the tables\n",
    "# audit_delta_path = \"/mnt/ingest00curatedsboxsilver/ARIADM/ARM/AUDIT/APPEALS/ARIAFTA/apl_fta_cr_audit_table\"\n",
    "# ack_path = \"/mnt/ingest00curatedsboxsilver/ARIADM/ARM/AUDIT/APPEALS/ARIAFTA/fta_ack_audit\"\n",
    "# output_subdir_input_upload = \"/mnt/ingest00curatedsboxsilverARIADM/ARM/AUDIT/APPEALS/ARIAFTA/input_upload\"\n",
    "# output_subdir_create_record_upload = \"/mnt/ingest00curatedsboxsilverARIADM/ARM/AUDIT/APPEALS/ARIAFTA/create_record\"\n",
    "# output_subdir_upload_file_upload = \"/mnt/ingest00curatedsboxsilverARIADM/ARM/AUDIT/APPEALS/ARIAFTA/upload_file\"\n",
    "# output_subdir_amalgamated_responses = \"/mnt/ingest00curatedsboxsilverARIADM/ARM/AUDIT/APPEALS/ARIAFTA/amalgamated_responses\"\n",
    "\n",
    "# # Read the Delta table for joh_cr_audit_table\n",
    "# df_audit = spark.read.format(\"delta\").load(audit_delta_path)\n",
    "\n",
    "# # Read the Delta table for joh_ack_audit\n",
    "# df_ack = spark.read.format(\"delta\").load(ack_path)\n",
    "\n",
    "# # Read the response data for input_upload\n",
    "# # df_input_upload = spark.read.format(\"parquet\").load(output_subdir_input_upload)\n",
    "\n",
    "# # # Read the response data for create_record_upload\n",
    "# # df_create_record_upload = spark.read.format(\"parquet\").load(output_subdir_create_record_upload)\n",
    "\n",
    "# # # Read the response data for upload_file\n",
    "# # df_upload_file_upload = spark.read.format(\"parquet\").load(output_subdir_upload_file_upload)\n",
    "\n",
    "\n",
    "# # Read the response data for df_amalgamated_responses\n",
    "# # df_amalgamated_responses = spark.read.format(\"delta\").load(output_subdir_amalgamated_responses)\n",
    "\n",
    "# # Display the DataFrames using Databricks display\n",
    "# display(df_audit)\n",
    "# display(df_ack)\n",
    "# # display(df_input_upload)\n",
    "# # display(df_create_record_upload)\n",
    "# # display(df_upload_file_upload)\n",
    "# # display(df_amalgamated_responses)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ARIADM_ACK_AUTOLOADER_FTA",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
