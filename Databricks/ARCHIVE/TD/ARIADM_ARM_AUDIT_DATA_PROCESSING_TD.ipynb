{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2522e663-16c8-479b-a5d0-cde07c2e4358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## TD Audit Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cbb73ff-ea97-4d52-8cb2-bef1d6a7c843",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import json\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "from pyspark.sql.window import Window\n",
    "import uuid\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c454989a-3fa2-44e9-b0ff-6aa6e1dbdd10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/configs/config.json\")\n",
    "env_name = config.first()[\"env\"].strip().lower()\n",
    "lz_key = config.first()[\"lz_key\"].strip().lower()\n",
    "\n",
    "print(f\"env_code: {lz_key}\")  # This won't be redacted\n",
    "print(f\"env_name: {env_name}\")  # This won't be redacted\n",
    "\n",
    "KeyVault_name = f\"ingest{lz_key}-meta002-{env_name}\"\n",
    "print(f\"KeyVault_name: {KeyVault_name}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee245ea1-7a1f-42e9-b9c1-3fa11d287871",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configure SP OAuth"
    }
   },
   "outputs": [],
   "source": [
    "# Service principal credentials\n",
    "client_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-ID\")\n",
    "client_secret = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-SECRET\")\n",
    "tenant_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-TENANT-ID\")\n",
    "\n",
    "# Storage account names\n",
    "curated_storage = f\"ingest{lz_key}curated{env_name}\"\n",
    "\n",
    "# Spark config for curated storage (Delta table)\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{curated_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{curated_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{curated_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{curated_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{curated_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0d2f113-790d-4706-80c6-7b161ab5a259",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "audit_delta_path = f\"abfss://silver@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ARM/AUDIT/TD/td_cr_audit_table\"\n",
    "hive_schema = \"ariadm_arm_td\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21d6bc13-19f1-4d4f-93b1-3420aa291f45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def datetime_uuid():\n",
    "    dt_str = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return str(uuid.uuid5(uuid.NAMESPACE_DNS, dt_str))\n",
    "\n",
    "run_id_value = datetime_uuid()\n",
    "\n",
    "audit_schema = StructType([\n",
    "    StructField(\"Run_Id\", StringType(), True),\n",
    "    StructField(\"Unique_Identifier_Desc\", StringType(), True),\n",
    "    StructField(\"Unique_Identifier\", StringType(), True),\n",
    "    StructField(\"Table_Name\", StringType(), True),\n",
    "    StructField(\"Stage_Name\", StringType(), True),\n",
    "    StructField(\"Record_Count\", IntegerType(), True),\n",
    "    StructField(\"Run_DateTime\", TimestampType(), True),\n",
    "    StructField(\"Batch_Id\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"File_name\", StringType(), True),\n",
    "    StructField(\"Status\", StringType(), True)\n",
    "])\n",
    "\n",
    "def create_audit_df(df: DataFrame, Unique_Identifier_Desc: str,Table_Name: str, Stage_Name: str, Description: str, file_name = False,status = False) -> None:\n",
    "    \"\"\"\n",
    "    Creates an audit DataFrame and writes it to Delta format.\n",
    "\n",
    "    :param df: Input DataFrame from which unique identifiers are extracted.\n",
    "    :param Unique_Identifier_Desc: Column name that acts as a unique identifier.\n",
    "    :param Table_Name: Name of the source table.\n",
    "    :param Stage_Name: Name of the data processing stage.\n",
    "    :param Description: Description of the table.\n",
    "    :param additional_columns: options File_name or Status. List of additional columns to include in the audit DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    dt_desc = datetime.utcnow()\n",
    "\n",
    "    additional_columns = []\n",
    "    if file_name is True:\n",
    "        additional_columns.append(\"File_name\")\n",
    "    if status is True:\n",
    "        additional_columns.append(\"Status\")\n",
    "\n",
    "\n",
    "     # Default to an empty list if None   \n",
    "    additional_columns = [col(c) for c in additional_columns if c is not None]  # Filter out None values\n",
    "\n",
    "    audit_df = df.select(col(Unique_Identifier_Desc).alias(\"Unique_Identifier\"),col(\"Batch_Id\"),*additional_columns)\\\n",
    "    .withColumn(\"Run_Id\", lit(run_id_value))\\\n",
    "        .withColumn(\"Unique_Identifier_Desc\", lit(Unique_Identifier_Desc))\\\n",
    "            .withColumn(\"Stage_Name\", lit(Stage_Name))\\\n",
    "                .withColumn(\"Table_Name\", lit(Table_Name))\\\n",
    "                    .withColumn(\"Run_DateTime\", lit(dt_desc).cast(TimestampType()))\\\n",
    "                        .withColumn(\"Description\", lit(Description))\n",
    "\n",
    "    list_cols = audit_df.columns\n",
    "\n",
    "    final_audit_df = audit_df.groupBy(*list_cols).agg(count(\"*\").cast(IntegerType()).alias(\"Record_Count\"))\n",
    "\n",
    "    # final_audit_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\",\"true\").save(audit_delta_path)\n",
    "    \n",
    "    return final_audit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf4da11e-d865-4a22-8ec7-ae1866b46213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Delta Table Path in Azure Storage\n",
    "\n",
    "\n",
    "if not DeltaTable.isDeltaTable(spark, audit_delta_path):\n",
    "    print(f\"ðŸ›‘ Delta table '{audit_delta_path}' does not exist. Creating an empty Delta table...\")\n",
    "\n",
    "    # Create an empty DataFrame\n",
    "    empty_df = spark.createDataFrame([], audit_schema)\n",
    "\n",
    "    # Write the empty DataFrame in Delta format to create the table\n",
    "    empty_df.write.format(\"delta\").mode(\"overwrite\").save(audit_delta_path)\n",
    "\n",
    "    print(\"âœ… Empty Delta table successfully created in Azure Storage.\")\n",
    "else:\n",
    "    print(f\"âš¡ Delta table '{audit_delta_path}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "484ab2a6-69b1-43bd-b6f1-3a0effd732c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "audit_params = [\n",
    "        {\n",
    "        \"Unique_Identifier_cols\": [\"CaseNo\", \"Forenames\", \"Name\"],\n",
    "        \"Table_Name\": \"bronze_ac_ca_ant_fl_dt_hc\",\n",
    "        \"Stage_Name\": \"bronze_stage\",\n",
    "        \"Description\": \"The bronze_ac_ca_ant_fl_dt_hc table Delta Live Table combining Appeal Case data with Case Appellant, Appellant, File Location, Department, and Hearing Centre.\"\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"CaseNo\", \"Forenames\", \"Name\"],\n",
    "        \"Table_Name\": \"bronze_iris_extract\",\n",
    "        \"Stage_Name\": \"bronze_stage\",\n",
    "        \"Description\": \"Delta Live Table extracted from the IRIS Tribunal decision file extract.\"\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"CaseNo\"],\n",
    "        \"Table_Name\": \"stg_td_filtered\",\n",
    "        \"Stage_Name\": \"segmentation_stage\",\n",
    "        \"Description\": \"The stg_td_filtered - segmentation Table for appeal cases requiring tribunal decisions with unique list of CaseNo's\"\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"CaseNo\", \"Forenames\", \"Name\"],\n",
    "        \"Table_Name\": \"silver_tribunaldecision_detail\",\n",
    "        \"Stage_Name\": \"silver_stage\",\n",
    "        \"Description\": \"The silver_tribunaldecision_detail - or Tribunal Decision information\"\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"client_identifier\", \"bf_002\", \"bf_003\"],\n",
    "        \"Table_Name\": \"silver_archive_metadata\",\n",
    "        \"Stage_Name\": \"silver_stage\",\n",
    "        \"Description\": \"The silver_archive_metadata table consolidates keys metadata for Archive Metadata da\"\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"CaseNo\", \"Forenames\", \"Name\"],\n",
    "        \"Table_Name\": \"stg_create_td_iris_json_content\",\n",
    "        \"Stage_Name\": \"silver_stage\",\n",
    "        \"Description\": \"The stg_create_td_iris_json_content table generates JSON content for TD cases\",\n",
    "        \"extra_columns\": [\"File_name\", \"Status\"]\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"CaseNo\", \"Forenames\", \"Name\"],\n",
    "        \"Table_Name\": \"stg_create_td_iris_html_content\",\n",
    "        \"Stage_Name\": \"silver_stage\",\n",
    "        \"Description\": \"The stg_create_td_iris_html_content table generates HTML content for TD cases\",\n",
    "        \"extra_columns\": [\"File_name\", \"Status\"]\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"client_identifier\", \"bf_002\", \"bf_003\"],\n",
    "        \"Table_Name\": \"stg_create_td_iris_a360_content\",\n",
    "        \"Stage_Name\": \"silver_stage\",\n",
    "        \"Description\": \"The stg_create_td_iris_a360_content table generates A360 content for TD cases\",\n",
    "        \"extra_columns\": [ \"Status\"]\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"CaseNo\", \"Forenames\", \"Name\"],\n",
    "        \"Table_Name\": \"stg_td_iris_unified\",\n",
    "        \"Stage_Name\": \"silver_stage\",\n",
    "        \"Description\": \"The stg_td_iris_unified table generates A360 BatchId for TD cases\",\n",
    "        \"extra_columns\": [\"File_name\", \"Status\"]\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"CaseNo\", \"Forenames\", \"Name\"],\n",
    "        \"Table_Name\": \"gold_td_iris_with_html\",\n",
    "        \"Stage_Name\": \"silver_stage\",\n",
    "        \"Description\": \"The gold_td_iris_with_html with HTML Outputs Uploded..\",\n",
    "        \"extra_columns\": [\"File_name\", \"Status\"]\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"CaseNo\", \"Forenames\", \"Name\"],\n",
    "        \"Table_Name\": \"gold_td_iris_with_json\",\n",
    "        \"Stage_Name\": \"silver_stage\",\n",
    "        \"Description\": \"The gold_td_iris_with_json with HTML Outputs Uploded..\",\n",
    "        \"extra_columns\": [\"File_name\", \"Status\"]\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"A360_BatchId\"],\n",
    "        \"Table_Name\": \"gold_td_iris_with_a360\",\n",
    "        \"Stage_Name\": \"silver_stage\",\n",
    "        \"Description\": \"The gold_td_iris_with_a360 with HTML Outputs Uploded..\",\n",
    "        \"extra_columns\": [\"File_name\", \"Status\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "audit_dataframes = []\n",
    "\n",
    "for params in audit_params:\n",
    "    Table_Name = params[\"Table_Name\"]\n",
    "    Stage_Name = params[\"Stage_Name\"]\n",
    "    Unique_Identifier_cols = params[\"Unique_Identifier_cols\"]\n",
    "    Description = params[\"Description\"]\n",
    "    extra_columns = params[\"extra_columns\"] if \"extra_columns\" in params else []\n",
    "    Unique_Identifier_Desc = \"_\".join(Unique_Identifier_cols)\n",
    "\n",
    "    try:\n",
    "\n",
    "        df_logging = spark.read.table(f\"hive_metastore.{hive_schema}.{Table_Name}\")\n",
    "\n",
    "        df_audit = df_logging\n",
    "        if len(Unique_Identifier_cols) > 1:\n",
    "            df_audit = df_audit.withColumn(\n",
    "                Unique_Identifier_Desc, \n",
    "                concat_ws(\"_\", *[col(c).cast(\"string\") for c in Unique_Identifier_cols])\n",
    "            )\n",
    "        else:\n",
    "            df_audit = df_audit.withColumn(Unique_Identifier_Desc, col(Unique_Identifier_Desc).cast(\"string\"))\n",
    "\n",
    "        # Add Batch_Id from A360_BatchId if present, else set to None\n",
    "        if \"A360_BatchId\" in df_audit.columns:\n",
    "            df_audit = df_audit.withColumn(\"Batch_Id\", col(\"A360_BatchId\").cast(\"string\"))\n",
    "        else:\n",
    "            df_audit = df_audit.withColumn(\"Batch_Id\", lit(None).cast(\"string\"))\n",
    "\n",
    "        # Apply extra column mappings dynamically\n",
    "        if len(extra_columns) <= 1:\n",
    "            missing_columns = list(set([\"File_name\", \"Status\"]) - set(extra_columns))\n",
    "            for new_col in missing_columns:\n",
    "                df_audit = df_audit.withColumn(new_col, lit(None))\n",
    "\n",
    "        # Generate the audit DataFrame\n",
    "        df_audit_appended = create_audit_df(\n",
    "            df_audit,\n",
    "            Unique_Identifier_Desc=Unique_Identifier_Desc,\n",
    "            Table_Name=Table_Name,\n",
    "            Stage_Name=Stage_Name,\n",
    "            Description=Description,\n",
    "            file_name = True,\n",
    "            status = True\n",
    "        )\n",
    "\n",
    "        audit_dataframes.append(df_audit_appended)\n",
    "\n",
    "        print(f\"âœ… Successfully processed table: {Table_Name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ðŸ›‘ Failed to process table: {Table_Name}. Error: {str(e)}\")\n",
    "        failed_table = f\"Table {Table_Name} does not exist\"\n",
    "\n",
    "        # Table does not exist, create an audit entry for it\n",
    "        status = f\"Failed - Table {Table_Name} does not exist\"\n",
    "\n",
    "        row_data = {\n",
    "            \"Run_Id\": run_id_value,\n",
    "            \"Unique_Identifier_Desc\": Unique_Identifier_Desc,\n",
    "            \"Unique_Identifier\": None,\n",
    "            \"Table_Name\": Table_Name,\n",
    "            \"Stage_Name\": Stage_Name,\n",
    "            \"Record_Count\": 0,\n",
    "            \"Run_DateTime\": datetime.now(),\n",
    "            \"Batch_Id\": None,\n",
    "            \"Description\": Description,\n",
    "            \"File_name\": None,\n",
    "            \"Status\": status\n",
    "        }\n",
    "\n",
    "        row_df = spark.createDataFrame([row_data], schema=audit_schema)\n",
    "        audit_dataframes.append(row_df)\n",
    "\n",
    "\n",
    "df_final_audit = audit_dataframes[0]\n",
    "for df in audit_dataframes[1:]:\n",
    "    df_final_audit = df_final_audit.unionByName(df, allowMissingColumns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb545dd2-e117-4f63-b601-44f8cacc2b2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final_audit.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(audit_delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "422614f9-4665-4ad5-bb0a-81ef6b9a7a00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Notebook completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf76e704-f4ce-4b95-82fa-4c2f5fe05d67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84bc5377-29b8-458f-92bc-a192584354f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_final_audit = spark.read.format(\"delta\").load(audit_delta_path)\n",
    "# df_final_audit.createOrReplaceTempView(\"tv_final_audit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b6e79c5-d478-4575-9ee5-bd0d30e00c3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# select * from tv_final_audit\n",
    "# where Batch_Id is not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87088fa0-0836-4dd0-b436-a3e70935a1d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_final_audit.createOrReplaceTempView(\"tv_final_audit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a18c51b-01b5-4a10-ada8-d05f052be9ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# select * from tv_final_audit\n",
    "# where Table_Name like 'gold%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "364142bc-ff79-451d-bdf8-9026930cbd24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.read.format(\"delta\").load(audit_delta_path).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b60dea34-6b83-4a78-9e4f-4843eee5b7b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.read.format(\"delta\").load(audit_delta_path).filter(\"Batch_Id IS NOT NULL\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8774871110227082,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ARIADM_ARM_AUDIT_DATA_PROCESSING_TD",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
