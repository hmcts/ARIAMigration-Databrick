{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f7325bca-a910-4d9e-928d-14b3b52a21b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "########################\n",
    "#Install Dependancies\n",
    "########################\n",
    "#Install openpyxl so can export as excel (only required to once for your computer)\n",
    "# %pip install openpyxl\n",
    "# %pip install pycountry\n",
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89aea2bd-a89c-4b62-b5fb-137dbea3ae83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "####INSTRUCTIONS FOR CLONING FOR NEW STATE\n",
    "\n",
    "#1) Change in Cell 3 - Change this_notebook_state to value for State - this should be the name of the folder wheer the gold data is stored\n",
    "#2) Change in Cell 3 - Change The import Statement >>> import Test_Functions.AppealSubmitted_Tests as as_tests >> to the function\n",
    "#  file containing the test code for this new state and a new name for the import to be used\n",
    "#3) Change in Cell 3 - Change child_fields_to_exclude, if you need to override the testing for a field tested in an earlier state\n",
    "#4) Change in Cell 5 - The Child Notebook you wish to Call.. Usualy previous as they are based on last one\n",
    "#5) Change in Cell 6 - Change Import line >>> importlib.reload(as_tests) to the name you give in step 2\n",
    "#6) Start Creating Tests in cell 7 (demo tests) , alwaysd inserting new cells for each field group but ensuring that they are always before the last cell (currently cell 8) which produces the test report / return data back to parent notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "514678b6-5dec-419e-b19e-347679064239",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "########################\n",
    "#Setup for Tranformation Tests\n",
    "########################\n",
    "\n",
    "#####################\n",
    "#Define This notebook name\n",
    "this_notebook_state = \"appealSubmitted\"\n",
    "\n",
    "#Setup Test Reporting\n",
    "from datetime import datetime, timedelta\n",
    "run_user = spark.sql(\"SELECT current_user()\").first()[0]\n",
    "run_tag = \"Testing Transformation Tests\"\n",
    "run_by_automation_name = \"Transformation_Tests\"\n",
    "#Capture Test Start datetime\n",
    "run_start_datetime = datetime.now()\n",
    "#####################\n",
    "\n",
    "#Setup Notebook Parameters (Defaulting to Payment Pending and Running all tests)\n",
    "dbutils.widgets.text(\"state_under_test\", \"\")\n",
    "\n",
    "#fields_to_exclude : Should be a comma separated list of fields to exclude \n",
    "dbutils.widgets.text(\"fields_to_exclude\", \"\")\n",
    "\n",
    "# Read parameters\n",
    "state_under_test = dbutils.widgets.get(\"state_under_test\")\n",
    "fields_to_exclude = dbutils.widgets.get(\"fields_to_exclude\")\n",
    "\n",
    "#Set Default Values if not called from another Notebook\n",
    "if state_under_test == \"\" :    \n",
    "    state_under_test = this_notebook_state\n",
    "\n",
    "#Get Fields to Exclude\n",
    "if fields_to_exclude != \"\":    \n",
    "    fields_to_exclude = [item.strip() for item in fields_to_exclude.split(\",\")]\n",
    "else:\n",
    "    fields_to_exclude = []\n",
    "    \n",
    "#List of Fields to Exclude in Child Notebooks Called\n",
    "child_fields_to_exclude = [\"feeWithHearing\"]\n",
    "#Combine any Fields to Exclude\n",
    "fields_to_exclude.extend(child_fields_to_exclude)\n",
    "\n",
    "print(f\"Fields to exclude = {str(fields_to_exclude)}\")\n",
    "print(f\"Testing State = {state_under_test}\")\n",
    "\n",
    "#Restart Python When needed (when changed tests)\n",
    "# dbutils.library.restartPython()\n",
    "\n",
    "#Import Tests\n",
    "import Test_Functions.AppealSubmitted_Tests as as_tests\n",
    "\n",
    "#Import models.test_result as test_result\n",
    "from models.test_result import TestResult\n",
    "# TestResult.test_from_state = \"appealSubmitted\"\n",
    "\n",
    "#Import asdict to convert Dataclass to Dictionary\n",
    "from dataclasses import asdict\n",
    "import os\n",
    "\n",
    "#Setup Global Variables\n",
    "all_test_results = []\n",
    "current_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "base_path = current_path.rsplit(\"/\", 1)[0] + \"/\"\n",
    "#Below will be replaced eventually to store the reults in a spark table\n",
    "test_results_path= \"/Workspace/Users/\" + run_user + \"/Results/Transformation_Tests\"\n",
    "os.makedirs(test_results_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8330a47a-0d5b-400e-aa68-f3887d3c8de5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Load Config and Setup Enviorment Variables\n",
    "config = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/configs/config.json\")\n",
    "env_name = config.first()[\"env\"].strip().lower()\n",
    "lz_key = config.first()[\"lz_key\"].strip().lower()\n",
    " \n",
    "# print(f\"env_code: {lz_key}\")  # This won't be redacted\n",
    "# print(f\"env_name: {env_name}\")  # This won't be redacted\n",
    " \n",
    "KeyVault_name = f\"ingest{lz_key}-meta002-{env_name}\"\n",
    "# print(f\"KeyVault_name: {KeyVault_name}\")\n",
    " \n",
    "# Service principal credentials\n",
    "client_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-ID\")\n",
    "client_secret = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-SECRET\")\n",
    "tenant_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-TENANT-ID\")\n",
    " \n",
    "# Storage account names\n",
    "curated_storage = f\"ingest{lz_key}curated{env_name}\"\n",
    "checkpoint_storage = f\"ingest{lz_key}xcutting{env_name}\"\n",
    "raw_storage = f\"ingest{lz_key}raw{env_name}\"\n",
    "landing_storage = f\"ingest{lz_key}landing{env_name}\"\n",
    "external_storage = f\"ingest{lz_key}external{env_name}\"\n",
    "  \n",
    "# Spark config for curated storage (Delta table)\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{curated_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{curated_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{curated_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{curated_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{curated_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{checkpoint_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{checkpoint_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{checkpoint_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{checkpoint_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{checkpoint_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{raw_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{raw_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{raw_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{raw_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{raw_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{landing_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{landing_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{landing_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{landing_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{landing_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{external_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{external_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{external_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{external_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{external_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "  \n",
    "# Setting variables for use in subsequent cells\n",
    "bronze_path = f\"abfss://bronze@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/\"\n",
    "silver_path = f\"abfss://silver@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/\"\n",
    "audit_path = f\"abfss://silver@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/AUDIT/{state_under_test}\"\n",
    "gold_path = f\"abfss://gold@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/{state_under_test}\"\n",
    " \n",
    " \n",
    "# Print all variables\n",
    "# variables = {\n",
    "#     # \"read_hive\": read_hive,\n",
    "    \n",
    "#     \"bronze_path\": bronze_path,\n",
    "#     \"silver_path\": silver_path,\n",
    "#     \"audit_path\": audit_path,\n",
    "#     \"gold_path\": gold_path,\n",
    "#     \"key_vault\": KeyVault_name,\n",
    "#     \"AppealState\": state_under_test\n",
    " \n",
    "# }\n",
    " \n",
    "# display(variables)\n",
    "\n",
    "import json\n",
    "\n",
    "#Get Latest Json Folder\n",
    "json_location = dbutils.fs.ls(f\"{gold_path}/\")[-1]\n",
    "latest_json_location = json_location.name\n",
    "dbutils.fs.ls(f\"{gold_path}/{latest_json_location}\")\n",
    "\n",
    "#Set Paths\n",
    "try: \n",
    "    json_path = f\"{gold_path}/{latest_json_location}/JSON/\"\n",
    "    # json_path = f\"{gold_path}/{latest_json_location}/INVALID_JSON/\"\n",
    "    M1_silver = f\"{silver_path}/silver_appealcase_detail\"\n",
    "    M1_bronze = f\"{bronze_path}/bronze_appealcase_crep_rep_floc_cspon_cfs\"\n",
    "    M2_silver = f\"{silver_path}/silver_caseapplicant_detail\"\n",
    "    M3_silver = f\"{silver_path}/silver_status_detail\"\n",
    "    C = f\"{silver_path}/silver_appealcategory_detail\"\n",
    "    bhc = f\"{bronze_path}/bronze_hearing_centres\"    \n",
    "except:\n",
    "    print(f\"Error during fetch: {str(e)}\")\n",
    "\n",
    "#Create and Load Dataframes\n",
    "json_data = spark.read.format(\"json\").load(json_path)\n",
    "M1_silver = spark.read.format(\"delta\").load(M1_silver)\n",
    "M1_bronze = spark.read.format(\"delta\").load(M1_bronze)\n",
    "M2_silver = spark.read.format(\"delta\").load(M2_silver)\n",
    "M3_silver = spark.read.format(\"delta\").load(M3_silver)\n",
    "C = spark.read.format(\"delta\").load(C)\n",
    "bhc = spark.read.format(\"delta\").load(bhc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a714c4f-7c79-4aba-9b8c-658d74668986",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    #Call Parent Tests - with field exclusion comma separated list\n",
    "    if len(fields_to_exclude) !=  0:\n",
    "        str_fields_to_exclude = \",\".join(fields_to_exclude)\n",
    "    else:\n",
    "        str_fields_to_exclude = \"\"\n",
    "    \n",
    "    child_notebook_to_run = \"PaymentPending_TESTS\"\n",
    "    pp_results = dbutils.notebook.run(child_notebook_to_run, timeout_seconds=1800, arguments={\n",
    "        \"fields_to_exclude\": str_fields_to_exclude,\n",
    "        \"state_under_test\": state_under_test\n",
    "    })\n",
    "\n",
    "    from dataclasses import dataclass\n",
    "    @dataclass\n",
    "    class TestResult:\n",
    "        test_field: str =\"\"\n",
    "        status: str =\"\"\n",
    "        message: str = \"\"\n",
    "        test_from_state: str=\"\"\n",
    "        test_name: str=\"\"    \n",
    "\n",
    "    #Convert string of results back into list of TestResults class objects\n",
    "    all_test_results = [TestResult(**d) for d in json.loads(pp_results)]\n",
    "    print (all_test_results)\n",
    "\n",
    "except Exception as e:\n",
    "    message =  f\"Notebook Execution Failed for: {child_notebook_to_run} -- execution failed with Message : {str(e)}\\nFurther Execution Stopped\"    \n",
    "    dbutils.notebook.exit(message)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88e3ff3b-f1f5-4019-b763-ecef752af30a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Used to Quickly reload the function file containing the test, rather than restart python\n",
    "import importlib\n",
    "importlib.reload(as_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ec4ac1b-82a5-4c7d-aa71-5cb8963c49ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#DUMMY TEST CASEs\n",
    "if \"fakeappealsubmitted_test1\" not in fields_to_exclude:\n",
    "    all_test_results.append(as_tests.testcase1())\n",
    "    print(all_test_results)\n",
    "\n",
    "if \"fakeappealsubmitted_test2\" not in fields_to_exclude:\n",
    "    all_test_results.append(as_tests.testcase2())\n",
    "    print(all_test_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cce2e6b-9eb0-4747-88d3-2bb771b1ec9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "#PROCESS RESULTS\n",
    "#######################\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "#Convert Results Classes to string\n",
    "all_test_results_string = json.dumps([asdict(r) for r in all_test_results])\n",
    "lines = []\n",
    "\n",
    "#Print Results\n",
    "grouped = defaultdict(list)\n",
    "for r in all_test_results:\n",
    "        grouped[r.test_from_state].append(r)\n",
    "total_tests = len(all_test_results)\n",
    "total_passed = sum(1 for r in all_test_results if r.status == \"PASS\")\n",
    "total_failed = sum(1 for r in all_test_results if r.status == \"FAIL\")\n",
    "\n",
    "lines.append(\"TEST EXECUTION REPORT\")\n",
    "lines.append(f\"Generated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}\")\n",
    "lines.append(\"-\" * 80)\n",
    "lines.append(f\"Total tests : {total_tests}\")\n",
    "lines.append(f\"Passed      : {total_passed}\")\n",
    "lines.append(f\"Failed      : {total_failed}\")\n",
    "lines.append(\"-\" * 80)\n",
    "\n",
    "for state in sorted(grouped):  \n",
    "    state_results = grouped[state]\n",
    "    state_passed = sum(1 for r in state_results if r.status == \"PASS\")\n",
    "    state_failed = sum(1 for r in state_results if r.status == \"FAIL\")\n",
    "    lines.append(f\"\\nSTATE: {state}\")\n",
    "    lines.append(f\"Tests: {len(state_results)} | Passed: {state_passed} | Failed: {state_failed}\")\n",
    "    lines.append(\"-\" * 60)\n",
    "\n",
    "print(\"\\n\".join(lines))\n",
    "\n",
    "#Check If if this notebook is parent notebook(so produce report), or has been called by another(return data to parent)\n",
    "if state_under_test == this_notebook_state:\n",
    "    #Call Print Test Report Notebook\n",
    "    dbutils.notebook.run( base_path + \"Shared_Notebooks/Common_Produce_TestResults\", 600, {\n",
    "    \"all_test_results_string\":all_test_results_string,\n",
    "    \"state_under_test\":state_under_test,\n",
    "    \"base_path\":base_path,\n",
    "    \"test_results_path\": test_results_path,\n",
    "    \"run_user\":run_user,\n",
    "    \"run_tag\":run_tag,\n",
    "    \"run_by_automation_name\": run_by_automation_name,\n",
    "    \"run_start_datetime\": run_start_datetime.strftime(\"%Y-%m-%d %H:%M:%S\"), \n",
    "    })\n",
    "    \n",
    "#else return to parent notebook\n",
    "else:\n",
    "    print(f\"Exiting Notebook : {this_notebook_state} and returning Data to Parent notebook : {state_under_test}\")\n",
    "    dbutils.notebook.exit(all_test_results_string)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "TEMPLATE_STATE_TESTS",
   "widgets": {
    "fields_to_exclude": {
     "currentValue": "",
     "nuid": "1ed8b420-ad78-43b4-8fd1-85db7bdb328f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "fields_to_exclude",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "fields_to_exclude",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "state_under_test": {
     "currentValue": "",
     "nuid": "77e5a08e-c14b-4c16-b7e6-c213f248da24",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "state_under_test",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "state_under_test",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
