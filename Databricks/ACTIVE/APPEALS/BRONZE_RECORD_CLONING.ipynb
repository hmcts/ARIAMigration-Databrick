{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08f667ea-0ba0-4b4c-8898-894abfee2b1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"pipelines.tableManagedByMultiplePipelinesCheck.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ebf534f-e397-4a16-bd0a-2796768bcaa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import json\n",
    "from pyspark.sql.functions import when, col,coalesce, current_timestamp, lit, date_format\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d04c99b-0867-419d-a2fe-6f7b409ac030",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/configs/config.json\")\n",
    "env_name = config.first()[\"env\"].strip().lower()\n",
    "lz_key = config.first()[\"lz_key\"].strip().lower()\n",
    "\n",
    "print(f\"env_code: {lz_key}\")  # This won't be redacted\n",
    "print(f\"env_name: {env_name}\")  # This won't be redacted\n",
    "\n",
    "KeyVault_name = f\"ingest{lz_key}-meta002-{env_name}\"\n",
    "print(f\"KeyVault_name: {KeyVault_name}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a88f022-d2fd-40cb-87e7-cfc3f2d096d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Service principal credentials\n",
    "client_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-ID\")\n",
    "client_secret = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-SECRET\")\n",
    "tenant_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-TENANT-ID\")\n",
    "\n",
    "# Storage account names\n",
    "curated_storage = f\"ingest{lz_key}curated{env_name}\"\n",
    "checkpoint_storage = f\"ingest{lz_key}xcutting{env_name}\"\n",
    "raw_storage = f\"ingest{lz_key}raw{env_name}\"\n",
    "landing_storage = f\"ingest{lz_key}landing{env_name}\"\n",
    "external_storage = f\"ingest{lz_key}external{env_name}\"\n",
    "\n",
    "\n",
    "# Spark config for curated storage (Delta table)\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{curated_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{curated_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{curated_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{curated_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{curated_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{checkpoint_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{checkpoint_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{checkpoint_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{checkpoint_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{checkpoint_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{raw_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{raw_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{raw_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{raw_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{raw_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{landing_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{landing_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{landing_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{landing_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{landing_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{external_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{external_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{external_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{external_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{external_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4af5be1-974c-4d8a-8513-bae9a3f5be94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read_hive = False\n",
    "\n",
    "# Setting variables for use in subsequent cells\n",
    "raw_path = f\"abfss://raw@ingest{lz_key}raw{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS\"\n",
    "landing_path = f\"abfss://landing@ingest{lz_key}landing{env_name}.dfs.core.windows.net/SQLServer/Sales/IRIS/dbo/\"\n",
    "bronze_path = f\"abfss://bronze@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS\"\n",
    "silver_path = f\"abfss://silver@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS\"\n",
    "gold_path = f\"abfss://gold@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS\"\n",
    "external_path = f\"abfss:///external-csv@ingest{lz_key}external{env_name}.dfs.core.windows.net/\"\n",
    "gold_outputs = \"ARIADM/CCD/APPEALS\"\n",
    "hive_schema = \"ariadm_ccd_apl\"\n",
    "# key_vault = \"ingest00-keyvault-sbox\"\n",
    "\n",
    "html_path = f\"abfss://html-template@ingest{lz_key}landing{env_name}.dfs.core.windows.net/\"\n",
    "\n",
    "# Print all variables\n",
    "variables = {\n",
    "    # \"read_hive\": read_hive,\n",
    "    \"raw_path\": raw_path,\n",
    "    \"landing_path\": landing_path,\n",
    "    \"bronze_path\": bronze_path,\n",
    "    \"silver_path\": silver_path,\n",
    "    \"gold_path\": gold_path,\n",
    "    \"html_path\": html_path,\n",
    "    \"gold_outputs\": gold_outputs,\n",
    "    \"hive_schema\": hive_schema,\n",
    "    \"key_vault\": KeyVault_name\n",
    "}\n",
    "\n",
    "display(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9390510e-6bdb-4249-a89a-5d1777de66e7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769686314109}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1769687336601}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.read.table('ariadm_active_appeals_bronze.bronze_appealcase_crep_rep_floc_cspon_cfs').display()\n",
    "# spark.read.table('ariadm_active_appeals_bronze.bronze_appealcase_crep_rep_floc_cspon_cfs').filter(col(\"CaseNo\").isin(\"AA/00026/2005\",\"EA/00026/2005/COPY\")).display()\n",
    "# spark.read.table('ariadm_active_appeals_bronze.bronze_appealcase_caseappellant_appellant').filter(col(\"CaseNo\").isin(\"AA/00026/2005\",\"EA/00026/2005/COPY\")).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dbc6b57-106e-4a9b-bbb9-0dbd5e1741f0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "View input tables"
    }
   },
   "outputs": [],
   "source": [
    "# spark.read.table('hive_metastore.ariadm_active_appeals_bronze.bronze_status_htype_clist_list_ltype_court_lsitting_adj').display()\n",
    "# spark.read.table('hive_metastore.ariadm_active_appeals_bronze.bronze_appealcase_transaction_transactiontype').display()\n",
    "# spark.read.table('hive_metastore.ariadm_active_appeals_bronze.bronze_appealcase_link_linkdetail').display()\n",
    "# spark.read.table('hive_metastore.ariadm_active_appeals_bronze.bronze_caseadjudicator_adjudicator').display()\n",
    "# spark.read.table('hive_metastore.ariadm_active_appeals_bronze.bronze_appealcategory').display()\n",
    "# spark.read.table('hive_metastore.ariadm_active_appeals_bronze.bronze_documentsreceived').display()\n",
    "# spark.read.table('hive_metastore.ariadm_active_appeals_bronze.bronze_history').display()\n",
    "# spark.read.table('hive_metastore.ariadm_active_appeals_bronze.bronze_history').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26010fe2-a04e-4782-9800-24aff197227e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = [\n",
    "    {\n",
    "        \"caseMapping\": {\n",
    "            \"keyColumn\": \"CaseNo\",\n",
    "            \"existingCaseNo\": \"AA/00026/2005\",\n",
    "            \"newCaseNo\": \"AA/00026/2005/COPY\"\n",
    "        },\n",
    "        \"tables\": {\n",
    "            \"hive_metastore.ariadm_active_appeals_bronze.bronze_appealcase_crep_rep_floc_cspon_cfs\": {\n",
    "                \"CasePrefix\": \"ZZ\",\n",
    "                \"CentreId\": 444\n",
    "            },\n",
    "            \"hive_metastore.ariadm_active_appeals_bronze.bronze_appealcase_caseappellant_appellant\": {\n",
    "                \"Appellant_Email\": \"testytest123@gmail.com\",\n",
    "                \"FCONumber\": \"555\"\n",
    "            },\n",
    "            \"hive_metastore.ariadm_active_appeals_bronze.bronze_status_htype_clist_list_ltype_court_lsitting_adj\": {\n",
    "                \"CaseStatus\": \"100\",\n",
    "                \"Outcome\": 100\n",
    "            },\n",
    "            \"hive_metastore.ariadm_active_appeals_bronze.bronze_appealcase_transaction_transactiontype\": {\n",
    "                \"TransactionId\": 454545,\n",
    "                \"Status\": 454545\n",
    "            },\n",
    "            \"hive_metastore.ariadm_active_appeals_bronze.bronze_appealcase_link_linkdetail\": {\n",
    "                \"LinkNo\": 55555,\n",
    "                \"ReasonLinkId\": 55555\n",
    "            },\n",
    "            \"hive_metastore.ariadm_active_appeals_bronze.bronze_caseadjudicator_adjudicator\": {\n",
    "                \"Judge_Surname\": \"Simpson\",\n",
    "                \"Judge_Forenames\": \"Homer\"\n",
    "            },\n",
    "            \"hive_metastore.ariadm_active_appeals_bronze.bronze_appealcategory\": {\n",
    "                \"CategoryId\": 50\n",
    "            },\n",
    "            \"hive_metastore.ariadm_active_appeals_bronze.bronze_documentsreceived\": {\n",
    "                \"ReceivedDocumentId\": 50\n",
    "            },\n",
    "            \"hive_metastore.ariadm_active_appeals_bronze.bronze_history\": {\n",
    "                \"HistType\": 100\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"caseMapping\": {\n",
    "            \"keyColumn\": \"CaseNo\",\n",
    "            \"existingCaseNo\": \"AA/00026/2005\",\n",
    "            \"newCaseNo\": \"AA/00026/2005/COPY2\"\n",
    "        },\n",
    "        \"tables\": {\n",
    "            \"hive_metastore.ariadm_active_appeals_bronze.bronze_appealcase_crep_rep_floc_cspon_cfs\": {\n",
    "                \"CasePrefix\": \"ZX\",\n",
    "                \"CentreId\": 444\n",
    "            },\n",
    "            \"hive_metastore.ariadm_active_appeals_bronze.bronze_appealcase_caseappellant_appellant\": {\n",
    "                \"Appellant_Email\": \"testytest123@gmail.com\",\n",
    "                \"FCONumber\": \"555\"\n",
    "            },\n",
    "            \"hive_metastore.ariadm_active_appeals_bronze.bronze_status_htype_clist_list_ltype_court_lsitting_adj\": {\n",
    "                \"CaseStatus\": \"100\",\n",
    "                \"Outcome\": 100\n",
    "            },\n",
    "            \"hive_metastore.ariadm_active_appeals_bronze.bronze_appealcase_transaction_transactiontype\": {\n",
    "                \"TransactionId\": 454545,\n",
    "                \"Status\": 454545\n",
    "            },\n",
    "            \"hive_metastore.ariadm_active_appeals_bronze.bronze_appealcase_link_linkdetail\": {\n",
    "                \"LinkNo\": 55555,\n",
    "                \"ReasonLinkId\": 55555\n",
    "            },\n",
    "            \"hive_metastore.ariadm_active_appeals_bronze.bronze_caseadjudicator_adjudicator\": {\n",
    "                \"Judge_Surname\": \"Simpson\",\n",
    "                \"Judge_Forenames\": \"Homer\"\n",
    "            },\n",
    "            \"hive_metastore.ariadm_active_appeals_bronze.bronze_appealcategory\": {\n",
    "                \"CategoryId\": 50\n",
    "            },\n",
    "            \"hive_metastore.ariadm_active_appeals_bronze.bronze_documentsreceived\": {\n",
    "                \"ReceivedDocumentId\": 50\n",
    "            },\n",
    "            \"hive_metastore.ariadm_active_appeals_bronze.bronze_history\": {\n",
    "                \"HistType\": 100\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "        # ##CaseNo settings 3\n",
    "]\n",
    " \n",
    "    # ##CaseNo settings 3\n",
    "    # {\n",
    "    # \"caseMapping\": {\n",
    "    #     \"keyColumn\": \"CaseNo\",\n",
    "    #     \"existingCaseNo\": \"AA/00026/2005\",\n",
    "    #     \"newCaseNo\": \"EA/00026/2005/COPY\"\n",
    "    # },\n",
    "    # \"tables\": {\n",
    "    #     \"hive_metastore.ariadm_active_appeals_bronze.bronze_appealcase_crep_rep_floc_cspon_cfs\": {\n",
    "    #         \"CasePrefix\" : \"AB\",\n",
    "    #         \"CentreId\" : 444\n",
    "    #     },\n",
    "    #     \"hive_metastore.ariadm_active_appeals_bronze.bronze_appealcase_caseappellant_appellant\": {\n",
    "    #         \"Appellant_Email\" : \"testytest123@gmail.com\",\n",
    "    #         \"FCONumber\" : 555\n",
    "    #     },\n",
    "    #     \"hive_metastore.ariadm_active_appeals_bronze.bronze_status_htype_clist_list_ltype_court_lsitting_adj\": {\n",
    "    #         \"CaseStatus\" : 100,\n",
    "    #         \"Outcome\" : 100\n",
    "    #     },\n",
    "    #     \"hive_metastore.ariadm_active_appeals_bronze.bronze_appealcase_transaction_transactiontype\": {\n",
    "    #         \"TransactionId\" : 454545,\n",
    "    #         \"Status\" : 454545\n",
    "    #     },\n",
    "    #     \"hive_metastore.ariadm_active_appeals_bronze.bronze_appealcase_link_linkdetail\": {\n",
    "    #         \"LinkNo\" : 55555,\n",
    "    #         \"ReasonLinkId\" : 55555\n",
    "    #     },\n",
    "    #     \"hive_metastore.ariadm_active_appeals_bronze.bronze_caseadjudicator_adjudicator\": {\n",
    "    #         \"Judge_Surname\" : \"Simpson\",\n",
    "    #         \"Judge_Forenames\" : \"Homer\"\n",
    "    #     },\n",
    "    #     \"hive_metastore.ariadm_active_appeals_bronze.bronze_appealcategory\": {\n",
    "    #         \"CategoryId\" : 50\n",
    "    #     },\n",
    "    #     \"hive_metastore.ariadm_active_appeals_bronze.bronze_documentsreceived\": {\n",
    "    #         \"ReceivedDocumentId\" : 50\n",
    "    #     },\n",
    "    #     \"hive_metastore.ariadm_active_appeals_bronze.bronze_history\": {\n",
    "    #         \"HistType\" : 100\n",
    "    #     }}\n",
    "    # }]\n",
    " \n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "261f5a9b-70c6-4abe-8bf4-8b924053118b",
     "showTitle": true,
     "tableResultSettingsMap": {
      "4": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769700768468}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 4
      }
     },
     "title": "Append"
    }
   },
   "outputs": [],
   "source": [
    "def create_data(config_list):\n",
    " \n",
    "    for config in config_list:\n",
    "        key_col = config['caseMapping']['keyColumn']\n",
    "        existing_case = config['caseMapping']['existingCaseNo']\n",
    "        new_case = config['caseMapping']['newCaseNo']\n",
    " \n",
    "        for table_name, overrides in config['tables'].items():\n",
    "            print(f\"Processing table: {table_name}\")\n",
    "           \n",
    "            try:\n",
    "                df = spark.table(table_name)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Could not read table {table_name}: {e}\")\n",
    "                continue\n",
    "           \n",
    "            row_to_copy = df.filter(col(key_col) == existing_case)\n",
    "           \n",
    "            if row_to_copy.count() == 0:\n",
    "                print(f\"ℹ️ No row found with {key_col}={existing_case} in {table_name}\")\n",
    "                continue\n",
    "           \n",
    "            # 3️⃣ Apply column overrides from config\n",
    "            for col_name, value in overrides.items():\n",
    "                if col_name in df.columns:\n",
    "                    row_to_copy = row_to_copy.withColumn(col_name, lit(value))\n",
    "                else:\n",
    "                    print(f\"⚠️ Column {col_name} does not exist in {table_name}, skipping override\")\n",
    "           \n",
    "            # 4️⃣ Update the CaseNo to the new one\n",
    "            row_to_copy = row_to_copy.withColumn(key_col, lit(new_case))\n",
    " \n",
    "            row_to_copy.display()\n",
    "           \n",
    "            try:\n",
    "                row_to_copy.write.format(\"delta\") \\\n",
    "                    .mode(\"append\") \\\n",
    "                    .saveAsTable(table_name)\n",
    "                print(f\"✅ Successfully appended new row to {table_name} with New CaseNo = {new_case}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to append case : {new_case} to {table_name}: {e}\")\n",
    " \n",
    "create_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6800ef4-c72e-45e3-b71d-fde80b1ab459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def delete_copy_case_nums_from_tables(config_list, suffix=\"/COPY\"):\n",
    "def delete_copy_case_nums_from_tables(config_list, suffix=\"\"):\n",
    "    for config in config_list:\n",
    "        new_caseNo = config['caseMapping']['newCaseNo']\n",
    "        tables = [\n",
    "            t.strip() for t in config['tables'].keys() if t.strip() != \"\"\n",
    "        ]\n",
    "        print(tables)\n",
    " \n",
    "        for table_name in tables:\n",
    "            print(f\"Processing table: {table_name}\")\n",
    "            try:\n",
    "                df = spark.table(table_name)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Could not read table {table_name}: {e}\")\n",
    "                continue\n",
    " \n",
    "            # Filter rows matching the suffix\n",
    "            if suffix == \"\":\n",
    "                to_delete_df = df.filter(col(\"CaseNo\") == new_caseNo)\n",
    "                count_to_delete = to_delete_df.count()\n",
    "            else:\n",
    "                to_delete_df = df.filter(col(\"CaseNo\").like(f\"%{suffix}\"))\n",
    "                count_to_delete = to_delete_df.count()\n",
    " \n",
    "            if count_to_delete == 0:\n",
    "                print(f\"ℹ️ No rows to delete in {table_name}\")\n",
    "                continue\n",
    " \n",
    "            # Preview rows to delete\n",
    "            display(to_delete_df.select(\"CaseNo\"))\n",
    " \n",
    "            # Perform deletion\n",
    "            try:\n",
    "                if suffix == \"\":\n",
    "                    spark.sql(\n",
    "                        f\"DELETE FROM {table_name} WHERE CaseNo = '{new_caseNo}'\"\n",
    "                    )\n",
    "                    print(f\"✅ Deleted {count_to_delete} row(s) from {table_name} where = {new_caseNo} \")\n",
    "                else:\n",
    "                    spark.sql(\n",
    "                        f\"DELETE FROM {table_name} WHERE CaseNo LIKE '%{suffix}'\"\n",
    "                    )\n",
    "                    print(f\"✅ Deleted {count_to_delete} row(s) from {table_name} where suffix = {suffix}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed to delete rows from {table_name}: {e}\")\n",
    " \n",
    "delete_copy_case_nums_from_tables(config)\n",
    "# delete_copy_case_nums_from_tables(config, suffix=\"/COPY2\" )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff3b3136-a258-47ed-8d30-98d520d30b56",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "View config as JSON"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "config = json.dumps(config)\n",
    "print(json.dumps(json.loads(config), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "164bb905-6aa7-4777-b223-588f4dd235fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BRONZE_RECORD_CLONING",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
