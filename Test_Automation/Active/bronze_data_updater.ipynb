{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4f55781a-86c7-47ff-bba4-e4f343863097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Load Config and Setup Enviorment Variables\n",
    "# state_under_test = \"paymentPending\"\n",
    "state_under_test = \"appealSubmitted\"\n",
    "# state_under_test = \"awaitingRespondentEvidence(a)\"\n",
    "# state_under_test = \"awaitingRespondentEvidence(b)\"\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "config = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/configs/config.json\")\n",
    "env_name = config.first()[\"env\"].strip().lower()\n",
    "lz_key = config.first()[\"lz_key\"].strip().lower()\n",
    " \n",
    "# print(f\"env_code: {lz_key}\")  # This won't be redacted\n",
    "# print(f\"env_name: {env_name}\")  # This won't be redacted\n",
    " \n",
    "KeyVault_name = f\"ingest{lz_key}-meta002-{env_name}\"\n",
    "# print(f\"KeyVault_name: {KeyVault_name}\")\n",
    " \n",
    "# Service principal credentials\n",
    "client_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-ID\")\n",
    "client_secret = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-SECRET\")\n",
    "tenant_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-TENANT-ID\")\n",
    " \n",
    "# Storage account names\n",
    "curated_storage = f\"ingest{lz_key}curated{env_name}\"\n",
    "checkpoint_storage = f\"ingest{lz_key}xcutting{env_name}\"\n",
    "raw_storage = f\"ingest{lz_key}raw{env_name}\"\n",
    "landing_storage = f\"ingest{lz_key}landing{env_name}\"\n",
    "external_storage = f\"ingest{lz_key}external{env_name}\"\n",
    "  \n",
    "# Spark config for curated storage (Delta table)\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{curated_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{curated_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{curated_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{curated_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{curated_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{checkpoint_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{checkpoint_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{checkpoint_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{checkpoint_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{checkpoint_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{raw_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{raw_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{raw_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{raw_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{raw_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{landing_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{landing_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{landing_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{landing_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{landing_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{external_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{external_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{external_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{external_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{external_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "  \n",
    "# Setting variables for use in subsequent cells\n",
    "bronze_path = f\"abfss://bronze@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/\"\n",
    "silver_path = f\"abfss://silver@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/\"\n",
    "audit_path = f\"abfss://silver@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/AUDIT/{state_under_test}\"\n",
    "gold_path = f\"abfss://gold@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/{state_under_test}\"\n",
    " \n",
    " \n",
    "# Print all variables\n",
    "# variables = {\n",
    "#     # \"read_hive\": read_hive,\n",
    "    \n",
    "#     \"bronze_path\": bronze_path,\n",
    "#     \"silver_path\": silver_path,\n",
    "#     \"audit_path\": audit_path,\n",
    "#     \"gold_path\": gold_path,\n",
    "#     \"key_vault\": KeyVault_name,\n",
    "#     \"AppealState\": state_under_test\n",
    " \n",
    "# }\n",
    " \n",
    "# display(variables)\n",
    "\n",
    "import json\n",
    "\n",
    "#Get Latest Json Folder\n",
    "json_location = dbutils.fs.ls(f\"{gold_path}/\")[-1]\n",
    "latest_json_location = json_location.name\n",
    "dbutils.fs.ls(f\"{gold_path}/{latest_json_location}\")\n",
    "\n",
    "#Set Paths\n",
    "try: \n",
    "    json_path = f\"{gold_path}/{latest_json_location}/JSON/\"\n",
    "    json_failed_path = f\"{gold_path}/{latest_json_location}/INVALID_JSON/\"\n",
    "    M1_silver = f\"{silver_path}/silver_appealcase_detail\"\n",
    "    M1_bronze = f\"{bronze_path}/bronze_appealcase_crep_rep_floc_cspon_cfs\"\n",
    "    M2_bronze = f\"{bronze_path}/bronze_appealcase_caseappellant_appellant\"\n",
    "    M2_silver = f\"{silver_path}/silver_caseapplicant_detail\"\n",
    "    M3_silver = f\"{silver_path}/silver_status_detail\"\n",
    "    C = f\"{silver_path}/silver_appealcategory_detail\"\n",
    "    bhc = f\"{bronze_path}/bronze_hearing_centres\"\n",
    "    bat = f\"{bronze_path}/bronze_appealtype\" \n",
    "    docsr = f\"{bronze_path}/bronze_documentsreceived\"   \n",
    "    apl_audit = f\"{audit_path}/apl_active_payment_pending_cr_audit_table/\"\n",
    "    sh =  f\"{silver_path}/silver_history_detail\"\n",
    "except:\n",
    "    print(f\"Error during fetch: {str(e)}\")\n",
    "\n",
    "#Create and Load Dataframes\n",
    "json_data = spark.read.format(\"json\").load(json_path)\n",
    "json_failed_data = spark.read.format(\"json\").load(json_failed_path)\n",
    "M1_silver = spark.read.format(\"delta\").load(M1_silver)\n",
    "M1_bronze = spark.read.format(\"delta\").load(M1_bronze)\n",
    "M2_bronze = spark.read.format(\"delta\").load(M2_bronze)\n",
    "M2_silver = spark.read.format(\"delta\").load(M2_silver)\n",
    "M3_silver = spark.read.format(\"delta\").load(M3_silver)\n",
    "C = spark.read.format(\"delta\").load(C)\n",
    "bhc = spark.read.format(\"delta\").load(bhc)\n",
    "bat = spark.read.format(\"delta\").load(bat)\n",
    "docsr = spark.read.format(\"delta\").load(docsr)\n",
    "apl_audit = spark.read.format(\"delta\").load(apl_audit)\n",
    "sh_audit = spark.read.format(\"delta\").load(sh)\n",
    "\n",
    "#Can be removed later, added to allow developing of code in this notebook to begin with before moving to func files\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, lit, array, struct, collect_list, \n",
    "    max as spark_max, date_format, row_number, expr, \n",
    "    size, udf, coalesce, concat_ws, concat, trim, year, split, datediff,\n",
    "    collect_set, current_timestamp,transform, first, array_contains\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c106e8ee-d429-42e3-8461-6c7c11117088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Patching - For Data Cut : 07-04-2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29a33535-9aae-430e-b410-f3ce10df1123",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"BirthDate\":316},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770218175778}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "UPDATE BRONZE DATA SCRIPT"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "#UPDATE BRONZE DATA SCRIPT FOR PAYMENT PENDING.\n",
    "#\n",
    "#NOTE: The below code will update bronze data that will not pass the DQ expecation checks due to\n",
    "#issues in the data that will be resolved before live but are needeed to get all the data through the checks\n",
    "#and sent to CCD in the mean time\n",
    "###############################\n",
    "from pyspark.sql.functions import *\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "######################\n",
    "#TO FIX PP DATA FROM FIRST STAGING DATA CUT (now superseeded due to new data cut)\n",
    "######################\n",
    "# BirthDate / appellantDateOfBirth\n",
    "\n",
    "# bronze_table = DeltaTable.forName(spark,\"ariadm_active_appeals.bronze_appealcase_caseappellant_appellant\")\n",
    "\n",
    "# display(bronze_table.toDF().filter(col(\"CaseNo\").isin(\"HU/00278/2025\", \"HU/00455/2025\", \"HU/00472/2025\" )).select(\"CaseNo\", \"BirthDate\"))\n",
    "\n",
    "# bronze_table.update(\n",
    "#     condition=col(\"CaseNo\").isin(\"HU/00278/2025\", \"HU/00455/2025\", \"HU/00472/2025\"),\n",
    "#     set={\"BirthDate\": lit(\"2000-02-01T00:00:00Z\")}\n",
    "# )\n",
    "\n",
    "# display(bronze_table.toDF().filter(col(\"CaseNo\").isin(\"HU/00278/2025\", \"HU/00455/2025\", \"HU/00472/2025\" )).select(\"CaseNo\", \"BirthDate\"))\n",
    "\n",
    "\n",
    "# #################\n",
    "# #valid_appellantNationalitiesDescription_not_null\n",
    "# #and\n",
    "# #valid_appellantNationalities_not_null\n",
    "# #NationalityId\n",
    "# #Where No mapping required for 201/203\n",
    "\n",
    "# bronze_table = DeltaTable.forName(spark, \"ariadm_active_appeals.bronze_appealcase_crep_rep_floc_cspon_cfs\")\n",
    "\n",
    "# display(bronze_table.toDF().filter(col(\"CaseNo\").isin(\"HU/00302/2025\", \"HU/00569/2025\", \"HU/00586/2025\",\"HU/00560/2025\" )).select(\"CaseNo\", \"NationalityId\"))\n",
    "\n",
    "# bronze_table.update(\n",
    "#     condition=col(\"CaseNo\").isin(\"HU/00302/2025\", \"HU/00569/2025\", \"HU/00586/2025\",\"HU/00560/2025\"),\n",
    "#     set={\"NationalityId\": lit(\"41\")}\n",
    "# )\n",
    "\n",
    "# display(bronze_table.toDF().filter(col(\"CaseNo\").isin(\"HU/00302/2025\", \"HU/00569/2025\", \"HU/00586/2025\",\"HU/00560/2025\" )).select(\"CaseNo\", \"NationalityId\"))\n",
    "\n",
    "# #################\n",
    "# #valid_oocAddressLine1 valid_oocAddressLine2\n",
    "# #changing null values to actual values\n",
    "\n",
    "# bronze_table = DeltaTable.forName(spark, \"ariadm_active_appeals.bronze_appealcase_crep_rep_floc_cspon_cfs\")\n",
    "# display(bronze_table.toDF().filter(col(\"CaseNo\").isin(\"HU/00185/2025\", \"HU/02151/2024\")).select(\"CaseNo\", \"CaseRep_Address1\", \"CaseRep_Address2\", \"CaseRep_Address3\", \"CaseRep_Address4\" , \"CaseRep_Address5\", \"CaseRep_Postcode\"))\n",
    "\n",
    "# bronze_table.update(\n",
    "#     condition=col(\"CaseNo\").isin(\"HU/00185/2025\", \"HU/02151/2024\"),\n",
    "#     set={\"CaseRep_Address1\": lit(\"925 Lisa Plains Apt. 642X\"),\n",
    "#          \"CaseRep_Address2\" : lit(\"Hill SquareX\"),\n",
    "#         \"CaseRep_Address3\" : lit(\"LynchhavenX\"),\n",
    "#         \"CaseRep_Address4\" : lit(\"AustraliaX\"),\n",
    "#         \"CaseRep_Address5\" : lit(\"NLX\"),\n",
    "#         # \"CaseRep_Postcode\" : lit(\"Hill SquareX\"),\n",
    "         \n",
    "#          }\n",
    "# )\n",
    "\n",
    "\n",
    "# display(bronze_table.toDF().filter(col(\"CaseNo\").isin(\"HU/00185/2025\", \"HU/02151/2024\")).select(\"CaseNo\", \"CaseRep_Address1\", \"CaseRep_Address2\", \"CaseRep_Address3\", \"CaseRep_Address4\" , \"CaseRep_Address5\", \"CaseRep_Postcode\"))\n",
    "\n",
    "\n",
    "# #################\n",
    "# #valid_oocAddressLine1 valid_oocAddressLine2\n",
    "# #change in ooc4 ReunionX to GuamX\n",
    "\n",
    "# bronze_table = DeltaTable.forName(spark, \"ariadm_active_appeals.bronze_appealcase_crep_rep_floc_cspon_cfs\")\n",
    "# display(bronze_table.toDF().filter(col(\"CaseNo\").isin(\"HU/02191/2024\", \"HU/01475/2024\")).select(\"CaseNo\", \"CaseRep_Address1\", \"CaseRep_Address2\", \"CaseRep_Address3\", \"CaseRep_Address4\" , \"CaseRep_Address5\", \"CaseRep_Postcode\"))\n",
    "\n",
    "# bronze_table.update(\n",
    "#     condition=col(\"CaseNo\").isin(\"HU/01475/2024\", \"HU/02191/2024\"),\n",
    "#     set={\n",
    "#         \"CaseRep_Address4\" : lit(\"AustraliaX\")                 \n",
    "#          }\n",
    "# )\n",
    "\n",
    "\n",
    "# display(bronze_table.toDF().filter(col(\"CaseNo\").isin(\"HU/02191/2024\", \"HU/01475/2024\")).select(\"CaseNo\", \"CaseRep_Address1\", \"CaseRep_Address2\", \"CaseRep_Address3\", \"CaseRep_Address4\" , \"CaseRep_Address5\", \"CaseRep_Postcode\"))\n",
    "\n",
    "\n",
    "\n",
    "# ################################\n",
    "\n",
    "# cases_to_update = ['EA/02806/2023',\n",
    "# 'HU/00575/2025',\n",
    "# 'HU/00581/2025',\n",
    "# 'HU/00447/2025',\n",
    "# 'HU/00574/2023',\n",
    "# 'HU/00591/2025',\n",
    "# 'EA/00588/2025',\n",
    "# 'EA/00551/2025',\n",
    "# 'HU/00304/2025',\n",
    "# 'EA/00495/2025',\n",
    "# 'EA/00560/2025',\n",
    "# 'EA/06826/2022',\n",
    "# 'EA/00490/2025',\n",
    "# 'EA/00554/2025',\n",
    "# 'EA/01778/2024',\n",
    "# 'HU/00562/2025',\n",
    "# 'EA/00552/2025',\n",
    "# 'HU/00511/2025',\n",
    "# 'EA/00483/2025',\n",
    "# 'EA/09676/2022',\n",
    "# 'EA/00493/2025',\n",
    "# 'HU/00224/2025',\n",
    "# 'EA/00496/2025',\n",
    "# 'EA/00538/2025',\n",
    "# 'EA/00558/2025',\n",
    "# 'EA/08372/2022',\n",
    "# 'HU/00822/2024',\n",
    "# 'HU/00574/2025',\n",
    "# 'EA/02065/2024',\n",
    "# 'HU/00442/2025',\n",
    "# 'EA/00586/2025',\n",
    "# 'HU/00590/2025',\n",
    "# 'HU/00569/2025',\n",
    "# 'EA/00557/2025',\n",
    "# 'HU/02346/2024',\n",
    "# 'EA/00562/2025',\n",
    "# 'HU/00573/2025',\n",
    "# 'HU/00571/2025',\n",
    "# 'EA/01893/2023',\n",
    "# 'EA/00584/2025',\n",
    "# 'HU/00579/2025',\n",
    "# 'HU/00555/2025',\n",
    "# 'HU/00583/2025',\n",
    "# 'EA/00591/2025',\n",
    "# 'EA/00556/2025',\n",
    "# 'EA/00497/2025',\n",
    "# 'HU/01972/2023',\n",
    "# 'EA/00437/2025',\n",
    "# 'HU/00577/2025',\n",
    "# 'EA/00585/2025',\n",
    "# 'HU/00252/2025',\n",
    "# 'HU/00557/2025',\n",
    "# 'EA/00485/2025',\n",
    "# 'HU/00563/2025',\n",
    "# 'HU/00278/2025',\n",
    "# 'EA/00559/2025',\n",
    "# 'EA/00553/2025',\n",
    "# 'HU/00578/2025',\n",
    "# 'HU/00445/2025',\n",
    "# 'HU/00325/2025',\n",
    "# 'EA/00555/2025',\n",
    "# 'HU/00572/2025',\n",
    "# 'HU/00582/2025',\n",
    "# 'EA/00587/2025',\n",
    "# 'HU/00638/2024',\n",
    "# 'HU/00453/2025',\n",
    "# \"EA/00289/2025\"\n",
    "# ]\n",
    "\n",
    "\n",
    "# bronze_table = DeltaTable.forName(spark, \"ariadm_active_appeals.bronze_appealcase_caseappellant_appellant\")\n",
    "\n",
    "# display(bronze_table.toDF().filter(col(\"CaseNo\").isin(cases_to_update)).select(\"CaseNo\", \"Appellant_Address4\"))\n",
    "\n",
    "# bronze_table.update(\n",
    "#     condition=col(\"CaseNo\").isin(cases_to_update),\n",
    "#     set={\n",
    "#         \"Appellant_Address4\" : lit(\"AustraliaX\")                 \n",
    "#          }\n",
    "# )\n",
    "\n",
    "\n",
    "# display(bronze_table.toDF().filter(col(\"CaseNo\").isin(cases_to_update)).select(\"CaseNo\", \"Appellant_Address4\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c11c810a-a735-4227-b8c2-74c8786b3088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Patching - For Data cut from : 02-02-2026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51fc5165-52ea-4ca7-9a9b-10a42011fc13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TABLES\n",
    "# bronze_table_to_update = \"ariadm_active_appeals.bronze_appealcase_crep_rep_floc_cspon_cfs\"\n",
    "# bronze_table_to_update = \"ariadm_active_appeals.bronze_appealcase_caseappellant_appellant\"\n",
    "\n",
    "# FROM paymentpending_gold.stg_main_paymentPending_validation\n",
    "# FROM paymentpending_gold.stg_main_appeal_submitted_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c8b5619-8e20-4e70-b634-b63526eb0ef4",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1771511441047}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "countryGovUkOocAdminJ"
    }
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "#valid_countryGovUkOocAdminJ - patch a postcode in \"Appellant_Postcode\"\n",
    "##############################\n",
    "\n",
    "\n",
    "#Setup\n",
    "checks = {}\n",
    "bronze_table_to_update = \"ariadm_active_appeals.bronze_appealcase_caseappellant_appellant\"\n",
    "\n",
    "#Add the Gold Tables to the list below for any States that need updating\n",
    "state_to_update_1 =  \"paymentpending_gold.stg_main_paymentPending_validation\"\n",
    "state_to_update_2 = \"paymentpending_gold.stg_main_appeal_submitted_validation\"\n",
    "# states_to_update = [state_to_update_1, state_to_update_2]\n",
    "states_to_update = [state_to_update_2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "checks[\"valid_countryGovUkOocAdminJ\"] = (\"(((array_contains(valid_categoryIdList, 38)) AND (countryGovUkOocAdminJ IS NOT NULL) AND (countryGovUkOocAdminJ IN ('AF', 'AX', 'AL', 'DZ', 'AD', 'AO', 'AI', 'AG', 'AR', 'AM', 'AW', 'AC', 'AU', 'AT', 'AZ', 'BS', 'BH', 'BD', 'BB', 'BY', 'BE', 'BZ', 'BJ', 'BM', 'BT', 'BO', 'BQ', 'BA', 'BW', 'BR', 'IO', 'VG', 'BN', 'BG', 'BF', 'BI', 'KH', 'CM', 'CA', 'IC', 'CV', 'KY', 'CF', 'EA', 'TD', 'CL', 'CN', 'CX', 'CO', 'KM', 'CD', 'CG', 'CK', 'CR', 'HR', 'CU', 'CW', 'CY', 'CZ', 'DK', 'DJ', 'DM', 'DO', 'EC', 'EG', 'SV', 'GQ', 'ER', 'EE', 'ET', 'FK', 'FO', 'FJ', 'FI', 'FR', 'GF', 'PF', 'TF', 'GA', 'GM', 'GE', 'DE', 'GH', 'GI', 'GR', 'GL', 'GD', 'GP', 'GT', 'GN', 'GW', 'GY', 'HT', 'HN', 'HK', 'HU', 'IS', 'IN', 'ID', 'IR', 'IQ', 'IE', 'IL', 'IT', 'CI', 'JM', 'JP', 'JO', 'KZ', 'KE', 'KI', 'KO', 'KW', 'KG', 'LA', 'LV', 'LB', 'LS', 'LR', 'LY', 'LI', 'LT', 'LU', 'MO', 'MK', 'MG', 'YT', 'MW', 'MY', 'MV', 'ML', 'MT', 'MQ', 'MR', 'MU', 'MX', 'MD', 'MN', 'ME', 'MS', 'MA', 'MZ', 'MM', 'NA', 'NR', 'NF', 'NP', 'NL', 'NC', 'NZ', 'NI', 'NE', 'NG', 'NU', 'KP', 'NO', 'OM', 'PK', 'PW', 'PA', 'PG', 'PY', 'PE', 'PH', 'PN', 'PL', 'PT', 'PR', 'QA', 'RE', 'RO', 'RU', 'RW', 'SM', 'ST', 'SA', 'SN', 'RS', 'SC', 'SL', 'SG', 'SK', 'SI', 'SB', 'ZA', 'KR', 'SS', 'ES', 'LK', 'BQ', 'SH', 'KN', 'LC', 'MF', 'VC', 'SD', 'SR', 'SZ', 'SE', 'CH', 'SY', 'TW', 'TJ', 'TZ', 'TH', 'TL', 'TG', 'TK', 'TO', 'TT', 'TN', 'TR', 'TM', 'TC', 'TV', 'UG', 'UA', 'AE', 'GB', 'UY', 'US', 'UZ', 'VU', 'VA', 'VE', 'VN', 'WF', 'EH', 'WS', 'YE', 'ZM', 'ZW'))) OR (countryGovUkOocAdminJ IS NULL))\")\n",
    "\n",
    "#####################\n",
    "#Get Cases Failing Validation\n",
    "#####################\n",
    "dq_rules = \"({0})\".format(\" AND \".join(checks.values()))\n",
    "\n",
    "\n",
    "for state_table in states_to_update:\n",
    "\n",
    "    df_validation_filtered = spark.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {state_table}\n",
    "    WHERE CaseNo NOT IN (\n",
    "    SELECT\n",
    "        CaseNo\n",
    "    FROM {state_table}\n",
    "    WHERE {dq_rules})\n",
    "    \"\"\")\n",
    "    df_validation_filtered = df_validation_filtered.select(\"CaseNo\", \"countryGovUkOocAdminJ\")\n",
    "\n",
    "\n",
    "    #####################\n",
    "    #join with bronze data\n",
    "    #####################\n",
    "    #Join Failed Record in Gold With Bronze\n",
    "    df_validation_filtered = df_validation_filtered.join(M2_bronze, on=\"CaseNo\", how=\"inner\")\n",
    "    display(df_validation_filtered.select(\"CaseNo\",  \"countryGovUkOocAdminJ\", \"Appellant_Postcode\"))\n",
    "\n",
    "    #####################\n",
    "    #Update Data\n",
    "    #####################\n",
    "    #Pull out case Numbers to patch\n",
    "    case_nos_to_update = [row[\"CaseNo\"] for row in df_validation_filtered.select(\"CaseNo\").distinct().collect()]\n",
    "    #set table to update\n",
    "    bronze_table_to_update_df = DeltaTable.forName(spark, bronze_table_to_update)\n",
    "\n",
    "    #Display Before    \n",
    "    display(M2_bronze.filter(col(\"CaseNo\").isin(case_nos_to_update)).select(\"CaseNo\", \"Appellant_Postcode\"))    \n",
    "\n",
    "    #E3U 5EH\n",
    "    #SW1A 1A\n",
    "    # bronze_table_to_update_df.update(\n",
    "    #     condition=col(\"CaseNo\").isin(case_nos_to_update),\n",
    "    #     set={\n",
    "    #         \"Appellant_Postcode\" : lit(\"E3U 5EH\")                 \n",
    "    #          }\n",
    "    # )\n",
    "\n",
    "    #Display After\n",
    "    display(M2_bronze.filter(col(\"CaseNo\").isin(case_nos_to_update)).select(\"CaseNo\", \"Appellant_Postcode\"))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "056d52f9-b418-46a5-b2cc-2b9540b3e723",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1771519073005}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "legalrepEmail"
    }
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "#valid_legalrepEmail_not_null - patch one of these : \"Rep_Email\" , \"CaseRep_Email\" , \"FileSpecificEmail\"\n",
    "##############################\n",
    "\n",
    "\n",
    "#Setup\n",
    "checks = {}\n",
    "bronze_table_to_update = \"ariadm_active_appeals.bronze_appealcase_crep_rep_floc_cspon_cfs\"\n",
    "bronze_table = spark.table(bronze_table_to_update)\n",
    "\n",
    "#Add the Gold Tables to the list below for any States that need updating\n",
    "state_to_update_1 =  \"paymentpending_gold.stg_main_paymentPending_validation\"\n",
    "state_to_update_2 = \"paymentpending_gold.stg_main_appeal_submitted_validation\"\n",
    "# states_to_update = [state_to_update_1, state_to_update_2]\n",
    "states_to_update = [state_to_update_2]\n",
    "\n",
    "\n",
    "\n",
    "checks[\"valid_legalrepEmail_not_null\"] = \"((dv_representation = 'LR' AND legalRepEmail IS NOT NULL AND legalRepEmail RLIKE r'^([a-zA-Z0-9_\\\\-\\\\.]+)@([a-zA-Z0-9_\\\\-\\\\.]+)\\\\.([a-zA-Z]{2,5})$') OR (dv_representation != 'LR' AND legalRepEmail IS NULL))\"\n",
    "\n",
    "\n",
    "\n",
    "#####################\n",
    "#Get Cases Failing Validation\n",
    "#####################\n",
    "dq_rules = \"({0})\".format(\" AND \".join(checks.values()))\n",
    "\n",
    "\n",
    "for state_table in states_to_update:\n",
    "\n",
    "    df_validation_filtered = spark.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {state_table}\n",
    "    WHERE CaseNo NOT IN (\n",
    "    SELECT\n",
    "        CaseNo\n",
    "    FROM {state_table}\n",
    "    WHERE {dq_rules})\n",
    "    \"\"\")\n",
    "    df_validation_filtered = df_validation_filtered.select(\"CaseNo\", \"legalRepEmail\")\n",
    "\n",
    "\n",
    "    #####################\n",
    "    #join with bronze data\n",
    "    #####################\n",
    "    #Join Failed Record in Gold With Bronze\n",
    "    df_validation_filtered = df_validation_filtered.join(M1_bronze, on=\"CaseNo\", how=\"inner\")\n",
    "    # display(df_validation_filtered)\n",
    "    # display(df_validation_filtered.select(\"CaseNo\",  \"legalRepEmail\", \"CaseRep_Email\"))\n",
    "\n",
    "    #####################\n",
    "    #Update Data\n",
    "    #####################\n",
    "    #Pull out case Numbers to patch\n",
    "    case_nos_to_update = [row[\"CaseNo\"] for row in df_validation_filtered.select(\"CaseNo\").distinct().collect()]\n",
    "    #set table to update\n",
    "    bronze_table_to_update_df = DeltaTable.forName(spark, bronze_table_to_update)\n",
    "\n",
    "    #Display Before\n",
    "    # display(bronze_table)\n",
    "    display(M1_bronze.filter(col(\"CaseNo\").isin(case_nos_to_update)).select(\"CaseNo\", \"CaseRep_Email\"))    \n",
    "\n",
    "    bronze_table_to_update_df.update(\n",
    "        condition=col(\"CaseNo\").isin(case_nos_to_update),\n",
    "        set={\n",
    "           \"CaseRep_Email\" : lit(\"joexbloggs@fake.com\")                 \n",
    "            }\n",
    "    )\n",
    "\n",
    "    #Display After\n",
    "    display(M1_bronze.filter(col(\"CaseNo\").isin(case_nos_to_update)).select(\"CaseNo\", \"CaseRep_Email\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b83853a1-6224-4934-ba7b-e3cd166d4814",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "oocAddressLine1"
    }
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "#valid_oocAddressLine1 - patch : \"CaseRep_Address1\"\n",
    "##############################\n",
    "\n",
    "\n",
    "#Setup\n",
    "checks = {}\n",
    "bronze_table_to_update = \"ariadm_active_appeals.bronze_appealcase_crep_rep_floc_cspon_cfs\"\n",
    "\n",
    "#Add the Gold Tables to the list below for any States that need updating\n",
    "state_to_update_1 =  \"paymentpending_gold.stg_main_paymentPending_validation\"\n",
    "state_to_update_2 = \"paymentpending_gold.stg_main_appeal_submitted_validation\"\n",
    "# states_to_update = [state_to_update_1, state_to_update_2]\n",
    "states_to_update = [state_to_update_2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "checks[\"valid_oocAddressLine1\"] = (\n",
    "        \"\"\"(\n",
    "            (dv_representation = 'LR' AND oocAddressLine1 IS NOT NULL AND legalRepHasAddress <=> 'No')\n",
    "            OR\n",
    "            (dv_representation = 'LR' AND oocAddressLine1 IS NULL AND legalRepHasAddress <=> 'Yes')\n",
    "            OR\n",
    "            (dv_representation != 'LR' AND oocAddressLine1 IS NULL)\n",
    "        )\"\"\"\n",
    "    )\n",
    "\n",
    "#####################\n",
    "#Get Cases Failing Validation\n",
    "#####################\n",
    "dq_rules = \"({0})\".format(\" AND \".join(checks.values()))\n",
    "\n",
    "\n",
    "for state_table in states_to_update:\n",
    "\n",
    "    df_validation_filtered = spark.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {state_table}\n",
    "    WHERE CaseNo NOT IN (\n",
    "    SELECT\n",
    "        CaseNo\n",
    "    FROM {state_table}\n",
    "    WHERE {dq_rules})\n",
    "    \"\"\")\n",
    "    df_validation_filtered = df_validation_filtered.select(\"CaseNo\", \"oocAddressLine1\")\n",
    "\n",
    "\n",
    "    #####################\n",
    "    #join with bronze data\n",
    "    #####################\n",
    "    #Join Failed Record in Gold With Bronze\n",
    "    df_validation_filtered = df_validation_filtered.join(M1_bronze, on=\"CaseNo\", how=\"inner\")\n",
    "    display(df_validation_filtered.select(\"CaseNo\",  \"oocAddressLine1\", \"CaseRep_Address1\"))\n",
    "\n",
    "    #####################\n",
    "    #Update Data\n",
    "    #####################\n",
    "    #Pull out case Numbers to patch\n",
    "    case_nos_to_update = [row[\"CaseNo\"] for row in df_validation_filtered.select(\"CaseNo\").distinct().collect()]\n",
    "    #set table to update\n",
    "    bronze_table_to_update_df = DeltaTable.forName(spark, bronze_table_to_update)\n",
    "\n",
    "    #Display Before    \n",
    "    display(M1_bronze.filter(col(\"CaseNo\").isin(case_nos_to_update)).select(\"CaseNo\", \"CaseRep_Address1\"))    \n",
    "\n",
    "    #E3U 5EH\n",
    "    #SW1A 1A\n",
    "    # bronze_table_to_update_df.update(\n",
    "    #     condition=col(\"CaseNo\").isin(case_nos_to_update),\n",
    "    #     set={\n",
    "    #         \"CaseRep_Address1\" : lit(\"617 Joshua Park Apt. 191X\")                 \n",
    "    #          }\n",
    "    # )\n",
    "\n",
    "    #Display After\n",
    "    display(M1_bronze.filter(col(\"CaseNo\").isin(case_nos_to_update)).select(\"CaseNo\", \"CaseRep_Address1\"))    \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_data_updater",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
