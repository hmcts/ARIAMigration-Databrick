{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a328d67-ff86-44ed-9f12-0a7311839a8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install confluent-kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6d3662f-e541-49ef-927f-1e7c6ae27334",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "import json\n",
    "from  itertools import islice\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import col, decode, split, element_at, udf, lit, reduce, from_json\n",
    "import logging\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark import SparkContext\n",
    "import os\n",
    "from functools import reduce\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84acefcc-8861-4465-b331-f740d160460e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Assign configs\n",
    "config = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/configs/config.json\")\n",
    "env = config.first()[\"env\"].strip().lower()\n",
    "lz_key = config.first()[\"lz_key\"].strip().lower()\n",
    "\n",
    "keyvault_name = f\"ingest{lz_key}-meta002-{env}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "688eadb3-029f-4141-8725-2fb843088687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Access the Service Principle secrets from keyvaults\n",
    "client_secret = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-SECRET')\n",
    "tenant_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-TENANT-ID')\n",
    "client_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57e4ec7b-dbb2-4b96-ae0d-e5b4b98fd6fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Paramaterise containers\n",
    "curated_storage_account = f\"ingest{lz_key}curated{env}\"\n",
    "curated_container = \"gold\"\n",
    "silver_curated_container = \"silver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f7d048b-1249-4316-b286-3f5e61aab6f6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Assign OAuth"
    }
   },
   "outputs": [],
   "source": [
    "curated_storage_account = f\"ingest{lz_key}curated{env}\"\n",
    "checkpoint_storage_account = f\"ingest{lz_key}xcutting{env}\"\n",
    "\n",
    "##Assign OAuth to curated storage account\n",
    "storage_accounts = [curated_storage_account, checkpoint_storage_account]\n",
    "\n",
    "for storage_account in storage_accounts:\n",
    "    configs = {\n",
    "            f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\": \"OAuth\",\n",
    "            f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\":\n",
    "                \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "            f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\": client_id,\n",
    "            f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\": client_secret,\n",
    "            f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\":\n",
    "                f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "        }\n",
    "    for key,val in configs.items():\n",
    "        spark.conf.set(key,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85aabf9f-b64b-4338-b23f-6406765b7fd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print out the auth config for each storage account to confirm\n",
    "for storage_account in storage_accounts:\n",
    "    key = f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\"\n",
    "    print(f\"{key}: {spark.conf.get(key, 'MISSING')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e29f790a-cbd2-496f-ab33-a6945d7dd415",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eh_kv_secret = dbutils.secrets.get(scope=keyvault_name, key=\"RootManageSharedAccessKey\")\n",
    "\n",
    "# Event Hub configurations\n",
    "eventhubs_hostname = f\"ingest{lz_key}-integration-eventHubNamespace001-{env}.servicebus.windows.net:9093\"\n",
    "conf = {\n",
    "    'bootstrap.servers': eventhubs_hostname,\n",
    "    'security.protocol': 'SASL_SSL',\n",
    "    'sasl.mechanism': 'PLAIN',\n",
    "    'sasl.username': '$ConnectionString',\n",
    "    'sasl.password': eh_kv_secret,\n",
    "    'retries': 5,                     # Increased retries\n",
    "    'enable.idempotence': True,        # Enable idempotent producer\n",
    "}\n",
    "broadcast_conf = sc.broadcast(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf6af932-7af8-41d8-a7cb-1ea23019268d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Workflow - Sequential State Processing\n",
    "states = [\n",
    "    \"paymentPending\", \n",
    "    \"appealSubmitted\", \n",
    "    \"awaitingRespondentEvidence(a)\", \n",
    "    \"awaitingRespondentEvidence(b)\", \n",
    "    \"caseUnderReview\", \n",
    "    \"reasonForAppealSubmitted\", \n",
    "    \"listing\",\n",
    "    \"PrepareForHearing\",\n",
    "    \"Decision\",\n",
    "    \"FTPA Submitted (a)\",\n",
    "    \"FTPA Submitted (b)\",\n",
    "    \"Decided (b)\",\n",
    "    \"Decided (a)\",\n",
    "    \"FTPA Decided\",\n",
    "    \"Ended\",\n",
    "    \"Remitted\"\n",
    "]\n",
    "\n",
    "# Retrieve the state parameter from the Databricks Workflow\n",
    "# The workflow will pass this parameter to each task\n",
    "dbutils.widgets.text(\"current_state\", \"paymentPending\", \"State to Process\")\n",
    "\n",
    "# Get the current state to process\n",
    "current_state = dbutils.widgets.get(\"current_state\")\n",
    "\n",
    "print(f\"üîÑ Processing state: {current_state}\")\n",
    "\n",
    "# Validate that the state exists in our list\n",
    "if current_state not in states:\n",
    "    raise ValueError(f\"Invalid state: {current_state}. Must be one of: {states}\")\n",
    "\n",
    "# \n",
    "curated_storage_account = f\"ingest{lz_key}curated{env}\"\n",
    "curated_container = \"gold\"\n",
    "silver_curated_container = \"silver\"\n",
    "\n",
    "gold_files_base_path = f\"abfss://{curated_container}@{curated_storage_account}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/{current_state}/\"\n",
    "\n",
    "try:\n",
    "    files = dbutils.fs.ls(gold_files_base_path)[-1] # Index on newest file\n",
    "    \n",
    "    valid_json = files.path + \"JSON/\"\n",
    "    print(f\"üìÇ Valid JSON path: {valid_json}\")\n",
    "\n",
    "    ## Process INVALID_JSON\n",
    "    invalid_json = files.path + \"INVALID_JSON/\"\n",
    "    try:\n",
    "        dbutils.fs.ls(invalid_json)\n",
    "    except Exception:\n",
    "        print(f\"‚ÑπÔ∏è No INVALID_JSON directory found for state: {current_state}\")\n",
    "        print(\"‚úÖ Processing complete for this state!\")\n",
    "    else:\n",
    "        from pyspark.sql.functions import *\n",
    "        \n",
    "        # Load binary data\n",
    "        binary_df = (\n",
    "            spark.read.format('binaryFile')\n",
    "            .option('pathGlobFilter', '*.{html,json}')\n",
    "            .option('recursiveFileLookup', 'true')\n",
    "            .load(invalid_json)\n",
    "        )\n",
    "        \n",
    "        # Process data\n",
    "        html_df = (\n",
    "            binary_df\n",
    "            .withColumn(\"content_str\", decode(col('content'), 'utf-8'))\n",
    "            .withColumn(\"file_path\", element_at(split(col('path'), '/'), -1))\n",
    "            .withColumn(\"state\", lit(current_state))\n",
    "            .select('content_str', 'file_path', 'state')\n",
    "        )\n",
    "        \n",
    "        # Check if we have data to process\n",
    "        record_count = html_df.count()\n",
    "        if record_count == 0:\n",
    "            print(f\"‚ÑπÔ∏è No data to process for state: {current_state}\")\n",
    "        else:\n",
    "            print(f\"üìä Found {record_count} records for state: {current_state}\")\n",
    "            \n",
    "            # Apply repartitioning\n",
    "            num_spark_partitions = 1\n",
    "            optimized_html_df = html_df.repartition(num_spark_partitions)\n",
    "            \n",
    "            display(optimized_html_df)\n",
    "            \n",
    "            # KAFKA CONFIGURATION\n",
    "            print(f\"üì§ Sending {record_count} records to Kafka for state: {current_state}\")\n",
    "                        \n",
    "            def process_partition(partition):\n",
    "                import logging\n",
    "                from confluent_kafka import Producer\n",
    "                from datetime import datetime\n",
    "\n",
    "                # Initialize logger\n",
    "                logging.basicConfig(level=logging.INFO)\n",
    "                logger = logging.getLogger('KafkaProducer')\n",
    "                \n",
    "                failure_list = []\n",
    "                success_list = []\n",
    "                results = []\n",
    "\n",
    "                # Initialize producer\n",
    "                producer = Producer(**broadcast_conf.value)\n",
    "\n",
    "                for row in partition:\n",
    "                    if row.file_path is None or row.content_str is None:\n",
    "                        logger.warning(f\"Skipping row with missing file_path/content_str: {row}\")\n",
    "                        continue\n",
    "\n",
    "                    ## Use current row for callback\n",
    "                    current_state_row = row.state\n",
    "                    current_file_path = row.file_path\n",
    "\n",
    "                    def delivery_report(err, msg):\n",
    "                        key_str = msg.key().decode('utf-8') if msg.key() is not None else \"Unknown\"\n",
    "                        timestamp = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                        \n",
    "                        if err is not None:\n",
    "                            err_msg = str(err)\n",
    "                            logger.error(f\"Message delivery failed for key {key_str}: {err}\")\n",
    "                            failure_list.append((key_str, current_state_row, \"failure\", err_msg, timestamp))\n",
    "                        else:\n",
    "                            success_list.append((key_str, current_state_row, \"success\", \"\", timestamp))\n",
    "\n",
    "                    try:\n",
    "                        # Handle different content_str types\n",
    "                        if isinstance(row.content_str, str):\n",
    "                            value = row.content_str.encode('utf-8')\n",
    "                        elif isinstance(row.content_str, bytearray):\n",
    "                            value = bytes(row.content_str)\n",
    "                        elif isinstance(row.content_str, bytes):\n",
    "                            value = row.content_str\n",
    "                        else:\n",
    "                            logger.error(f\"Unsupported type for content_str: {type(row.content_str)}\")\n",
    "                            failure_list.append((current_file_path, current_state_row, \"failure\", \"Unsupported content type\", datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "                            continue\n",
    "\n",
    "                        # Produce message to Kafka\n",
    "                        producer.produce(\n",
    "                            topic=f'evh-active-pub-{env}-{lz_key}-uks-dlrm-01',\n",
    "                            key=current_file_path.encode('utf-8'),\n",
    "                            value=value,\n",
    "                            callback=delivery_report\n",
    "                        )\n",
    "\n",
    "                    except BufferError:\n",
    "                        logger.error(\"Producer buffer full. Polling for events.\")\n",
    "                        producer.poll(1)\n",
    "                        # Retry the message production\n",
    "                        try:\n",
    "                            producer.produce(\n",
    "                                topic=f'evh-active-pub-{env}-{lz_key}-uks-dlrm-01',\n",
    "                                key=current_file_path.encode('utf-8'),\n",
    "                                value=value,\n",
    "                                callback=delivery_report\n",
    "                            )\n",
    "                        except Exception as retry_e:\n",
    "                            logger.error(f\"Failed to produce message after buffer retry: {retry_e}\")\n",
    "                            failure_list.append((current_file_path, current_state_row, \"failure\", f\"Buffer error retry failed: {str(retry_e)}\", datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Unexpected error during production: {e}\")\n",
    "                        failure_list.append((current_file_path, current_state_row, \"failure\", str(e), datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "\n",
    "                # Flush producer and handle any remaining messages\n",
    "                try:\n",
    "                    producer.flush()\n",
    "                    logger.info(\"Producer flushed successfully.\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Unexpected error during flush: {e}\")\n",
    "\n",
    "                # Combine all results\n",
    "                results.extend(success_list)\n",
    "                results.extend(failure_list)\n",
    "\n",
    "                return results\n",
    "\n",
    "            # Define schema for results\n",
    "            from pyspark.sql.types import StructType, StructField, StringType\n",
    "            \n",
    "            schema = StructType([\n",
    "                StructField(\"file_name\", StringType(), True),\n",
    "                StructField(\"state\", StringType(), True),\n",
    "                StructField(\"status\", StringType(), True),\n",
    "                StructField(\"error_message\", StringType(), True),\n",
    "                StructField(\"timestamp\", StringType(), True)\n",
    "            ])\n",
    "\n",
    "            # Process the current state's data\n",
    "            print(f\"üîÑ Starting Kafka processing for state: {current_state}\")\n",
    "            \n",
    "            result_rdd = optimized_html_df.rdd.mapPartitions(process_partition)\n",
    "            result_df = spark.createDataFrame(result_rdd, schema)\n",
    "\n",
    "            # Trigger execution & force completion for this state\n",
    "            kafka_result_count = result_df.count()\n",
    "            print(f\"üìä Kafka processing completed: {kafka_result_count} records for state: {current_state}\")\n",
    "\n",
    "            # Display results for this state\n",
    "            display(result_df)\n",
    "            \n",
    "            # Optional: Save results for this state\n",
    "            # result_df.write.mode(\"append\").saveAsTable(f\"kafka_results_{current_state}\")\n",
    "            \n",
    "            print(f\"‚úÖ Successfully sent {record_count} records to Kafka for state: {current_state}\")\n",
    "            \n",
    "            print(f\"‚úÖ Successfully processed state: {current_state}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error processing state {current_state}: {e}\")\n",
    "    raise  # Re-raise so the workflow task shows as failed\n",
    "\n",
    "print(f\"üéâ Completed processing for state: {current_state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78e836ec-86af-42ea-8708-b6a7c37e60ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Display failed files\n",
    "\n",
    "failed_files = result_df.filter(col(\"status\") == \"failure\")\n",
    "\n",
    "display(failed_files)\n",
    "failed_files.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c089fc4-12f3-46a9-8070-d2545f61451f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Filter over the relevant state as all data is merged together and append each state seperately\n",
    "\n",
    "final_results_df = result_df.coalesce(4)\n",
    "silver_base_path = f\"abfss://{silver_curated_container}@{curated_storage_account}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/{current_state}/publish_audit_db_eh\"\n",
    "    \n",
    "(final_results_df\n",
    "    .filter(col(\"state\") == current_state)\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .save(f\"{silver_base_path}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58fafee9-d66c-45b4-a30d-fe788f2ce346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6274941-a5b2-4633-a1cd-4cd8d404da25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Acknowledge data has been sent to EventHubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "732c7b32-3121-44b9-b895-ba684aa6195c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define schema, EH parameters, Kafka Options"
    }
   },
   "outputs": [],
   "source": [
    "ack_schema = StructType([\n",
    "    StructField(\"file_name\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"error_message\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "EH_NAMESPACE = f\"ingest{lz_key}-integration-eventHubNamespace001-{env}\"\n",
    "EH_NAME = f\"evh-active-pub-{env}-{lz_key}-uks-dlrm-01\" #To create this Eventhub in the UI\n",
    "\n",
    "connection_string = dbutils.secrets.get(keyvault_name, \"RootManageSharedAccessKey\")\n",
    "\n",
    "KAFKA_OPTIONS = {\n",
    "    \"kafka.bootstrap.servers\": f\"{EH_NAMESPACE}.servicebus.windows.net:9093\",\n",
    "    \"subscribe\": EH_NAME,\n",
    "    \"consumer.group.id\": current_state,\n",
    "    # \"startingOffsets\": \"earliest\",\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"failOnDataLoss\": \"false\",\n",
    "    \"startingOffsets\": \"latest\",\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.sasl.jaas.config\": f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{connection_string}\";'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f124680-bcc9-4baf-84d2-be58f75606d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paths specific to this state\n",
    "data_path = f\"abfss://silver@ingest{lz_key}curated{env}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/{current_state}/publish_audit_db_eh/\"\n",
    "checkpoint_path = f\"abfss://db-ack-checkpoint@ingest{lz_key}xcutting{env}.dfs.core.windows.net/{current_state}/ACK/\"\n",
    "\n",
    "print(f\"üìÇ Data path: {data_path}\")\n",
    "print(f\"üìÇ Checkpoint path: {checkpoint_path}\")\n",
    "\n",
    "# Read stream from EventHub (Kafka)\n",
    "eventhubdf = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .options(**KAFKA_OPTIONS)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Parse payload\n",
    "parsed_df = (\n",
    "    eventhubdf\n",
    "    .select(col(\"value\").cast(\"string\").alias(\"json_str\"))\n",
    "    .select(from_json(col(\"json_str\"), ack_schema).alias(\"json_obj\"))\n",
    "    .select(\"json_obj.*\")\n",
    ")\n",
    "\n",
    "# Start streaming write to Delta\n",
    "query = (\n",
    "    parsed_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .outputMode(\"append\")\n",
    "    .start(data_path)\n",
    ")\n",
    "\n",
    "# Wait briefly to allow ingestion, then stop\n",
    "time.sleep(30)\n",
    "query.stop()\n",
    "\n",
    "# Read results back (optional validation step)\n",
    "df = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(data_path)\n",
    "    .filter(col(\"status\").isNotNull())\n",
    ")\n",
    "\n",
    "print(f\"Ack records for {current_state}: {df.count()}\")\n",
    "display(df)\n",
    "\n",
    "print(f\"‚úÖ Acknowledgement processing completed for state: {current_state}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5614f6fe-1de8-4af6-9359-c7d4e584fad4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(f\"{current_state} notebook completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Active_Publish_EH_HTML_JSON",
   "widgets": {
    "current_state": {
     "currentValue": "paymentPending",
     "nuid": "4c98c858-ae9b-4f27-9a91-4776bec72675",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "paymentPending",
      "label": "State to Process",
      "name": "current_state",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "paymentPending",
      "label": "State to Process",
      "name": "current_state",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
