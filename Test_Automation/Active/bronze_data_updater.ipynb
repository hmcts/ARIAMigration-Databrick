{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4f55781a-86c7-47ff-bba4-e4f343863097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Load Config and Setup Enviorment Variables\n",
    "state_under_test = \"paymentPending\"\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "config = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/configs/config.json\")\n",
    "env_name = config.first()[\"env\"].strip().lower()\n",
    "lz_key = config.first()[\"lz_key\"].strip().lower()\n",
    " \n",
    "# print(f\"env_code: {lz_key}\")  # This won't be redacted\n",
    "# print(f\"env_name: {env_name}\")  # This won't be redacted\n",
    " \n",
    "KeyVault_name = f\"ingest{lz_key}-meta002-{env_name}\"\n",
    "# print(f\"KeyVault_name: {KeyVault_name}\")\n",
    " \n",
    "# Service principal credentials\n",
    "client_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-ID\")\n",
    "client_secret = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-SECRET\")\n",
    "tenant_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-TENANT-ID\")\n",
    " \n",
    "# Storage account names\n",
    "curated_storage = f\"ingest{lz_key}curated{env_name}\"\n",
    "checkpoint_storage = f\"ingest{lz_key}xcutting{env_name}\"\n",
    "raw_storage = f\"ingest{lz_key}raw{env_name}\"\n",
    "landing_storage = f\"ingest{lz_key}landing{env_name}\"\n",
    "external_storage = f\"ingest{lz_key}external{env_name}\"\n",
    "  \n",
    "# Spark config for curated storage (Delta table)\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{curated_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{curated_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{curated_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{curated_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{curated_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{checkpoint_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{checkpoint_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{checkpoint_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{checkpoint_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{checkpoint_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{raw_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{raw_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{raw_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{raw_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{raw_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{landing_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{landing_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{landing_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{landing_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{landing_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{external_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{external_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{external_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{external_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{external_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "  \n",
    "# Setting variables for use in subsequent cells\n",
    "bronze_path = f\"abfss://bronze@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/\"\n",
    "silver_path = f\"abfss://silver@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/\"\n",
    "audit_path = f\"abfss://silver@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/AUDIT/{state_under_test}\"\n",
    "gold_path = f\"abfss://gold@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/{state_under_test}\"\n",
    " \n",
    " \n",
    "# Print all variables\n",
    "# variables = {\n",
    "#     # \"read_hive\": read_hive,\n",
    "    \n",
    "#     \"bronze_path\": bronze_path,\n",
    "#     \"silver_path\": silver_path,\n",
    "#     \"audit_path\": audit_path,\n",
    "#     \"gold_path\": gold_path,\n",
    "#     \"key_vault\": KeyVault_name,\n",
    "#     \"AppealState\": state_under_test\n",
    " \n",
    "# }\n",
    " \n",
    "# display(variables)\n",
    "\n",
    "import json\n",
    "\n",
    "#Get Latest Json Folder\n",
    "json_location = dbutils.fs.ls(f\"{gold_path}/\")[-1]\n",
    "latest_json_location = json_location.name\n",
    "dbutils.fs.ls(f\"{gold_path}/{latest_json_location}\")\n",
    "\n",
    "#Set Paths\n",
    "try: \n",
    "    #json_path = f\"{gold_path}/{latest_json_location}/JSON/\"\n",
    "    json_path = f\"{gold_path}/{latest_json_location}/INVALID_JSON/\"\n",
    "    M1_silver = f\"{silver_path}/silver_appealcase_detail\"\n",
    "    M1_bronze = f\"{bronze_path}/bronze_appealcase_crep_rep_floc_cspon_cfs\"\n",
    "    M2_silver = f\"{silver_path}/silver_caseapplicant_detail\"\n",
    "    M3_silver = f\"{silver_path}/silver_status_detail\"\n",
    "    C = f\"{silver_path}/silver_appealcategory_detail\"\n",
    "    bhc = f\"{bronze_path}/bronze_hearing_centres\"\n",
    "    bat = f\"{bronze_path}/bronze_appealtype\" \n",
    "    docsr = f\"{bronze_path}/bronze_documentsreceived\"   \n",
    "    apl_audit = f\"{audit_path}/apl_active_payment_pending_cr_audit_table/\"\n",
    "    sh =  f\"{silver_path}/silver_history_detail\"\n",
    "except:\n",
    "    print(f\"Error during fetch: {str(e)}\")\n",
    "\n",
    "#Create and Load Dataframes\n",
    "json_data = spark.read.format(\"json\").load(json_path)\n",
    "M1_silver = spark.read.format(\"delta\").load(M1_silver)\n",
    "M1_bronze = spark.read.format(\"delta\").load(M1_bronze)\n",
    "M2_silver = spark.read.format(\"delta\").load(M2_silver)\n",
    "M3_silver = spark.read.format(\"delta\").load(M3_silver)\n",
    "C = spark.read.format(\"delta\").load(C)\n",
    "bhc = spark.read.format(\"delta\").load(bhc)\n",
    "bat = spark.read.format(\"delta\").load(bat)\n",
    "docsr = spark.read.format(\"delta\").load(docsr)\n",
    "apl_audit = spark.read.format(\"delta\").load(apl_audit)\n",
    "sh_audit = spark.read.format(\"delta\").load(sh)\n",
    "\n",
    "#Can be removed later, added to allow developing of code in this notebook to begin with before moving to func files\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, lit, array, struct, collect_list, \n",
    "    max as spark_max, date_format, row_number, expr, \n",
    "    size, udf, coalesce, concat_ws, concat, trim, year, split, datediff,\n",
    "    collect_set, current_timestamp,transform, first, array_contains\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29a33535-9aae-430e-b410-f3ce10df1123",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"BirthDate\":316},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770218175778}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "UPDATE BRONZE DATA SCRIPT"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "#UPDATE BRONZE DATA SCRIPT FOR PAYMENT PENDING.\n",
    "#\n",
    "#NOTE: The below code will update bronze data that will not pass the DQ expecation checks due to\n",
    "#issues in the data that will be resolved before live but are needeed to get all the data through the checks\n",
    "#and sent to CCD in the mean time\n",
    "###############################\n",
    "from pyspark.sql.functions import *\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "#################\n",
    "# BirthDate / appellantDateOfBirth\n",
    "\n",
    "bronze_table = DeltaTable.forName(spark,\"ariadm_active_appeals.bronze_appealcase_caseappellant_appellant\")\n",
    "\n",
    "display(bronze_table.toDF().filter(col(\"CaseNo\").isin(\"HU/00278/2025\", \"HU/00455/2025\", \"HU/00472/2025\" )).select(\"CaseNo\", \"BirthDate\"))\n",
    "\n",
    "bronze_table.update(\n",
    "    condition=col(\"CaseNo\").isin(\"HU/00278/2025\", \"HU/00455/2025\", \"HU/00472/2025\"),\n",
    "    set={\"BirthDate\": lit(\"2000-02-01T00:00:00Z\")}\n",
    ")\n",
    "\n",
    "display(bronze_table.toDF().filter(col(\"CaseNo\").isin(\"HU/00278/2025\", \"HU/00455/2025\", \"HU/00472/2025\" )).select(\"CaseNo\", \"BirthDate\"))\n",
    "\n",
    "\n",
    "#################\n",
    "#valid_appellantNationalitiesDescription_not_null\n",
    "#and\n",
    "#valid_appellantNationalities_not_null\n",
    "#NationalityId\n",
    "#Where No mapping required for 201/203\n",
    "\n",
    "bronze_table = DeltaTable.forName(spark, \"ariadm_active_appeals.bronze_appealcase_crep_rep_floc_cspon_cfs\")\n",
    "\n",
    "display(bronze_table.toDF().filter(col(\"CaseNo\").isin(\"HU/00302/2025\", \"HU/00569/2025\", \"HU/00586/2025\",\"HU/00560/2025\" )).select(\"CaseNo\", \"NationalityId\"))\n",
    "\n",
    "bronze_table.update(\n",
    "    condition=col(\"CaseNo\").isin(\"HU/00302/2025\", \"HU/00569/2025\", \"HU/00586/2025\",\"HU/00560/2025\"),\n",
    "    set={\"NationalityId\": lit(\"41\")}\n",
    ")\n",
    "\n",
    "display(bronze_table.toDF().filter(col(\"CaseNo\").isin(\"HU/00302/2025\", \"HU/00569/2025\", \"HU/00586/2025\",\"HU/00560/2025\" )).select(\"CaseNo\", \"NationalityId\"))\n",
    "\n",
    "#################\n",
    "#valid_oocAddressLine1 valid_oocAddressLine2\n",
    "#changing null values to actual values\n",
    "\n",
    "bronze_table = DeltaTable.forName(spark, \"ariadm_active_appeals.bronze_appealcase_crep_rep_floc_cspon_cfs\")\n",
    "display(bronze_table.toDF().filter(col(\"CaseNo\").isin(\"HU/00185/2025\", \"HU/02151/2024\")).select(\"CaseNo\", \"CaseRep_Address1\", \"CaseRep_Address2\", \"CaseRep_Address3\", \"CaseRep_Address4\" , \"CaseRep_Address5\", \"CaseRep_Postcode\"))\n",
    "\n",
    "bronze_table.update(\n",
    "    condition=col(\"CaseNo\").isin(\"HU/00185/2025\", \"HU/02151/2024\"),\n",
    "    set={\"CaseRep_Address1\": lit(\"925 Lisa Plains Apt. 642X\"),\n",
    "         \"CaseRep_Address2\" : lit(\"Hill SquareX\"),\n",
    "        \"CaseRep_Address3\" : lit(\"LynchhavenX\"),\n",
    "        \"CaseRep_Address4\" : lit(\"AustraliaX\"),\n",
    "        \"CaseRep_Address5\" : lit(\"NLX\"),\n",
    "        # \"CaseRep_Postcode\" : lit(\"Hill SquareX\"),\n",
    "         \n",
    "         }\n",
    ")\n",
    "\n",
    "\n",
    "display(bronze_table.toDF().filter(col(\"CaseNo\").isin(\"HU/00185/2025\", \"HU/02151/2024\")).select(\"CaseNo\", \"CaseRep_Address1\", \"CaseRep_Address2\", \"CaseRep_Address3\", \"CaseRep_Address4\" , \"CaseRep_Address5\", \"CaseRep_Postcode\"))\n",
    "\n",
    "\n",
    "#################\n",
    "#valid_oocAddressLine1 valid_oocAddressLine2\n",
    "#change in ooc4 ReunionX to GuamX\n",
    "\n",
    "bronze_table = DeltaTable.forName(spark, \"ariadm_active_appeals.bronze_appealcase_crep_rep_floc_cspon_cfs\")\n",
    "display(bronze_table.toDF().filter(col(\"CaseNo\").isin(\"HU/02191/2024\", \"HU/01475/2024\")).select(\"CaseNo\", \"CaseRep_Address1\", \"CaseRep_Address2\", \"CaseRep_Address3\", \"CaseRep_Address4\" , \"CaseRep_Address5\", \"CaseRep_Postcode\"))\n",
    "\n",
    "bronze_table.update(\n",
    "    condition=col(\"CaseNo\").isin(\"HU/01475/2024\", \"HU/02191/2024\"),\n",
    "    set={\n",
    "        \"CaseRep_Address4\" : lit(\"AustraliaX\")                 \n",
    "         }\n",
    ")\n",
    "\n",
    "\n",
    "display(bronze_table.toDF().filter(col(\"CaseNo\").isin(\"HU/02191/2024\", \"HU/01475/2024\")).select(\"CaseNo\", \"CaseRep_Address1\", \"CaseRep_Address2\", \"CaseRep_Address3\", \"CaseRep_Address4\" , \"CaseRep_Address5\", \"CaseRep_Postcode\"))\n",
    "\n",
    "\n",
    "\n",
    "################################\n",
    "\n",
    "cases_to_update = ['EA/02806/2023',\n",
    "'HU/00575/2025',\n",
    "'HU/00581/2025',\n",
    "'HU/00447/2025',\n",
    "'HU/00574/2023',\n",
    "'HU/00591/2025',\n",
    "'EA/00588/2025',\n",
    "'EA/00551/2025',\n",
    "'HU/00304/2025',\n",
    "'EA/00495/2025',\n",
    "'EA/00560/2025',\n",
    "'EA/06826/2022',\n",
    "'EA/00490/2025',\n",
    "'EA/00554/2025',\n",
    "'EA/01778/2024',\n",
    "'HU/00562/2025',\n",
    "'EA/00552/2025',\n",
    "'HU/00511/2025',\n",
    "'EA/00483/2025',\n",
    "'EA/09676/2022',\n",
    "'EA/00493/2025',\n",
    "'HU/00224/2025',\n",
    "'EA/00496/2025',\n",
    "'EA/00538/2025',\n",
    "'EA/00558/2025',\n",
    "'EA/08372/2022',\n",
    "'HU/00822/2024',\n",
    "'HU/00574/2025',\n",
    "'EA/02065/2024',\n",
    "'HU/00442/2025',\n",
    "'EA/00586/2025',\n",
    "'HU/00590/2025',\n",
    "'HU/00569/2025',\n",
    "'EA/00557/2025',\n",
    "'HU/02346/2024',\n",
    "'EA/00562/2025',\n",
    "'HU/00573/2025',\n",
    "'HU/00571/2025',\n",
    "'EA/01893/2023',\n",
    "'EA/00584/2025',\n",
    "'HU/00579/2025',\n",
    "'HU/00555/2025',\n",
    "'HU/00583/2025',\n",
    "'EA/00591/2025',\n",
    "'EA/00556/2025',\n",
    "'EA/00497/2025',\n",
    "'HU/01972/2023',\n",
    "'EA/00437/2025',\n",
    "'HU/00577/2025',\n",
    "'EA/00585/2025',\n",
    "'HU/00252/2025',\n",
    "'HU/00557/2025',\n",
    "'EA/00485/2025',\n",
    "'HU/00563/2025',\n",
    "'HU/00278/2025',\n",
    "'EA/00559/2025',\n",
    "'EA/00553/2025',\n",
    "'HU/00578/2025',\n",
    "'HU/00445/2025',\n",
    "'HU/00325/2025',\n",
    "'EA/00555/2025',\n",
    "'HU/00572/2025',\n",
    "'HU/00582/2025',\n",
    "'EA/00587/2025',\n",
    "'HU/00638/2024',\n",
    "'HU/00453/2025',\n",
    "\"EA/00289/2025\"\n",
    "]\n",
    "\n",
    "\n",
    "bronze_table = DeltaTable.forName(spark, \"ariadm_active_appeals.bronze_appealcase_caseappellant_appellant\")\n",
    "\n",
    "display(bronze_table.toDF().filter(col(\"CaseNo\").isin(cases_to_update)).select(\"CaseNo\", \"Appellant_Address4\"))\n",
    "\n",
    "bronze_table.update(\n",
    "    condition=col(\"CaseNo\").isin(cases_to_update),\n",
    "    set={\n",
    "        \"Appellant_Address4\" : lit(\"AustraliaX\")                 \n",
    "         }\n",
    ")\n",
    "\n",
    "\n",
    "display(bronze_table.toDF().filter(col(\"CaseNo\").isin(cases_to_update)).select(\"CaseNo\", \"Appellant_Address4\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_data_updater",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
