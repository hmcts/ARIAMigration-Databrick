{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2522e663-16c8-479b-a5d0-cde07c2e4358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## JOH Audit Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cbb73ff-ea97-4d52-8cb2-bef1d6a7c843",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import json\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "from pyspark.sql.window import Window\n",
    "import uuid\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ac1fb8b-ccc4-4fa5-8e6a-4a84d35f11cd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Determine KeyVault Name Based on Workspace URL"
    }
   },
   "outputs": [],
   "source": [
    "# Get the current Databricks workspace URL from Spark configuration\n",
    "workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "\n",
    "# Define mapping of known workspace URLs to Key Vault/environment names\n",
    "workspace_mapping = {\n",
    "    \"adb-3635282203417052.12.azuredatabricks.net\": \"ingest00-meta002-sbox\",\n",
    "    \"adb-376876256300083.3.azuredatabricks.net\": \"ingest01-meta002-sbox\",\n",
    "    \"adb-1879076228317698.18.azuredatabricks.net\": \"ingest02-meta002-sbox\",\n",
    "    \"adb-4305432441461530.10.azuredatabricks.net\": \"ingest00-meta002-stg\",\n",
    "    \"adb-3100629970551492.12.azuredatabricks.net\": \"ingest00-meta002-prod\"\n",
    "}\n",
    "\n",
    "# Fail if the current workspace URL is not found in the mapping\n",
    "if workspace_url not in workspace_mapping:\n",
    "    raise ValueError(f\"Unrecognized Databricks workspace URL: {workspace_url}\")\n",
    "\n",
    "# Retrieve the corresponding Key Vault/environment name\n",
    "KeyVault_name = workspace_mapping[workspace_url]\n",
    "\n",
    "# Print the resolved Key Vault/environment name\n",
    "print(f\"Workspace URL maps to Key Vault: {KeyVault_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "142d8cb5-2884-4de1-afc7-ec20b466155e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Extract Environment Details and Generate KeyVault Name"
    }
   },
   "outputs": [],
   "source": [
    "config = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/configs/config.json\")\n",
    "env_name = config.first()[\"env\"].strip().lower()\n",
    "lz_key = config.first()[\"lz_key\"].strip().lower()\n",
    "\n",
    "print(f\"env_code: {lz_key}\")  # This won't be redacted\n",
    "print(f\"env_name: {env_name}\")  # This won't be redacted\n",
    "\n",
    "KeyVault_name = f\"ingest{lz_key}-meta002-{env_name}\"\n",
    "print(f\"KeyVault_name: {KeyVault_name}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03b4bda5-6803-495e-b6fa-fdee537acb3a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configure SP OAuth"
    }
   },
   "outputs": [],
   "source": [
    "# Service principal credentials\n",
    "client_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-ID\")\n",
    "client_secret = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-SECRET\")\n",
    "tenant_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-TENANT-ID\")\n",
    "\n",
    "# Storage account names\n",
    "curated_storage = f\"ingest{lz_key}curated{env_name}\"\n",
    "\n",
    "# Spark config for curated storage (Delta table)\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{curated_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{curated_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{curated_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{curated_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{curated_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3556cd38-cb70-41f5-b830-e49ec6874f68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "audit_delta_path = f\"abfss://silver@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ARM/AUDIT/JOH/joh_cr_audit_table\"\n",
    "hive_schema = \"ariadm_arm_joh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21d6bc13-19f1-4d4f-93b1-3420aa291f45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def datetime_uuid():\n",
    "    dt_str = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return str(uuid.uuid5(uuid.NAMESPACE_DNS, dt_str))\n",
    "\n",
    "run_id_value = datetime_uuid()\n",
    "\n",
    "audit_schema = StructType([\n",
    "    StructField(\"Run_Id\", StringType(), True),\n",
    "    StructField(\"Unique_Identifier_Desc\", StringType(), True),\n",
    "    StructField(\"Unique_Identifier\", StringType(), True),\n",
    "    StructField(\"Table_Name\", StringType(), True),\n",
    "    StructField(\"Stage_Name\", StringType(), True),\n",
    "    StructField(\"Record_Count\", IntegerType(), True),\n",
    "    StructField(\"Run_DateTime\", TimestampType(), True),\n",
    "    StructField(\"Batch_Id\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"File_Name\", StringType(), True),\n",
    "    StructField(\"Status\", StringType(), True)\n",
    "])\n",
    "\n",
    "def create_audit_df(df: DataFrame, Unique_Identifier_Desc: str,Table_Name: str, Stage_Name: str, description: str, File_Name = False,status = False) -> None:\n",
    "    \"\"\"\n",
    "    Creates an audit DataFrame and writes it to Delta format.\n",
    "\n",
    "    :param df: Input DataFrame from which unique identifiers are extracted.\n",
    "    :param Unique_Identifier_Desc: Column name that acts as a unique identifier.\n",
    "    :param Table_Name: Name of the source table.\n",
    "    :param Stage_Name: Name of the data processing stage.\n",
    "    :param description: Description of the table.\n",
    "    :param additional_columns: options File_Name or Status. List of additional columns to include in the audit DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    dt_desc = datetime.utcnow()\n",
    "\n",
    "    additional_columns = []\n",
    "    if File_Name is True:\n",
    "        additional_columns.append(\"File_Name\")\n",
    "    if status is True:\n",
    "        additional_columns.append(\"Status\")\n",
    "\n",
    "\n",
    "     # Default to an empty list if None   \n",
    "    additional_columns = [col(c) for c in additional_columns if c is not None]  # Filter out None values\n",
    "\n",
    "    audit_df = df.select(col(Unique_Identifier_Desc).alias(\"Unique_Identifier\"),*additional_columns)\\\n",
    "    .withColumn(\"Run_Id\", lit(run_id_value))\\\n",
    "        .withColumn(\"Unique_Identifier_Desc\", lit(Unique_Identifier_Desc))\\\n",
    "            .withColumn(\"Stage_Name\", lit(Stage_Name))\\\n",
    "                .withColumn(\"Table_Name\", lit(Table_Name))\\\n",
    "                    .withColumn(\"Run_DateTime\", lit(dt_desc).cast(TimestampType()))\\\n",
    "                        .withColumn(\"Description\", lit(description))\n",
    "\n",
    "    list_cols = audit_df.columns\n",
    "\n",
    "    final_audit_df = audit_df.groupBy(*list_cols).agg(count(\"*\").cast(IntegerType()).alias(\"Record_Count\"))\n",
    "\n",
    "    # final_audit_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\",\"true\").save(audit_delta_path)\n",
    "    \n",
    "    return final_audit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf4da11e-d865-4a22-8ec7-ae1866b46213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Delta Table Path in Azure Storage\n",
    "\n",
    "\n",
    "if not DeltaTable.isDeltaTable(spark, audit_delta_path):\n",
    "    print(f\"🛑 Delta table '{audit_delta_path}' does not exist. Creating an empty Delta table...\")\n",
    "\n",
    "    # Create an empty DataFrame\n",
    "    empty_df = spark.createDataFrame([], audit_schema)\n",
    "\n",
    "    # Write the empty DataFrame in Delta format to create the table\n",
    "    empty_df.write.format(\"delta\").mode(\"overwrite\").save(audit_delta_path)\n",
    "\n",
    "    print(\"✅ Empty Delta table successfully created in Azure Storage.\")\n",
    "else:\n",
    "    print(f\"⚡ Delta table '{audit_delta_path}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0392391e-e2c0-49dd-9854-59ba0fa9f1c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "audit_params = [\n",
    "        {\n",
    "        \"Unique_Identifier_cols\": [\"AdjudicatorId\"],\n",
    "        \"Table_Name\": \"bronze_adjudicator_et_hc_dnur\",\n",
    "        \"Stage_Name\": \"bronze_stage\",\n",
    "        \"description\": \"Combines adjudicator data with hearing centre, employment terms, and do not use reason details. Provides a standardized view of adjudicators, their designated centres, employment terms, judicial status, and restrictions. Includes metadata such as source files, modification timestamps, and process tracking for auditing.\"\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"AdjudicatorId\"],\n",
    "        \"Table_Name\": \"bronze_johistory_users\",\n",
    "        \"Stage_Name\": \"bronze_stage\",\n",
    "        \"description\": \"Combines JoHistory records with user details, providing historical adjudicator activity along with corresponding user information. Includes comments, user names, audit timestamps, source filenames, and process metadata for tracking and auditing.\"\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"AdjudicatorId\"],\n",
    "        \"Table_Name\": \"bronze_othercentre_hearingcentre\",\n",
    "        \"Stage_Name\": \"bronze_stage\",\n",
    "        \"description\": \"Combines OtherCentre records with HearingCentre details, linking adjudicators to their assigned hearing centres. Includes metadata such as timestamps, source files, and process tracking for auditing and traceability.\"\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"AdjudicatorId\"],\n",
    "        \"Table_Name\": \"bronze_adjudicator_role\",\n",
    "        \"Stage_Name\": \"bronze_stage\",\n",
    "        \"description\": \"Filters and segments adjudicators, retaining only those who are not assigned roles 7 or 8, or have no assigned role. Uses data from bronze_adjudicator_et_hc_dnur and bronze_adjudicator_role to generate a complete list of adjudicators.\"\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"AdjudicatorId\"],\n",
    "        \"Table_Name\": \"stg_joh_filtered\",\n",
    "        \"Stage_Name\": \"segmentation_stage\",\n",
    "        \"description\": \"Filters and segments adjudicators, retaining only those who are not assigned roles 7 or 8, or have no assigned role. Uses data from bronze_adjudicator_et_hc_dnur and bronze_adjudicator_role to generate a complete list of adjudicators.\"\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"AdjudicatorId\"],\n",
    "        \"Table_Name\": \"silver_adjudicator_detail\",\n",
    "        \"Stage_Name\": \"silver_stage\",\n",
    "        \"description\": \"Filters adjudicators based on segmentation criteria and enriches their records with Hearing Centre and Do Not Use Reason (DNUR) details. Standardizes key attributes such as correspondence address, contact details, employment terms, and judicial status.\"\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"AdjudicatorId\"],\n",
    "        \"Table_Name\": \"silver_history_detail\",\n",
    "        \"Stage_Name\": \"silver_stage\",\n",
    "        \"description\": \"Filters and enhances historical adjudicator activity records by incorporating user details. Maps history types to their corresponding descriptions, providing a structured view of events such as allocations, case updates, and administrative actions.\"\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"AdjudicatorId\"],\n",
    "        \"Table_Name\": \"silver_othercentre_detail\",\n",
    "        \"Stage_Name\": \"silver_stage\",\n",
    "        \"description\": \"Filters and enhances OtherCentre records by applying adjudicator segmentation criteria. Retains adjudicators linked to Hearing Centres while ensuring completeness through process metadata, timestamps, and source tracking.\"\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"AdjudicatorId\"],\n",
    "        \"Table_Name\": \"silver_appointment_detail\",\n",
    "        \"Stage_Name\": \"silver_stage\",\n",
    "        \"description\": \"Filters and enhances adjudicator role records by mapping role codes to their descriptions. Provides a structured view of adjudicator appointments, including start and end dates, and metadata for auditing.\"\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"client_identifier\"],\n",
    "        \"Table_Name\": \"silver_archive_metadata\",\n",
    "        \"Stage_Name\": \"silver_stage\",\n",
    "        \"description\": \"Metadata table of adjudicator records by combining various metadata fields. Provides a structured view of adjudicator details, including event dates, region, and other relevant information for archival purposes.\"\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"AdjudicatorId\"],\n",
    "        \"Table_Name\": \"stg_judicial_officer_combined\",\n",
    "        \"Stage_Name\": \"staging_stage\",\n",
    "        \"description\": \"Metadata table of adjudicator records by combining various metadata fields. Provides a structured view of adjudicator details, including event dates, region, and other relevant information for archival purposes.\"\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"AdjudicatorId\"],\n",
    "        \"Table_Name\": \"stg_create_joh_json_content\",\n",
    "        \"Stage_Name\": \"staging_stage\",\n",
    "        \"description\": \"Generates JSON-formatted adjudicator records for gold-level outputs. Creates structured JSON content for each adjudicator and assigns a filename. Tracks JSON creation status to identify failures and successful transformations\",\n",
    "        \"extra_columns\": [\"File_Name\", \"Status\"]\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"AdjudicatorId\"],\n",
    "        \"Table_Name\": \"stg_create_joh_html_content\",\n",
    "        \"Stage_Name\": \"staging_stage\",\n",
    "        \"description\": \"Generates HTML-formatted adjudicator records for gold-level outputs. Uses a UDF to transform data into structured HTML content and assigns a filename. Tracks HTML creation status to identify failures and succe\",\n",
    "        \"extra_columns\": [\"File_Name\", \"Status\"]\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"client_identifier\"],\n",
    "        \"Table_Name\": \"stg_create_joh_a360_content\",\n",
    "        \"Stage_Name\": \"staging_stage\",\n",
    "        \"description\": \"Generates A360-formatted adjudicator records for gold-level outputs. Uses a UDF to transform metadata into A360 content and assigns processing statuses. Supports Hive-based retrieval for non-initial loads, ensuring comprehensive archival integration.\",\n",
    "        \"extra_columns\": [\"Status\"]\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"AdjudicatorId\"],\n",
    "        \"Table_Name\": \"gold_judicial_officer_with_json\",\n",
    "        \"Stage_Name\": \"Gold_stage\",\n",
    "        \"description\": \"Final gold-level table integrating adjudicator records with validated HTML content. Ensures data integrity by enforcing error-free JSON content. Optimizes processing through repartitioning and triggers upload operations for structured archival and distribution.\",\n",
    "        \"extra_columns\": [\"File_Name\", \"Status\"]\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"AdjudicatorId\"],\n",
    "        \"Table_Name\": \"gold_judicial_officer_with_html\",\n",
    "        \"Stage_Name\": \"Gold_stage\",\n",
    "        \"description\": \"Final gold-level table integrating adjudicator records with validated JSON content. Ensures data integrity by enforcing error-free HTML content. Optimizes processing through repartitioning and triggers upload operations for structured archival and distribution.\",\n",
    "        \"extra_columns\": [\"File_Name\", \"Status\"]\n",
    "    },\n",
    "    {\n",
    "        \"Unique_Identifier_cols\": [\"A360_BatchId\"],\n",
    "        \"Table_Name\": \"gold_judicial_officer_with_a360\",\n",
    "        \"Stage_Name\": \"Gold_stage\",\n",
    "        \"description\": \"Final gold-level table consolidating adjudicator A360 content for structured archival and processing. Ensures data integrity by filtering out records with errors in A360 content. Aggregates and batches records, optimizes processing through repartitioning, and triggers upload operations.\",\n",
    "        \"extra_columns\": [\"File_Name\", \"Status\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "audit_dataframes = []\n",
    "\n",
    "for params in audit_params:\n",
    "    Table_Name = params[\"Table_Name\"]\n",
    "    Stage_Name = params[\"Stage_Name\"]\n",
    "    Unique_Identifier_cols = params[\"Unique_Identifier_cols\"]\n",
    "    description = params[\"description\"]\n",
    "    extra_columns = params[\"extra_columns\"] if \"extra_columns\" in params else []\n",
    "    Unique_Identifier_Desc = \"_\".join(Unique_Identifier_cols)\n",
    "\n",
    "    try:\n",
    "\n",
    "        df_logging = spark.read.table(f\"hive_metastore.{hive_schema}.{Table_Name}\")\n",
    "\n",
    "        df_audit = df_logging\n",
    "        if len(Unique_Identifier_cols) > 1:\n",
    "            df_audit = df_audit.withColumn(\n",
    "                Unique_Identifier_Desc, \n",
    "                concat_ws(\"_\", *[col(c).cast(\"string\") for c in Unique_Identifier_cols])\n",
    "            )\n",
    "        else:\n",
    "            df_audit = df_audit.withColumn(Unique_Identifier_Desc, col(Unique_Identifier_Desc).cast(\"string\"))\n",
    "\n",
    "     \n",
    "        # Apply extra column mappings dynamically\n",
    "        if len(extra_columns) <= 1:\n",
    "            missing_columns = list(set([\"File_Name\", \"Status\"]) - set(extra_columns))\n",
    "            for new_col in missing_columns:\n",
    "                df_audit = df_audit.withColumn(new_col, lit(None))\n",
    "\n",
    "        # Generate the audit DataFrame\n",
    "        df_audit_appended = create_audit_df(\n",
    "            df_audit,\n",
    "            Unique_Identifier_Desc=Unique_Identifier_Desc,\n",
    "            Table_Name=Table_Name,\n",
    "            Stage_Name=Stage_Name,\n",
    "            description=description,\n",
    "            File_Name = True,\n",
    "            status = True\n",
    "        )\n",
    "\n",
    "        audit_dataframes.append(df_audit_appended)\n",
    "\n",
    "        print(f\"✅ Successfully processed table: {Table_Name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"🛑 Failed to process table: {Table_Name}. Error: {str(e)}\")\n",
    "        failed_table = f\"Table {Table_Name} does not exist\"\n",
    "\n",
    "\n",
    "        # Table does not exist, create an audit entry for it\n",
    "        status = f\"Failed - Table {Table_Name} does not exist\"\n",
    "\n",
    "        row_data = {\n",
    "            \"Run_Id\": run_id_value,\n",
    "            \"Unique_Identifier_Desc\": Unique_Identifier_Desc,\n",
    "            \"Unique_Identifier\": None,\n",
    "            \"Table_Name\": Table_Name,\n",
    "            \"Stage_Name\": Stage_Name,\n",
    "            \"Record_Count\": 0,\n",
    "            \"Run_DateTime\": datetime.now(),\n",
    "            \"Batch_Id\": None,\n",
    "            \"Description\": description,\n",
    "            \"File_Name\": None,\n",
    "            \"Status\": status\n",
    "        }\n",
    "\n",
    "        row_df = spark.createDataFrame([row_data], schema=audit_schema)\n",
    "        audit_dataframes.append(row_df)\n",
    "\n",
    "\n",
    "df_final_audit = audit_dataframes[0]\n",
    "for df in audit_dataframes[1:]:\n",
    "    df_final_audit = df_final_audit.unionByName(df, allowMissingColumns=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb545dd2-e117-4f63-b601-44f8cacc2b2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final_audit.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(audit_delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72a503c6-da5f-4c9c-be65-9575e11a69c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Notebook completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a48e666d-b58a-4670-ae05-fe6dc93d0747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb07e90c-3a47-4c4a-bf87-74855d514e37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# df_final_audit.createOrReplaceTempView(\"tv_final_audit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71e4e88f-68c9-4dd9-8d9e-0a26800bfec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_final_audit = spark.read.format(\"delta\").load(audit_delta_path)\n",
    "# df_final_audit.createOrReplaceTempView(\"tv_final_audit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31616534-a183-4389-8bb8-3e2329cff376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# %sql\n",
    "# select *\n",
    "# --  Status, count(*) \n",
    "#  from tv_final_audit\n",
    "# where Table_Name like 'gold_judicial_officer_with_json%'\n",
    "# -- group by all"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7954082128872396,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ARIADM_ARM_AUDIT_DATA_PROCESSING_JOH",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
