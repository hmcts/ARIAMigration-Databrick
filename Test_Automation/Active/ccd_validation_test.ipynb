{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "520de911-dc99-42a3-96dd-b78f20a3e8d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Setup cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fd920608-506a-4e4f-864f-a46869227417",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-monitor-query\n",
    "%pip install azure-identity\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c803dba3-11d7-4712-8ec1-e5190d7b4421",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config_path = \"dbfs:/configs/config.json\"\n",
    "config = spark.read.option(\"multiline\", \"true\").json(config_path)\n",
    "first_row = config.first()\n",
    "env = first_row[\"env\"].strip().lower()\n",
    "lz_key = first_row[\"lz_key\"].strip().lower()\n",
    "keyvault_name = f\"ingest{lz_key}-meta002-{env}\"\n",
    "client_secret = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-SECRET')\n",
    "tenant_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-TENANT-ID')\n",
    "client_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-ID') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e00d9564-2c84-4eff-9259-1c1a816d2f09",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Specifying parameters"
    }
   },
   "outputs": [],
   "source": [
    "from azure.identity import ClientSecretCredential\n",
    "from azure.monitor.query import LogsQueryClient, LogsQueryStatus\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "desiredState = \"pendingPayment\"\n",
    "end_time = datetime.utcnow()\n",
    "start_time = end_time - timedelta(days=2)\n",
    "\n",
    "AppealState = \"paymentPending\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff46e3cf-bc96-4832-b176-5c027dde1dfe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Query Traces generated"
    }
   },
   "outputs": [],
   "source": [
    "from azure.identity import ClientSecretCredential\n",
    "from azure.monitor.query import LogsQueryClient, LogsQueryStatus\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "credential = ClientSecretCredential(\n",
    "    tenant_id=tenant_id,\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret\n",
    ")\n",
    "\n",
    "client = LogsQueryClient(credential)\n",
    "workspace_id = \"9db45d41-bfe4-49dd-9a3b-5da0ab1a95d0\"\n",
    "\n",
    "# First, let's see which tables have data in the last 5 hours\n",
    "query = \"\"\"\n",
    "search *\n",
    "| where TimeGenerated > ago(1d)\n",
    "| summarize count() by $table\n",
    "| sort by count_ desc\n",
    "\"\"\"\n",
    "\n",
    "response = client.query_workspace(\n",
    "    workspace_id,\n",
    "    query,\n",
    "    timespan=(start_time, end_time)\n",
    ")\n",
    "\n",
    "if response.status == LogsQueryStatus.SUCCESS:\n",
    "    print(\"Tables with data in the last 3 days:\\n\")\n",
    "    for table in response.tables:\n",
    "        for row in table.rows:\n",
    "            print(f\"Table: {row[0]}, Count: {row[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4f8ef75-f1fe-4bf1-a089-05320e70e5f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Gathering data from the logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cf63c6a-79a7-4068-9f5e-7ed78aa94326",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Return messages with \"Validate posting payload\" present"
    }
   },
   "outputs": [],
   "source": [
    "from azure.identity import ClientSecretCredential\n",
    "from azure.monitor.query import LogsQueryClient, LogsQueryStatus\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "credential = ClientSecretCredential(\n",
    "    tenant_id=tenant_id,\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret\n",
    ")\n",
    "\n",
    "client = LogsQueryClient(credential)\n",
    "workspace_id = \"9db45d41-bfe4-49dd-9a3b-5da0ab1a95d0\"\n",
    "\n",
    "# Search AppTraces for your specific message\n",
    "query = \"\"\"\n",
    "AppTraces\n",
    "| where TimeGenerated > ago(1d)\n",
    "| where Message contains \"Validate posting payload\"\n",
    "| project TimeGenerated, Message\n",
    "| order by TimeGenerated desc\n",
    "| take 100\n",
    "\"\"\"\n",
    "\n",
    "response = client.query_workspace(\n",
    "    workspace_id,\n",
    "    query,\n",
    "    timespan=(start_time, end_time)\n",
    ")\n",
    "\n",
    "if response.status == LogsQueryStatus.SUCCESS:\n",
    "    for table in response.tables:\n",
    "        print(f\"Found {len(table.rows)} matching messages\\n\")\n",
    "        \n",
    "        for i, row in enumerate(table.rows[:10]):\n",
    "            timestamp = row[0]\n",
    "            message = row[1]\n",
    "            print(f\"\\nMessage {i+1}\")\n",
    "            print(f\"Time: {timestamp}\")\n",
    "            print(f\"Message: {message}\")\n",
    "else:\n",
    "    print(f\"Query failed: {response.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7181677f-30d5-4c8f-aea0-3d773519b599",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Validation Query to CCD"
    }
   },
   "outputs": [],
   "source": [
    "from azure.identity import ClientSecretCredential\n",
    "from azure.monitor.query import LogsQueryClient, LogsQueryStatus\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "credential = ClientSecretCredential(\n",
    "    tenant_id=tenant_id,\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret\n",
    ")\n",
    "\n",
    "client = LogsQueryClient(credential)\n",
    "workspace_id = \"9db45d41-bfe4-49dd-9a3b-5da0ab1a95d0\"\n",
    "\n",
    "query_validate = \"\"\"\n",
    "AppTraces\n",
    "| where TimeGenerated > ago(1d)\n",
    "| where Message contains \"Validate posting payload\"\n",
    "| project TimeGenerated, Message\n",
    "| order by TimeGenerated desc\n",
    "| take 1400\n",
    "\"\"\"\n",
    "\n",
    "response = client.query_workspace(\n",
    "    workspace_id,\n",
    "    query_validate,\n",
    "    timespan=(start_time, end_time)\n",
    ")\n",
    "\n",
    "if response.status == LogsQueryStatus.SUCCESS:\n",
    "    for table in response.tables:\n",
    "        # Convert to DataFrame\n",
    "        df_validate = pd.DataFrame(\n",
    "            data=table.rows,\n",
    "            columns=table.columns\n",
    "        )\n",
    "        \n",
    "else:\n",
    "    print(f\"Query failed: {response.status}\")\n",
    "    df_validate = pd.DataFrame()\n",
    "\n",
    "df_validate.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c3aab68-e30c-4b83-81da-43f15a859570",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769785952892}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Extracting all {desiredState}  payload values"
    }
   },
   "outputs": [],
   "source": [
    "# extract the payload and filter for 'pendingPayment'\n",
    "df_validate['payload_validation'] = df_validate['Message'].str.split(\"json = \", n=1).str[1]\n",
    "df_validate['payload_validation'] = df_validate['payload_validation'].str.split(\"'event_token'\", n=1).str[0]\n",
    "\n",
    "# # filter rows where ariaDesiredState is pendingPayment\n",
    "df_validate = df_validate[\n",
    "    df_validate['payload_validation'].str.contains(f\"'ariaDesiredState': '{desiredState}' \", na=False)\n",
    "].copy()\n",
    "\n",
    "# extract CaseNo\n",
    "df_validate['CaseNo'] = df_validate['Message'].str.extract(r\"'appealReferenceNumber':\\s*'([A-Z]+/\\d+/\\d+)'\")\n",
    "\n",
    "# rename\n",
    "df_validate = df_validate.rename(columns={'TimeGenerated': 'payload_validation_time'})\n",
    "\n",
    "# DISTINCT LOGIC: Keep the latest record for each CaseNo - sort by time descending, then drop duplicates on CaseNo\n",
    "df_validate = df_validate.sort_values('payload_validation_time', ascending=False)\n",
    "df_validate = df_validate.drop_duplicates(subset=['CaseNo'])\n",
    "\n",
    "# final selection\n",
    "df_validate = df_validate[['CaseNo', 'payload_validation', 'payload_validation_time']]\n",
    "df_validate.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5da9349-fa36-4379-b9f9-cb52a52cbf78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Gathering gold layer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b628c526-6d70-4ef9-a445-c122248be4e3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Oauth to fetch gold path info"
    }
   },
   "outputs": [],
   "source": [
    "config = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/configs/config.json\")\n",
    "env_name = config.first()[\"env\"].strip().lower()\n",
    "lz_key = config.first()[\"lz_key\"].strip().lower()\n",
    " \n",
    "print(f\"env_code: {lz_key}\")  # This won't be redacted\n",
    "print(f\"env_name: {env_name}\")  # This won't be redacted\n",
    " \n",
    "KeyVault_name = f\"ingest{lz_key}-meta002-{env_name}\"\n",
    "print(f\"KeyVault_name: {KeyVault_name}\")\n",
    " \n",
    "# Service principal credentials\n",
    "client_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-ID\")\n",
    "client_secret = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-SECRET\")\n",
    "tenant_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-TENANT-ID\")\n",
    " \n",
    "# Storage account names\n",
    "curated_storage = f\"ingest{lz_key}curated{env_name}\"\n",
    "checkpoint_storage = f\"ingest{lz_key}xcutting{env_name}\"\n",
    "raw_storage = f\"ingest{lz_key}raw{env_name}\"\n",
    "landing_storage = f\"ingest{lz_key}landing{env_name}\"\n",
    "external_storage = f\"ingest{lz_key}external{env_name}\"\n",
    " \n",
    " \n",
    "# Spark config for curated storage (Delta table)\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{curated_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{curated_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{curated_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{curated_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{curated_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{checkpoint_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{checkpoint_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{checkpoint_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{checkpoint_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{checkpoint_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{raw_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{raw_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{raw_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{raw_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{raw_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{landing_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{landing_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{landing_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{landing_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{landing_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{external_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{external_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{external_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{external_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{external_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    "# Setting variables for use in subsequent cells\n",
    "bronze_path = f\"abfss://bronze@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/\"\n",
    "silver_path = f\"abfss://silver@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/\"\n",
    "audit_path = f\"abfss://silver@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/AUDIT/{AppealState}\"\n",
    "gold_path = f\"abfss://gold@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/{AppealState}\"\n",
    " \n",
    "# Print all variables\n",
    "variables = {\n",
    "    # \"read_hive\": read_hive,\n",
    "    \n",
    "    \"bronze_path\": bronze_path,\n",
    "    \"silver_path\": silver_path,\n",
    "    \"audit_path\": audit_path,\n",
    "    \"gold_path\": gold_path,\n",
    "    \"key_vault\": KeyVault_name,\n",
    "    \"AppealState\": AppealState\n",
    " \n",
    "}\n",
    " \n",
    "display(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "432c2ef5-847e-4334-859d-d07b183cfae9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Defining JSON to test"
    }
   },
   "outputs": [],
   "source": [
    "files = dbutils.fs.ls(gold_path)[-1].path\n",
    "valid_json = files + \"/JSON\"\n",
    "json =  dbutils.fs.ls(valid_json)\n",
    "\n",
    "# print(json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca05fbfd-2cb8-43aa-b621-2c5cf84386ef",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769616153326}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Assembling JSON into a df for validation payloads test"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import input_file_name, regexp_extract, regexp_replace, col\n",
    "\n",
    "# 1. Extract the paths from your FileInfo list\n",
    "path_list = [f.path for f in json]\n",
    "\n",
    "# 2. Read JSONs and grab the filename\n",
    "raw_df = spark.read.option(\"multiLine\", \"true\").json(path_list).withColumn(\"raw_filename\", input_file_name())\n",
    "\n",
    "# 3. Extract and Reformat the Reference Number\n",
    "# First, we pull 'EA_00366_2025' from the string\n",
    "# Then, we replace underscores with slashes\n",
    "final_df = raw_df.withColumn(\n",
    "    \"appealReferenceNumber\", \n",
    "    regexp_replace(\n",
    "        regexp_extract(col(\"raw_filename\"), r\"APPEALS_(.*)\\.json\", 1),\n",
    "        \"_\", \n",
    "        \"/\"\n",
    "    )\n",
    ").drop(\"raw_filename\")\n",
    "\n",
    "# 4. Final Output\n",
    "final_df.select(\"appealReferenceNumber\", \"*\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce2b50e8-5677-4815-b5a4-a125425e089f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49695098-779a-45a8-b566-18199dac457d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Gold vs validation data comparison test"
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# matching case numbers\n",
    "gold_cases = {row['appealReferenceNumber']: row.asDict() for row in final_df.collect()}\n",
    "log_pdf = df_validate[['payload_validation']]\n",
    "\n",
    "mismatches = []\n",
    "matches = []\n",
    "\n",
    "# loop through all the log data & check it against the gold\n",
    "for _, row in log_pdf.iterrows():\n",
    "    # convert log string into dictionary\n",
    "    try:\n",
    "        log_data_full = ast.literal_eval(row['payload_validation'])\n",
    "        log_data = log_data_full.get('data', {})\n",
    "    except:\n",
    "        continue \n",
    "    \n",
    "    case_no = log_data.get('appealReferenceNumber')\n",
    "    \n",
    "    # if case number matches one in gold, compare them\n",
    "    if case_no in gold_cases:\n",
    "        gold_row = gold_cases[case_no]\n",
    "        \n",
    "        # check every field that exists in Gold\n",
    "        for field, gold_value in gold_row.items():\n",
    "            log_value = log_data.get(field)\n",
    "            \n",
    "            # convert both to strings to ensure a fair comparison (handles numbers/dates)\n",
    "            if str(gold_value) != str(log_value):\n",
    "                mismatches.append({\n",
    "                    \"CaseNo\": case_no,\n",
    "                    \"Field\": field,\n",
    "                    \"Gold_Value\": gold_value,\n",
    "                    \"Log_Value\": log_value\n",
    "                })\n",
    "            else:\n",
    "                matches.append({\n",
    "                    \"CaseNo\": case_no,\n",
    "                    \"Field\": field,\n",
    "                    \"Gold_Value\": gold_value,\n",
    "                    \"Log_Value\": log_value\n",
    "                })\n",
    "\n",
    "# 4. Display Results\n",
    "import pandas as pd\n",
    "if mismatches:\n",
    "    fails_df = pd.DataFrame(mismatches)\n",
    "    pass_df = pd.DataFrame(matches)\n",
    "    print(f\"Test failed - found {len(mismatches)} mismatches across the cases sent for validation.\")\n",
    "    display(fails_df)\n",
    "else:\n",
    "    print(\"Test pass - all matching case numbers have identical data.\")\n",
    "    display(pass_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ccd_validation_test",
   "widgets": {
    "fields_to_exclude": {
     "currentValue": "",
     "nuid": "3b6a759d-cdee-458a-a51b-56f299044d38",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "fields_to_exclude",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "fields_to_exclude",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "state_under_test": {
     "currentValue": "",
     "nuid": "e3416af5-bf7e-4fce-a3c5-ace8eb3808ab",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "state_under_test",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "state_under_test",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
