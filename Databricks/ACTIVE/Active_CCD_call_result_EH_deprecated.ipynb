{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bb3c5cf-b229-4aee-a829-67b51da04fa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "import json\n",
    "from  itertools import islice\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import col, decode, split, element_at, udf, lit, reduce, from_json\n",
    "import logging\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DateType\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark import SparkContext\n",
    "import os\n",
    "from functools import reduce\n",
    "import time\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0245e09f-90a2-4134-8fff-3a39d6fd5cc1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initialise logging"
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"DatabricksWorkflow\")\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "if not logger.hasHandlers():\n",
    "    logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a633bbde-145e-4063-a90a-13aa7107d099",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set-up configs"
    }
   },
   "outputs": [],
   "source": [
    "config_path = \"dbfs:/configs/config.json\"\n",
    "try:\n",
    "    config = spark.read.option(\"multiline\", \"true\").json(config_path)\n",
    "    logger.info(f\"Successfully read config file from {config_path}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not read config file at {config_path}: {e}\", exc_info=True)\n",
    "    raise FileNotFoundError(f\"Could not read config file at {config_path}: {e}\")\n",
    "\n",
    "try:\n",
    "    first_row = config.first()\n",
    "    env = first_row[\"env\"].strip().lower()\n",
    "    lz_key = first_row[\"lz_key\"].strip().lower()\n",
    "    logger.info(f\"Extracted configs: env={env}, lz_key={lz_key}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Missing expected keys 'env' or 'lz_key' in config file: {e}\", exc_info=True)\n",
    "    raise KeyError(f\"Missing expected keys 'env' or 'lz_key' in config file: {e}\")\n",
    "\n",
    "try:\n",
    "    keyvault_name = f\"ingest{lz_key}-meta002-{env}\"\n",
    "    logger.info(f\"Constructed keyvault name: {keyvault_name}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error constructing keyvault name: {e}\", exc_info=True)\n",
    "    raise ValueError(f\"Error constructing keyvault name: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dadbe708-794d-4c07-8922-afdef6c25f50",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set-up OAuth"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    client_secret = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-SECRET')\n",
    "    logger.info(\"Successfully retrieved SERVICE-PRINCIPLE-CLIENT-SECRET from Key Vault\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not retrieve 'SERVICE-PRINCIPLE-CLIENT-SECRET' from Key Vault '{keyvault_name}': {e}\", exc_info=True)\n",
    "    raise KeyError(f\"Could not retrieve 'SERVICE-PRINCIPLE-CLIENT-SECRET' from Key Vault '{keyvault_name}': {e}\")\n",
    "\n",
    "try:\n",
    "    tenant_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-TENANT-ID')\n",
    "    logger.info(\"Successfully retrieved SERVICE-PRINCIPLE-TENANT-ID from Key Vault\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not retrieve 'SERVICE-PRINCIPLE-TENANT-ID' from Key Vault '{keyvault_name}': {e}\", exc_info=True)\n",
    "    raise KeyError(f\"Could not retrieve 'SERVICE-PRINCIPLE-TENANT-ID' from Key Vault '{keyvault_name}': {e}\")\n",
    "\n",
    "try:\n",
    "    client_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-ID')\n",
    "    logger.info(\"Successfully retrieved SERVICE-PRINCIPLE-CLIENT-ID from Key Vault\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not retrieve 'SERVICE-PRINCIPLE-CLIENT-ID' from Key Vault '{keyvault_name}': {e}\", exc_info=True)\n",
    "    raise KeyError(f\"Could not retrieve 'SERVICE-PRINCIPLE-CLIENT-ID' from Key Vault '{keyvault_name}': {e}\")\n",
    "\n",
    "logger.info(\"âœ… Successfully retrieved all Service Principal secrets from Key Vault\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "002e771a-b124-4cee-b3e5-8b0f9abd3bd9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Assign OAuth"
    }
   },
   "outputs": [],
   "source": [
    "# --- Parameterise containers ---\n",
    "curated_storage_account = f\"ingest{lz_key}curated{env}\"\n",
    "curated_container = \"gold\"\n",
    "silver_curated_container = \"silver\"\n",
    "checkpoint_storage_account = f\"ingest{lz_key}xcutting{env}\"\n",
    "\n",
    "# --- Assign OAuth to storage accounts ---\n",
    "storage_accounts = [curated_storage_account, checkpoint_storage_account]\n",
    "\n",
    "for storage_account in storage_accounts:\n",
    "    try:\n",
    "        configs = {\n",
    "            f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\": \"OAuth\",\n",
    "            f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\":\n",
    "                \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "            f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\": client_id,\n",
    "            f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\": client_secret,\n",
    "            f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\":\n",
    "                f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "        }\n",
    "\n",
    "        for key, val in configs.items():\n",
    "            try:\n",
    "                spark.conf.set(key, val)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to set Spark config '{key}' for storage account '{storage_account}': {e}\", exc_info=True)\n",
    "                raise RuntimeError(f\"Failed to set Spark config '{key}' for storage account '{storage_account}': {e}\")\n",
    "\n",
    "        logger.info(f\"âœ… Successfully configured OAuth for storage account: {storage_account}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error configuring OAuth for storage account '{storage_account}': {e}\", exc_info=True)\n",
    "        raise RuntimeError(f\"Error configuring OAuth for storage account '{storage_account}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "779690b2-69f5-4c95-bb38-6b0b091fd088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "# Kafka / paths setup as you have\n",
    "data_path = f\"abfss://silver@ingest{lz_key}curated{env}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/AUDIT/APPEALS/all_active_states/ack_audit\"\n",
    "checkpoint_path = f\"abfss://db-ack-checkpoint@ingest{lz_key}xcutting{env}.dfs.core.windows.net/APPEALS/all_active_states/ACK/\"\n",
    "\n",
    "# Keep schema exactly as it exists in Pub notebook\n",
    "schema = StructType([\n",
    "    StructField(\"RunID\", StringType(), True),\n",
    "    StructField(\"CaseNo\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"StartDateTime\", TimestampType(), True),\n",
    "    StructField(\"EndDateTime\", TimestampType(), True), #need to convert this to timestamp type\n",
    "    StructField(\"CCDCaseID\", StringType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"Error\", StringType(), True),\n",
    "])\n",
    "\n",
    "EH_NAMESPACE = f\"ingest{lz_key}-integration-eventHubNamespace001-{env}\"\n",
    "EH_NAME = f\"evh-active-res-{env}-{lz_key}-uks-dlrm-01\"\n",
    "\n",
    "connection_string = dbutils.secrets.get(keyvault_name, \"RootManageSharedAccessKey\")\n",
    "\n",
    "KAFKA_OPTIONS = {\n",
    "    \"kafka.bootstrap.servers\": f\"{EH_NAMESPACE}.servicebus.windows.net:9093\",\n",
    "    \"subscribe\": EH_NAME,\n",
    "    \"startingOffsets\": \"earliest\",\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"failOnDataLoss\": \"false\",\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.sasl.jaas.config\": f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{connection_string}\";'\n",
    "}\n",
    "\n",
    "# --- Start the stream ---\n",
    "try: \n",
    "    eventhubdf = spark.readStream.format(\"kafka\").options(**KAFKA_OPTIONS).load()\n",
    "\n",
    "    display(eventhubdf)\n",
    "\n",
    "    parsed_df = (\n",
    "        eventhubdf\n",
    "        .select(col(\"value\").cast(\"string\").alias(\"json_str\"))\n",
    "        .select(from_json(col(\"json_str\"), schema).alias(\"json_obj\"))\n",
    "        .select(\"json_obj.*\")\n",
    "    )\n",
    "\n",
    "    display(parsed_df)\n",
    "\n",
    "    query = parsed_df.writeStream \\\n",
    "        .format(\"delta\") \\\n",
    "        .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .start(data_path)\n",
    "    \n",
    "    logger.info(\"Streaming query started successfully. Waiting for data...\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(\"Error starting the stream: %s\", str(e))\n",
    "    logger.error(traceback.format_exc())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c48c8f80-ed74-470e-adc8-7ca906d4ae4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"state\", \"paymentPending\", \"State to Process\") # change this once integrated into pipeline back to default\n",
    "state = dbutils.widgets.get(\"state\")\n",
    "logger.info(f\"ðŸ”„ Processing state: {state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "707d00aa-4b96-4f9a-84eb-88e7dfe178cd",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762868965252}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "CCD Publish Payload to CCD Call Result"
    }
   },
   "outputs": [],
   "source": [
    "ccd_response = spark.read.format(\"delta\").load(f\"abfss://silver@ingest{lz_key}curated{env}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/AUDIT/APPEALS/all_active_states/ack_audit\")\n",
    "ccd_response.createOrReplaceTempView(\"ccd_response\")\n",
    "\n",
    "publish_payload_to_ccd = spark.read.format(\"delta\").load(f\"abfss://{silver_curated_container}@{curated_storage_account}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/{state}/publish_audit_db_eh\")\n",
    "\n",
    "publish_payload_to_ccd.createOrReplaceTempView(\"publish_payload_to_ccd\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT \n",
    "          \n",
    "        COALESCE(t2.runid, t1.runid) as RunID,\n",
    "        COALESCE(t2.state, t1.state) as State,\n",
    "        COALESCE(t2.CaseNo, t1.CaseNo) as CaseNo,\n",
    "        t1.status as `Publish Payload Status`,\n",
    "        t2.status as `CCD Call Status`,\n",
    "        t1.PublishingDateTime as `CCD Publish Payload Publishing Date Time`,\n",
    "        t2.error as `CCD Publish Payload Error`,\n",
    "        t2.StartDateTime as `CCD Call Function App Start Date Time`,\n",
    "        t2.EndDateTime as `CCD Call Function App End Date Time`,\n",
    "        t1.error as `CCD Call Function App Error`\n",
    "          \n",
    "          \n",
    "          FROM publish_payload_to_ccd t1\n",
    "          full outer join ccd_response t2 on t2.CaseNo = t1.Filename and t1.state = t2.state\n",
    "          WHERE t2.StartDateTime >= '2025-11-12 16:30:00' and t1.PublishingDateTime >= '2025-11-12 16:30:00'\n",
    "          \n",
    "          \n",
    "          \n",
    "          \"\"\").display()\n",
    "        #   and t1.RunID = t2.RunID uncomment once we have working runs and add to join conditions\n",
    "        #        t2.CaseID as `CCD Case ID`,\n",
    "        #CaseID does not yet exist. we need a fresh run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6b9c233-fd6a-40fc-85d1-90fca1f53a0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7448024178269919,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Active_CCD_call_result_EH_deprecated",
   "widgets": {
    "state": {
     "currentValue": "paymentPending",
     "nuid": "b9936ce1-6d97-45e8-a451-4cda48a2e0b6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "paymentPending",
      "label": "State to Process",
      "name": "state",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "paymentPending",
      "label": "State to Process",
      "name": "state",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
