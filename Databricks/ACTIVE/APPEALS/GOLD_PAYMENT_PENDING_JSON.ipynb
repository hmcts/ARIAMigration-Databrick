{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18504bdf-794e-4966-b290-b635328c3ed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Active Appeals CCD MVP Payment Pending (Silver Layer)\n",
    "<table style='float:left;'>\n",
    "   <tbody>\n",
    "      <tr>\n",
    "         <td style='text-align: left;'><b>Name: </b></td>\n",
    "         <td>SILVER_TO_GOLD__PAYMENT_PENDING_JSON</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "         <td style='text-align: left;'><b>Description: </b></td>\n",
    "         <td>Notebook dedicated for the payment pending, not common for any other active appeal states.</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "         <td style='text-align: left;'><b>First Created: </b></td>\n",
    "         <td>July-2025</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "         <th style='text-align: left;'><b>Changelog (JIRA ref/initials/date):</b></th>\n",
    "         <th>Comments</th>\n",
    "      </tr>\n",
    "      <tr>\n",
    "         <td style='text-align: left;'><a href=\"https://tools.hmcts.net/jira/browse/ARIADM-667\">ARIADM-667</a>/NSA/JUL-2025</td>\n",
    "         <td>Create Silver Staging tables: stg_main_payment_pending_validation, stg_valid_payment_pending_records, stg_invalid_payment_pending_quarantine_records</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "         <td style='text-align: left;'><a href=\"https://tools.hmcts.net/jira/browse/ARIADM-668\">ARIADM-668</a>/NSA/JUL-2025</td>\n",
    "         <td>appealType 1:1 & defaults mappings</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "         <td style='text-align: left;'><a href=\"https://tools.hmcts.net/jira/browse/ARIADM-670\">ARIADM-670</a>/NSA/JUL-2025</td>\n",
    "         <td>appealType logic mappings</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "         <td style='text-align: left;'><a href=\"https://tools.hmcts.net/jira/browse/ARIADM-672\">ARIADM-672</a>/NSA/JUL-2025</td>\n",
    "         <td>caseData 1:1 & defaults mappings</td>\n",
    "      </tr>\n",
    "       <tr>\n",
    "         <td style='text-align: left;'><a href=\"https://tools.hmcts.net/jira/browse/ARIADM-669\">ARIADM-669</a>/NSA/JUL-2025</td>\n",
    "         <td>appealType 1:1 - Data quality & constriant checks implementation</td>\n",
    "      </tr>\n",
    "       <tr>\n",
    "         <td style='text-align: left;'><a href=\"https://tools.hmcts.net/jira/browse/ARIADM-671\">ARIADM-671</a>/NSA/JUL-2025</td>\n",
    "         <td>appealType logic - Data quality & constriant checks implementation</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "         <td style='text-align: left;'><a href=\"https://tools.hmcts.net/jira/browse/ARIADM-707\">ARIADM-707</a>/NSA/JUL-2025</td>\n",
    "         <td>caseData logic mappings - Hearing Centre</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "         <td style='text-align: left;'><a href=\"https://tools.hmcts.net/jira/browse/ARIADM-674\">ARIADM-674</a>/NSA/JUL-2025</td>\n",
    "         <td>caseData logic mappings</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "         <td style='text-align: left;'><a href=\"https://tools.hmcts.net/jira/browse/ARIADM-673\">ARIADM-673</a>/NSA/JUL-2025</td>\n",
    "         <td>caseData 1:1 - Data quality & constriant checks implementation</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "         <td style='text-align: left;'><a href=\"https://tools.hmcts.net/jira/browse/ARIADM-675\">ARIADM-675</a>/NSA/JUL-2025</td>\n",
    "         <td>caseData logic mappings - Data quality & constriant checks implementation</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "         <td style='text-align: left;'><a href=\"https://tools.hmcts.net/jira/browse/ARIADM-707\">ARIADM-707</a>/NSA/JUL-2025</td>\n",
    "         <td>caseData logic mappings - Hearing Centre</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "         <td style='text-align: left;'><a href=\"https://tools.hmcts.net/jira/browse/ARIADM-708\">ARIADM-708</a>/NSA/JUL-2025</td>\n",
    "         <td>caseData logic mappings - Hearing Centre - Data quality & constriant checks implementation</td>\n",
    "      </tr>\n",
    "   </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6b49dac-308d-447a-8532-b56207d5350c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47cb7071-a7f5-4143-aff8-d956deb54805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import json\n",
    "# from pyspark.sql.functions import when, col,coalesce, current_timestamp, lit, date_format\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ace5af5-8756-453e-b843-032a5c8baaba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/configs/config.json\")\n",
    "env_name = config.first()[\"env\"].strip().lower()\n",
    "lz_key = config.first()[\"lz_key\"].strip().lower()\n",
    "\n",
    "print(f\"env_code: {lz_key}\")  # This won't be redacted\n",
    "print(f\"env_name: {env_name}\")  # This won't be redacted\n",
    "\n",
    "KeyVault_name = f\"ingest{lz_key}-meta002-{env_name}\"\n",
    "print(f\"KeyVault_name: {KeyVault_name}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fce9d72-a29e-4db7-910a-d6b0d235c297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Service principal credentials\n",
    "client_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-ID\")\n",
    "client_secret = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-SECRET\")\n",
    "tenant_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-TENANT-ID\")\n",
    "\n",
    "# Storage account names\n",
    "curated_storage = f\"ingest{lz_key}curated{env_name}\"\n",
    "checkpoint_storage = f\"ingest{lz_key}xcutting{env_name}\"\n",
    "raw_storage = f\"ingest{lz_key}raw{env_name}\"\n",
    "landing_storage = f\"ingest{lz_key}landing{env_name}\"\n",
    "external_storage = f\"ingest{lz_key}external{env_name}\"\n",
    "\n",
    "\n",
    "# Spark config for curated storage (Delta table)\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{curated_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{curated_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{curated_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{curated_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{curated_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{checkpoint_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{checkpoint_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{checkpoint_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{checkpoint_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{checkpoint_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{raw_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{raw_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{raw_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{raw_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{raw_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{landing_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{landing_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{landing_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{landing_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{landing_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{external_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{external_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{external_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{external_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{external_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b86460-a200-4c6b-9bf1-382b6bc1e8db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6d53e3e-0282-4481-959b-3f6fbadab65f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read_hive = False\n",
    "\n",
    "# Setting variables for use in subsequent cells\n",
    "bronze_path = f\"abfss://bronze@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS\"\n",
    "silver_path = f\"abfss://silver@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS\"\n",
    "gold_path = f\"abfss://gold@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS\"\n",
    "gold_outputs = \"ARIADM/CCD/APPEALS\"\n",
    "# hive_schema = \"ariadm_ccd_apl\"\n",
    "# key_vault = \"ingest00-keyvault-sbox\"\n",
    "AppealState = \"paymentPending\"\n",
    "\n",
    "\n",
    "# Print all variables\n",
    "variables = {\n",
    "    # \"read_hive\": read_hive,\n",
    "    \n",
    "    \"bronze_path\": bronze_path,\n",
    "    \"silver_path\": silver_path,\n",
    "    \"gold_path\": gold_path,\n",
    "    # \"html_path\": html_path,\n",
    "    \"gold_outputs\": gold_outputs,\n",
    "    # \"hive_schema\": hive_schema,\n",
    "    \"key_vault\": KeyVault_name,\n",
    "    \"AppealState\": AppealState\n",
    "\n",
    "}\n",
    "\n",
    "display(variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a2e4c75-b6e3-403f-99f8-ca27a6e03b05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## PaymentPending: Silver DLT staging table for gold transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abaa7f53-6d8d-450b-b268-5a5386737a40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## JSON Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd658dd5-fa78-4548-b0ce-0f3fe6954871",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "caseManagementCategory JSON Schema"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json, trim, regexp_replace\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "\n",
    "# Define the schema of the JSON structure in caseManagementCategory\n",
    "caseManagementCategory_json_schema = StructType([\n",
    "    StructField(\"value\", StructType([\n",
    "        StructField(\"code\", StringType(), True),\n",
    "        StructField(\"label\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"list_items\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"code\", StringType(), True),\n",
    "            StructField(\"label\", StringType(), True)\n",
    "        ])\n",
    "    ), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9dc2f07-0f34-46eb-a4fc-73e29b7966e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Transformation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b88d0ab2-727d-44c8-ac47-f7ae52c0a531",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: appealType"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lit\n",
    "\n",
    "# AppealType grouping\n",
    "# Note: AppealType and other columns have been mapped with logic in m1 silver.\n",
    "# Additional Notes - All these mappings use logic held in the tab 'APPENDIX-AppealType'\n",
    "# Using a case statement - refer to appendix for logic.\n",
    "def appealType(silver_m1):\n",
    "    conditions = (col(\"dv_representation\").isin('LR', 'AIP')) & (col(\"lu_appealType\").isNotNull())\n",
    "\n",
    "\n",
    "    df = silver_m1.select(\n",
    "        col(\"CaseNo\"),\n",
    "        when(\n",
    "            conditions,\n",
    "            col(\"lu_appealType\")\n",
    "        ).otherwise(None).alias(\"appealType\"),\n",
    "        when(\n",
    "            conditions,\n",
    "            col(\"lu_hmctsCaseCategory\")\n",
    "        ).otherwise(None).alias(\"hmctsCaseCategory\"),\n",
    "        when(\n",
    "            conditions,\n",
    "            col(\"CaseNo\")\n",
    "        ).otherwise(None).alias(\"appealReferenceNumber\"),\n",
    "        when(\n",
    "            conditions,\n",
    "            col(\"lu_appealTypeDescription\")\n",
    "        ).otherwise(None).alias(\"appealTypeDescription\"),\n",
    "        when(\n",
    "            ((col(\"dv_representation\").isin('LR')) & (col(\"lu_appealType\").isNotNull())),\n",
    "            col(\"lu_caseManagementCategory\")\n",
    "        ).otherwise(lit(None)).alias(\"caseManagementCategory\"),\n",
    "        when(\n",
    "            ((col(\"dv_representation\").isin('AIP')) & (col(\"lu_appealType\").isNotNull())),\n",
    "            lit(\"Yes\")\n",
    "        ).otherwise(lit(None)).alias(\"isAppealReferenceNumberAvailable\"),\n",
    "        when(\n",
    "            conditions,\n",
    "            lit(\"\")\n",
    "        ).otherwise(lit(None)).alias(\"ccdReferenceNumberForDisplay\")\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "# AppealState = \"paymentPending\"\n",
    "# silver_m1 = spark.table(\"ariadm_active_appeals.silver_appealcase_detail\").filter(col(\"dv_targetState\") == lit(AppealState)).distinct()\n",
    "# df = appealType(silver_m1)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "281bbd63-a6bd-4048-839a-a48e62714025",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: caseData"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list, struct, when, lit, col, max as spark_max, date_format, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# caseData grouping\n",
    "def caseData(silver_m1, silver_m3, silver_m5):\n",
    "    # silver_m1 = silver_m1.filter( ((col(\"representation\").isin('LR', 'AIP')) & (col(\"appealType\").isNotNull())))\n",
    "\n",
    "    # Filter silver_m3 to get rows with max StatusId and Outcome is not null\n",
    "    # Define window partitioned by CaseNo and ordered by descending StatusId\n",
    "    window_spec = Window.partitionBy(\"CaseNo\").orderBy(col(\"StatusId\").desc())\n",
    "\n",
    "    # Add row_number to get the row with the highest StatusId per CaseNo\n",
    "    silver_m3_ranked = silver_m3.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "\n",
    "    # Filter the top-ranked rows where Outcome is not null\n",
    "    silver_m3_filtered = silver_m3_ranked.filter(\n",
    "        (col(\"row_num\") == 1) & (col(\"Outcome\").isNotNull())\n",
    "    ).select(\n",
    "        col(\"CaseNo\"),\n",
    "        lit(\"Yes\").alias(\"recordedOutOfTimeDecision\"), col(\"Outcome\")\n",
    "    )\n",
    "\n",
    "    conditions = (col(\"dv_representation\").isin('LR', 'AIP')) & (col(\"lu_appealType\").isNotNull())\n",
    "\n",
    "    df = silver_m1.alias(\"m1\").join(\n",
    "        silver_m3_filtered.alias(\"m3\"),\n",
    "        on=\"CaseNo\",\n",
    "        how=\"left\"\n",
    "    ).withColumn(\n",
    "        \"appellantsRepresentation\", when(((col(\"m1.dv_representation\") == \"LR\") &  (col(\"lu_appealType\").isNotNull())), \"No\").when(((col(\"m1.dv_representation\") == \"AIP\") & (col(\"lu_appealType\").isNotNull())), \"Yes\").otherwise(None)\n",
    "    ).withColumn(\n",
    "        \"submissionOutOfTime\", when(col(\"OutOfTimeIssue\") == 1, lit(\"Yes\")).otherwise(lit(\"No\"))\n",
    "    ).withColumn(\n",
    "        \"adminDeclaration1\", lit([\"hasDeclared\"])\n",
    "    ).withColumn(\n",
    "        \"appealWasNotSubmittedReason\", when(((col(\"m1.dv_representation\") == \"LR\") & (col(\"lu_appealType\").isNotNull())), \"This is an ARIA Migrated Case.\").otherwise(None)\n",
    "    ).withColumn(\n",
    "        \"applicationOutOfTimeExplanation\", when(col(\"OutOfTimeIssue\") == 1, \"This is a migrated ARIA case. Please refer to the documents.\").otherwise(None)\n",
    "    ).withColumn(\n",
    "        \"appealSubmissionDate\", date_format(col(\"m1.DateLodged\"), \"yyyy-MM-dd'T'HH:mm:ss'Z'\")\n",
    "    ).withColumn(\n",
    "        \"appealSubmissionInternalDate\", date_format(col(\"m1.DateLodged\"), \"yyyy-MM-dd'T'HH:mm:ss'Z'\")\n",
    "    ).withColumn(\n",
    "        \"tribunalReceivedDate\", date_format(col(\"m1.DateAppealReceived\"), \"yyyy-MM-dd'T'HH:mm:ss'Z'\")\n",
    "    ).select(\n",
    "        \"CaseNo\", \n",
    "        col(\"appellantsRepresentation\"),\n",
    "        when(conditions, col(\"submissionOutOfTime\")).otherwise(None).alias(\"submissionOutOfTime\"),\n",
    "        when(conditions, col(\"m3.recordedOutOfTimeDecision\")).otherwise(None).alias(\"recordedOutOfTimeDecision\"),\n",
    "        when(conditions, col(\"applicationOutOfTimeExplanation\")).otherwise(None).alias(\"applicationOutOfTimeExplanation\"), \n",
    "        when(conditions, col(\"lu_hearingCentre\")).otherwise(None).alias(\"hearingCentre\"),\n",
    "        when(conditions, col(\"lu_staffLocation\")).otherwise(None).alias(\"staffLocation\"),\n",
    "        when(conditions, col(\"lu_caseManagementLocation\")).otherwise(None).alias(\"caseManagementLocation\"),\n",
    "        when(conditions, col(\"dv_hearingCentreDynamicList\")).otherwise(None).alias(\"hearingCentreDynamicList\"),\n",
    "        when(conditions, col(\"dv_caseManagementLocationRefData\")).otherwise(None).alias(\"caseManagementLocationRefData\"),\n",
    "        when(conditions, col(\"lu_selectedHearingCentreRefData\")).otherwise(None).alias(\"selectedHearingCentreRefData\"),\n",
    "        col(\"appealWasNotSubmittedReason\"),\n",
    "        when(conditions, col(\"adminDeclaration1\")).otherwise(None).alias(\"adminDeclaration1\"),    \n",
    "        when(conditions, col(\"appealSubmissionDate\")).otherwise(None).alias(\"appealSubmissionDate\"), \n",
    "        when(conditions, col(\"appealSubmissionInternalDate\")).otherwise(None).alias(\"appealSubmissionInternalDate\"),\n",
    "        when(conditions, col(\"tribunalReceivedDate\")).otherwise(None).alias(\"tribunalReceivedDate\"),\n",
    "        when(conditions, lit([]).cast(\"array<int>\")).otherwise(None).alias(\"caseLinks\"), \n",
    "        when(conditions, lit(\"No\")).otherwise(None).alias(\"hasOtherAppeals\")\n",
    "        \n",
    "        \n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "# AppealState = \"paymentPending\"\n",
    "# silver_m5 = spark.table(\"ariadm_active_appeals.silver_link_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "# silver_m1 = spark.table(\"ariadm_active_appeals.silver_appealcase_detail\").filter(col(\"dv_targetState\") == lit(AppealState)).distinct()\n",
    "# silver_m3 = spark.table(\"ariadm_active_appeals.silver_status_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "# df = caseData(silver_m1, silver_m3, silver_m5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3fae8bf-8758-4e4a-8055-5b9fbdd5b0e1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752526691071}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "796671d8-9f79-437f-8c41-ef87273a34df",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752364743928}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Function: mainPaymentPendingJsonGenerator"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, StructType, ArrayType, MapType\n",
    "\n",
    "def mainPaymentPending(silver_m1, silver_m3,silver_m5):\n",
    "\n",
    "    AppealState = \"paymentPending\"\n",
    "\n",
    "    # Aggregate details\n",
    "    AppealType_df = appealType(silver_m1)\n",
    "    # grouped_transaction = TransactionDetails(silver_m4)\n",
    "    caseData_df = caseData( silver_m1, silver_m3,silver_m5)\n",
    "\n",
    "    # Join all aggregated data with Appeal Case Details\n",
    "    df_combined = AppealType_df.join(caseData_df, on=\"CaseNo\", how=\"left\")\n",
    "    \n",
    "\n",
    "    # Create JSON and filename and OMit columns that are with null values\n",
    "    df_final = df_combined.withColumn(\n",
    "        \"JSON_Content\", to_json(struct(*df_combined.drop(col(\"CaseNo\")).columns))\n",
    "    ).withColumn(\n",
    "        \"JSON_File_name\", concat(lit(f\"{gold_outputs}/JSON/APPEALS_\"), regexp_replace(col(\"CaseNo\"), \"/\", \"_\"), lit(\".json\"))\n",
    "    )\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# AppealState = \"paymentPending\"\n",
    "# silver_m1 = spark.table(\"ariadm_active_appeals.silver_appealcase_detail\").filter(col(\"dv_targetState\") == lit(AppealState)).distinct()\n",
    "# silver_m3 = spark.table(\"ariadm_active_appeals.silver_status_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "# silver_m5 = spark.table(\"ariadm_active_appeals.silver_link_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "\n",
    "# df_final = mainPaymentPending(silver_m1, silver_m3,silver_m5)\n",
    "# display(df_final.select(\"*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd194be7-d459-40fb-b7fb-e1754f659817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Upload Function and Blob Client Connection Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02e471dd-c9a6-42a9-8bfa-0225e92f2ad4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Secret Retrieval for Database Connection"
    }
   },
   "outputs": [],
   "source": [
    "secret = dbutils.secrets.get(KeyVault_name, \"CURATED-sbox-SAS-TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87a18b1c-66e9-473e-9527-d9b1d1417984",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Azure Blob Storage Connection Setup in Python"
    }
   },
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "import os\n",
    "\n",
    "# Set up the BlobServiceClient with your connection string\n",
    "connection_string = secret\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Specify the container name\n",
    "container_name = \"gold\"\n",
    "container_client = blob_service_client.get_container_client(container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0491f238-9f10-4d11-9bc6-3f76111c29d1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: Format Dates for UploadBlob Storage"
    }
   },
   "outputs": [],
   "source": [
    "# Upload HTML to Azure Blob Storage\n",
    "def upload_to_blob(file_name, file_content):\n",
    "    try:\n",
    "        # blob_client = container_client.get_blob_client(f\"{gold_outputs}/HTML/{file_name}\")\n",
    "        blob_client = container_client.get_blob_client(f\"{file_name}\")\n",
    "        blob_client.upload_blob(file_content, overwrite=True)\n",
    "        return \"success\"\n",
    "    except Exception as e:\n",
    "        return f\"error: {str(e)}\"\n",
    "\n",
    "# Register the upload function as a UDF\n",
    "upload_udf = udf(upload_to_blob)\n",
    "\n",
    "# df_with_upload_status = df_final.withColumn(\n",
    "#     \"Status\", upload_udf(col(\"JSON_File_name\"), col(\"JSON_Content\"))\n",
    "# )\n",
    "\n",
    "# display(df_with_upload_status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ee255ab-a380-4564-b454-2c10e65707a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Gold Outputs and Tracking DLT Table Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3966a5d7-de3a-467e-b1ed-04809e2d256b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![DQValidation.png](./Images/DQValidation.png \"DQValidation.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fce04fc8-6c49-48ac-bd21-f98b972f711c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Expectations collection"
    }
   },
   "outputs": [],
   "source": [
    "# Define a dictionary to hold data quality checks\n",
    "checks = {}\n",
    "\n",
    "\n",
    "checks[\"valid_appealtype_exists\"] = \"(AppealType IN ('refusalOfHumanRights', 'refusalOfEu', 'deprivation', 'protection', 'revocationOfProtection', 'euSettlementScheme'))\"\n",
    "checks[\"valid_hmctsCaseCategory_exists\"] = \"(hmctsCaseCategory IS NOT NULL)\"\n",
    "checks[\"valid_appealTypeDescription_exists\"] = \"(appealTypeDescription IS NOT NULL)\"\n",
    "# Null Values as accepted values as where Representation = AIP\n",
    "# checks[\"valid_caseManagementCategory_exists\"] = \"\"\"\n",
    "#     ((valid_caseManagementCategory IS NULL AND valid_representation = 'AIP') OR \n",
    "#     (valid_caseManagementCategory IS NOT NULL AND valid_representation = 'LR'))\n",
    "# \"\"\"\n",
    "\n",
    "checks[\"valid_caseManagementCategory_code_in_list_items\"] = \"\"\"\n",
    "  caseManagementCategory.value.code IS NULL OR\n",
    "  ARRAY_CONTAINS(\n",
    "    TRANSFORM(caseManagementCategory.list_items, x -> x.code),\n",
    "    caseManagementCategory.value.code\n",
    "  )\n",
    "\"\"\"\n",
    "\n",
    "checks[\"valid_caseManagementCategory_label_in_list_items\"] = \"\"\"\n",
    "  caseManagementCategory.value.label IS NULL OR\n",
    "  ARRAY_CONTAINS(\n",
    "    TRANSFORM(caseManagementCategory.list_items, x -> x.label),\n",
    "    caseManagementCategory.value.label\n",
    "  )\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "checks[\"valid_appealReferenceNumber_exists\"] = \"(appealReferenceNumber IS NOT NULL)\"\n",
    "checks[\"valid_isappealreferencenumberavailable_exists\"] = \"\"\"\n",
    "    ((isappealreferencenumberavailable = 'Yes' AND valid_representation = 'AIP') OR     \n",
    "    (isappealreferencenumberavailable IS NULL AND valid_representation = 'LR'))\n",
    "\"\"\"\n",
    "checks[\"valid_caseLinks_exists\"] = \"(size(caseLinks) = 0)\"\n",
    "checks[\"valid_hasOtherAppeals\"] = \"(hasOtherAppeals = 'No')\"\n",
    "\n",
    "\n",
    "# CaseData # ARIADM-675\n",
    "checks[\"valid_appellantsRepresentation\"] = \"(appellantsRepresentation IS NOT NULL AND appellantsRepresentation IN ('Yes', 'No'))\"\n",
    "checks[\"valid_submissionOutOfTime\"] = \"(submissionOutOfTime IS NOT NULL AND submissionOutOfTime IN ('Yes', 'No'))\"\n",
    "checks[\"valid_recordedOutOfTimeDecision\"] = \"(recordedOutOfTimeDecision IS NULL OR recordedOutOfTimeDecision IN ('Yes', 'No'))\"\n",
    "checks[\"valid_applicationOutOfTimeExplanation\"] = \"(applicationOutOfTimeExplanation IS NULL OR applicationOutOfTimeExplanation IN ('Yes', 'No'))\"\n",
    "\n",
    "#CaseData Part02\n",
    "# checks[\"valid_hearingCentre\"] = \"\"\"\n",
    "#     (hearingCentre IN ('taylorHouse', 'newport', 'newcastle', 'manchester', 'hattonCross', \n",
    "#     'glasgow', 'bradford', 'birmingham', 'arnhemHouse', 'crownHouse', 'harmondsworth', \n",
    "#     'yarlsWood', 'remoteHearing', 'decisionWithoutHearing'))\n",
    "# \"\"\"\n",
    "# checks[\"valid_staffLocation_exists\"] = \"(staffLocation IS NOT NULL)\"\n",
    "# checks[\"valid_caseManagementLocation_exists\"] = \"(valid_caseManagementLocation IS NOT NULL)\"\n",
    "\n",
    "# DynamicList does not seem to be applicable\n",
    "# checks[\"valid_hearingCentreDynamicList_exists\"] = \"(valid_hearingCentreDynamicList IS NOT NULL)\"\n",
    "# checks[\"valid_caseManagementLocationRefData_exists\"] = \"(valid_caseManagementLocationRefData IS NOT NULL)\"\n",
    "# checks[\"valid_selectedHearingCentreRefData_exists\"] = \"(selectedHearingCentreRefData IS NOT NULL)\"\n",
    "# checks[\"valid_applicationOutOfTimeExplanation\"] = \"(applicationOutOfTimeExplanation IS NOT NULL)\"\n",
    "\n",
    "# \\d is a regular expression (regex) metacharacter that matches any single digit from 0 to 9.\n",
    "checks[\"valid_appealSubmissionDate_format\"] = ( \"appealSubmissionDate RLIKE r'^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z$'\" )\n",
    "checks[\"valid_appealSubmissionInternalDate_format\"] = ( \"appealSubmissionInternalDate RLIKE r'^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z$'\" )\n",
    "checks[\"valid_tribunalReceivedDate_format\"] = ( \"tribunalReceivedDate RLIKE r'^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z$'\" )\n",
    "\n",
    "\n",
    "# checks[\"ccd_reference_number_for_display_exists\"] = \"(ccdReferenceNumberForDisplay IS NOT NULL)\"\n",
    "\n",
    "# Combine all checks into a single string with AND conditions\n",
    "# Create a validation expression to quarantine records\n",
    "dq_rules = \"({0})\".format(\" AND \".join(checks.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e673184-66e1-4269-8c3b-c83daffe4e0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# print(dq_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02b0aeb4-97d2-44d9-b548-ee055e9fa181",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Test: Validatiom"
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# SELECT \n",
    "#   CaseNo,\n",
    "#   appealSubmissionDate,\n",
    "#   submissionOutOfTime,\n",
    "#   valid_caseManagementCategory,\n",
    "#   caseManagementCategory\n",
    "# FROM ariadm_active_appeals.stg_main_payment_pending_validation\n",
    "# WHERE AppealType IN (\n",
    "#     'refusalOfHumanRights', 'refusalOfEu', 'deprivation', 'protection', \n",
    "#     'revocationOfProtection', 'euSettlementScheme'\n",
    "# )\n",
    "# AND appealReferenceNumber IS NOT NULL\n",
    "# AND hmctsCaseCategory IS NOT NULL\n",
    "# AND appealTypeDescription IS NOT NULL\n",
    "# AND (\n",
    "#     (valid_caseManagementCategory IS NULL AND valid_representation = 'AIP')\n",
    "#     OR (valid_caseManagementCategory IS NOT NULL AND valid_representation = 'LR')\n",
    "# )\n",
    "# AND (\n",
    "#     (isappealreferencenumberavailable = 'Yes' AND valid_representation = 'AIP') \n",
    "#     OR (isappealreferencenumberavailable IS NULL AND valid_representation = 'LR')\n",
    "# )\n",
    "# AND size(caseLinks) = 0\n",
    "# AND hasOtherAppeals = 'No'\n",
    "# AND (submissionOutOfTime IS NOT NULL AND submissionOutOfTime IN ('Yes', 'No'))\n",
    "# AND (recordedOutOfTimeDecision IS NULL OR recordedOutOfTimeDecision IN ('Yes', 'No'))\n",
    "# AND appealSubmissionDate RLIKE r'^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z$'\n",
    "# AND appellantsRepresentation IS NOT NULL AND appellantsRepresentation IN ('Yes', 'No')\n",
    "# AND (applicationOutOfTimeExplanation IS NULL OR applicationOutOfTimeExplanation IN ('Yes', 'No'))\n",
    "# AND (\n",
    "#   caseManagementCategory.value.code IS NULL OR\n",
    "#   ARRAY_CONTAINS(\n",
    "#     TRANSFORM(caseManagementCategory.list_items, x -> x.code),\n",
    "#     caseManagementCategory.value.code\n",
    "#   )\n",
    "# )\n",
    "# -- AND hearingCentre IN (\n",
    "# --     'taylorHouse', 'newport', 'newcastle', 'manchester', 'hattonCross', \n",
    "# --     'glasgow', 'bradford', 'birmingham', 'arnhemHouse', 'crownHouse', \n",
    "# --     'harmondsworth', 'yarlsWood', 'remoteHearing', 'decisionWithoutHearing'\n",
    "# -- )\n",
    "# -- AND staffLocation IS NOT NULL\n",
    "# -- AND valid_caseManagementLocation IS NOT NULL\n",
    "# -- AND valid_hearingCentreDynamicList IS NOT NULL\n",
    "# -- AND valid_caseManagementLocationRefData IS NOT NULL\n",
    "# -- AND selectedHearingCentreRefData IS NOT NULL\n",
    "# AND appealSubmissionInternalDate RLIKE r'^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z$'\n",
    "# AND tribunalReceivedDate RLIKE r'^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a504137-9ff3-42a5-a08e-8d61773eead0",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752582307313}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# select CaseNo, caseManagementCategory,JSON_Content,valid_representation  from ariadm_active_appeals.stg_main_payment_pending_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6742b7aa-6240-402b-9800-8ab881e416bb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transformation: stg_payment_pending_ccd_json_generator"
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, lit, expr\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"stg_main_payment_pending_validation\",\n",
    "    comment=\"DLT table running mainPaymentPending to generate a JSON_Content column for CCD validation. Applies DLT expectations on CCD, adding is_valid to flag validation results.\",\n",
    "    path=f\"{silver_path}/stg_main_payment_pending_validation\"\n",
    ")\n",
    "@dlt.expect_all(checks)\n",
    "def stg_main_payment_pending_validation():\n",
    "    try:\n",
    "        silver_m1 = dlt.read(\"silver_appealcase_detail\").filter(col(\"dv_targetState\") == lit(AppealState)).distinct()\n",
    "        bronze_appealtype_lookup_df = dlt.read(\"bronze_appealtype\").distinct()\n",
    "        bronze_hearing_centres_lookup_df = dlt.read(\"bronze_hearing_centres\").distinct()\n",
    "        # stg_representation = dlt.read(\"stg_representation\").select(col(\"Representation\").alias(\"valid_representation\"))\n",
    "        # silver_m2 = dlt.read(\"silver_caseapplicant_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_m3 = dlt.read(\"silver_status_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m4 = dlt.read(\"silver_transaction_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_m5 = dlt.read(\"silver_link_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m6 = dlt.read(\"silver_adjudicator_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m7 = dlt.read(\"silver_appealcategory_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m8 = dlt.read(\"silver_documentsreceived_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m9 = dlt.read(\"silver_history_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "    except:\n",
    "        silver_m1 = spark.table(\"ariadm_active_appeals.silver_appealcase_detail\").filter(col(\"dv_targetState\") == lit(AppealState)).distinct()\n",
    "        bronze_appealtype_lookup_df = spark.table(\"ariadm_active_appeals.bronze_appealtype\").distinct()\n",
    "        bronze_hearing_centres_lookup_df = spark.table(\"ariadm_active_appeals.bronze_hearing_centres\").distinct()\n",
    "        # stg_representation = spark.table(\"ariadm_active_appeals.stg_representation\").select(col(\"Representation\").alias(\"valid_representation\"))\n",
    "        # silver_m2 = spark.table(\"ariadm_active_appeals.silver_caseapplicant_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_m3 = spark.table(\"ariadm_active_appeals.silver_status_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m4 = spark.table(\"ariadm_active_appeals.silver_transaction_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_m5 = spark.table(\"ariadm_active_appeals.silver_link_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m6 = spark.table(\"ariadm_active_appeals.silver_adjudicator_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m7 = spark.table(\"ariadm_active_appeals.silver_appealcategory_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m8 = spark.table(\"ariadm_active_appeals.silver_documentsreceived_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m9 = spark.table(\"ariadm_active_appeals.silver_history_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "\n",
    "    df_final = mainPaymentPending(silver_m1, silver_m3, silver_m5)\n",
    "\n",
    "    # Join with valid CaseNo and AppealType references\n",
    "    df_joined = (\n",
    "        df_final.alias(\"final\")\n",
    "        .join(\n",
    "            bronze_appealtype_lookup_df.select(col(\"caseManagementCategory\").alias(\"valid_caseManagementCategory\")).alias(\"at_cmc\"),\n",
    "            col(\"final.caseManagementCategory\") == col(\"at_cmc.valid_caseManagementCategory\"),\n",
    "            \"left\"\n",
    "        )\n",
    "        # .join(\n",
    "        #     bronze_hearing_centres_lookup_df.select(col(\"caseManagementLocation\").alias(\"valid_caseManagementLocation\")).alias(\"hc_cml\"),\n",
    "        #     col(\"final.caseManagementLocation\") == col(\"hc_cml.valid_caseManagementLocation\"),\n",
    "        #     \"left\"\n",
    "        # )\n",
    "        # .join(\n",
    "        #     silver_m1.select(col(\"dv_hearingCentreDynamicList\").alias(\"valid_hearingCentreDynamicList\")).alias(\"m1_hcd\"),\n",
    "        #     col(\"final.hearingCentreDynamicList\") == col(\"m1_hcd.valid_hearingCentreDynamicList\"),\n",
    "        #     \"left\"\n",
    "        # )\n",
    "        # .join(\n",
    "        #     silver_m1.select(col(\"dv_caseManagementLocationRefData\").alias(\"valid_caseManagementLocationRefData\")).alias(\"m1_cmlr\"),\n",
    "        #     col(\"final.caseManagementLocationRefData\") == col(\"m1_cmlr.valid_caseManagementLocationRefData\"),\n",
    "        #     \"left\"\n",
    "        # )\n",
    "        # .join(\n",
    "        #     bronze_hearing_centres_lookup_df.select(col(\"selectedHearingCentreRefData\").alias(\"valid_selectedHearingCentreRefData\")).alias(\"hc_shcrd\"),\n",
    "        #     col(\"final.selectedHearingCentreRefData\") == col(\"hc_shcrd.valid_selectedHearingCentreRefData\"),\n",
    "        #     \"left\"\n",
    "        # )\n",
    "        .join(\n",
    "            silver_m1.select(col(\"dv_representation\").alias(\"valid_representation\"), col(\"CaseNo\").alias(\"valid_CaseNo\")).alias(\"m1\"),\n",
    "            col(\"final.CaseNo\") == col(\"m1.valid_CaseNo\"),\n",
    "            \"left\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df_final = df_joined.withColumn(\"is_valid\", expr(dq_rules))\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ee35c3c-bef2-4365-b3ab-f806bcf5f5fb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transformation: stg_valid_payment_pending_records"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"stg_valid_payment_pending_records\",\n",
    "    comment=\"Delta Live Gold Table with JSON content.\",\n",
    "    path=f\"{silver_path}/stg_valid_payment_pending_records\"\n",
    ")\n",
    "def stg_valid_payment_pending_records():\n",
    "    \"\"\"\n",
    "    Delta Live Table for creating and uploading JSON content for Appeals.\n",
    "    \"\"\"\n",
    "    # Load source data\n",
    "    df = dlt.read(\"stg_main_payment_pending_validation\")\n",
    "\n",
    "    df_filtered = df.filter(\n",
    "        (col(\"is_valid\") == True)\n",
    "    )\n",
    "\n",
    "    # Repartition to optimize parallelism\n",
    "    repartitioned_df = df_filtered.repartition(64)\n",
    "\n",
    "    df_with_upload_status = repartitioned_df.filter(~col(\"JSON_content\").like(\"Error%\")).withColumn(\n",
    "            \"Status\", upload_udf(col(\"JSON_File_Name\"), col(\"JSON_content\"))\n",
    "        )\n",
    "\n",
    "    # Return the DataFrame for DLT table creation\n",
    "    return df_with_upload_status.select(\"CaseNo\", \"JSON_content\",col(\"JSON_File_Name\").alias(\"File_Name\"),\"Status\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a73c1bae-add0-4280-a33f-24a2c5607f29",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transformation stg_invalid_payment_pending_quarantine_records"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"stg_invalid_payment_pending_quarantine_records\",\n",
    "    comment=\"Quarantined records that failed data quality checks or JSON generation.\",\n",
    "    path=f\"{silver_path}/stg_invalid_payment_pending_quarantine_records\"\n",
    ")\n",
    "def stg_invalid_payment_pending_quarantine_records():\n",
    "\n",
    "    df = dlt.read(\"stg_main_payment_pending_validation\")\n",
    "\n",
    "    df_filtered = df.filter(\n",
    "        (col(\"is_valid\") != True)\n",
    "    ).withColumn(\"JSON_File_Name\", regexp_replace(col(\"JSON_File_Name\"), \"/JSON/\", \"/INVALID_JSON/\"))\n",
    "\n",
    "    # Repartition to optimize parallelism\n",
    "    repartitioned_df = df_filtered.repartition(64)\n",
    "\n",
    "    df_with_upload_status = repartitioned_df.filter(~col(\"JSON_content\").like(\"Error%\")).withColumn(\n",
    "            \"Status\", upload_udf(col(\"JSON_File_Name\"), col(\"JSON_content\"))\n",
    "        )\n",
    "\n",
    "    return df_with_upload_status.select(\"CaseNo\", \"JSON_content\",col(\"JSON_File_Name\").alias(\"File_Name\"),\"Status\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed72e9c7-2621-4707-84f7-b4985beb9df4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Notebook completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89fdf409-78a7-47f2-bbcf-d4b001ea7051",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9edb7ba9-63ab-47ee-ac6a-245d6f2c0dc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4275bdae-5059-4d02-91b0-c15ac5642f3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from ariadm_active_appeals.stg_valid_payment_pending_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "046f7375-9942-4681-a07c-325f924cc001",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# SHOW TABLES IN ariadm_active_appeals\n",
    "# -- LIKE 'stg_%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c4ef364-8a1d-4cc3-8e75-0df56eddd88a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# select  from ariadm_active_appeals.stg_main_payment_pending_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c47e93e4-e3a3-4253-ab84-7c56b193b0a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ## Validation Framework\n",
    "\n",
    "# from pyspark.sql.types import *\n",
    "\n",
    "# def validate_AppealType():\n",
    "  \n",
    "#   test_df = AppealType(silver_m1)\n",
    "\n",
    "#   expected_types = {\n",
    "#     \"appealReferenceNumber\": StringType(),\n",
    "#     \"isAppealReferenceNumberAvailable\": Stringtype()\n",
    "\n",
    "#   }\n",
    "      \n",
    "#   nested_types = test_df.schema[\"appealType\"].dataType\n",
    "\n",
    "#   for field in nested_types.elementType.fields:\n",
    "#     field_name = field.name\n",
    "#     input_type = field.dataType\n",
    "#     # print(f\"Field: {field_name}, incoming type {input_type}\")\n",
    "\n",
    "#     expected_type = expected_types[field_name]\n",
    "\n",
    "#     assert input_type == expected_type, f\"Expected type {expected_type} but got {input_type}\"\n",
    "#     print(f\"Succesfully validated type {expected_type} for field {field_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4bc78bc-0e60-4c15-a65a-72e8aad32969",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import StructType, StructField, StringType, BooleanType, TimestampType, IntegerType\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3208702-0e8a-4297-b169-0c836f5e6843",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# def validate_AppealType_content():\n",
    "#     schema = StructType([\n",
    "#         StructField(\"CaseNo\", StringType(), True),\n",
    "#         StructField(\"CasePrefix\", StringType(), True),\n",
    "#         StructField(\"OutOfTimeIssue\", BooleanType(), True),\n",
    "#         StructField(\"DateLodged\", TimestampType(), True),\n",
    "#         StructField(\"DateAppealReceived\", TimestampType(), True),\n",
    "#         StructField(\"CentreId\", IntegerType(), True),\n",
    "#         StructField(\"NationalityId\", IntegerType(), True),\n",
    "#         StructField(\"AppealTypeId\", IntegerType(), True),\n",
    "#         StructField(\"DeportationDate\", TimestampType(), True),\n",
    "#         StructField(\"RemovalDate\", TimestampType(), True),\n",
    "#         StructField(\"VisitVisaType\", IntegerType(), True),\n",
    "#         StructField(\"DateOfApplicationDecision\", TimestampType(), True),\n",
    "#         StructField(\"HORef\", StringType(), True),\n",
    "#         StructField(\"InCamera\", BooleanType(), True),\n",
    "#         StructField(\"CourtPreference\", IntegerType(), True),\n",
    "#         StructField(\"LanguageId\", IntegerType(), True),\n",
    "#         StructField(\"Interpreter\", IntegerType(), True),\n",
    "#         StructField(\"RepresentativeId\", IntegerType(), True),\n",
    "#         StructField(\"CaseRepName\", StringType(), True)\n",
    "#     ])\n",
    "    \n",
    "#     data = [\n",
    "#         (\"12345\", \"CP\", True, datetime(2025, 6, 24, 0, 0, 0), datetime(2025, 6, 24, 0, 0, 0), 1, 1, 1, datetime(2025, 6, 24, 0, 0, 0), datetime(2025, 6, 24, 0, 0, 0), 1, datetime(2025, 6, 24, 0, 0, 0), \"HO123\", True, 1, 1, 1, 1, \"RepName\")\n",
    "#     ]\n",
    "    \n",
    "#     df = spark.createDataFrame(data, schema)\n",
    "\n",
    "#     appealtype_df = AppealType(df)\n",
    "#     appealtype_df.display()\n",
    "\n",
    "#     ### Some assert Content Test e.g appealReferenceNumber is in format first 3 characters are letters followed by/ followed by 3 numbers etc\n",
    "\n",
    "# validate_AppealType_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a884681e-468f-4be2-b0d9-02998e22c1cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# first_row = df_final.select(\"JSON_Content\").first()\n",
    "# json_content = first_row[\"JSON_Content\"]\n",
    "# parsed_json = json.loads(json_content)\n",
    "\n",
    "# def validate_json_types(parsed_json):\n",
    "#     expected_types = {\n",
    "#         \"appealReferenceNumber\": str,\n",
    "#         \"appealType\": list,\n",
    "#         \"Transactions\": list\n",
    "#     }\n",
    "    \n",
    "#     inner_expected_types = {\n",
    "#         \"Transactions\": {\n",
    "#             \"transactionId\": str,\n",
    "#             \"amount\": float,\n",
    "#             \"date\": str\n",
    "#         },\n",
    "#         \"appealType\": {\n",
    "#             \"appealReferenceNumber\": str,\n",
    "#             \"isAppealReferenceNumberAvailable\": str\n",
    "#         }\n",
    "#     }\n",
    "    \n",
    "#     for key, expected_type in expected_types.items():\n",
    "#         if key in parsed_json:\n",
    "#             if not isinstance(parsed_json[key], expected_type):\n",
    "#                 print(f\"Validation failed: Key '{key}' has incorrect type. Expected {expected_type}, got {type(parsed_json[key])}\")\n",
    "#                 raise TypeError(f\"Key '{key}' has incorrect type. Expected {expected_type}, got {type(parsed_json[key])}\")\n",
    "#             else:\n",
    "#                 print(f\"Validation passed: Key '{key}' has correct type {expected_type}\")\n",
    "#                 if key in inner_expected_types:\n",
    "#                     for item in parsed_json[key]:\n",
    "#                         for inner_key, inner_expected_type in inner_expected_types[key].items():\n",
    "#                             if inner_key in item:\n",
    "#                                 if not isinstance(item[inner_key], inner_expected_type):\n",
    "#                                     print(f\"Validation failed: Key '{inner_key}' in '{key}' has incorrect type. Expected {inner_expected_type}, got {type(item[inner_key])}\")\n",
    "#                                     raise TypeError(f\"Key '{inner_key}' in '{key}' has incorrect type. Expected {inner_expected_type}, got {type(item[inner_key])}\")\n",
    "#                                 else:\n",
    "#                                     print(f\"Validation passed: Key '{inner_key}' in '{key}' has correct type {inner_expected_type}\")\n",
    "#                                 if inner_key == \"isAppealReferenceNumberAvailable\" and item[inner_key] not in [\"YES\", \"no\"]:\n",
    "#                                     print(f\"Validation failed: Key '{inner_key}' in '{key}' has invalid value. Expected 'YES' or 'no', got {item[inner_key]}\")\n",
    "#                                     raise ValueError(f\"Key '{inner_key}' in '{key}' has invalid value. Expected 'YES' or 'no', got {item[inner_key]}\")\n",
    "\n",
    "# validate_json_types(parsed_json)\n",
    "# print(json.dumps(parsed_json, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72a6fd4f-e2d9-4674-a95b-2d15f5250099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cef11295-06e8-4e72-b36f-6d4b25f69c2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19259348-dcb7-45ff-929f-70975033f0ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_final = main_paymentPending()\n",
    "# display(df_final.select(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbeaf641-013a-4d3e-9c9a-b60db8212434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# first_row = df_final.filter(df_final[\"CaseNo\"] == \"HU/00035/2017\").select(\"JSON_Content\").first()\n",
    "# json_content = first_row[\"JSON_Content\"]\n",
    "# parsed_json = json.loads(json_content)\n",
    "# display(parsed_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7211d5a2-f1d8-496e-b3a7-02864b284700",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d49dd64-f505-4807-8dc9-3cb0a563d925",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, lit, when, coalesce, collect_list, struct\n",
    "\n",
    "# AppealState = \"paymentPending\"  # Define AppealState variable\n",
    "\n",
    "# silver_m1 = spark.table(\"ariadm_active_appeals.silver_appealcase_detail\").filter(col(\"TargetState\") == lit(AppealState)).distinct()\n",
    "\n",
    "# df = silver_m1.select(\n",
    "#     col(\"CaseNo\"),\n",
    "#     when(\n",
    "#         (col(\"representation\") == 'LR') | (col(\"representation\") == 'AIP'),\n",
    "#         coalesce(col(\"appealType\"), lit(\"\"))\n",
    "#     ).otherwise(\"\").alias(\"appealType\"),\n",
    "#     when(\n",
    "#         (col(\"representation\") == 'LR') | (col(\"representation\") == 'AIP'),\n",
    "#         coalesce(col(\"hmctsCaseCategory\"), lit(\"\"))\n",
    "#     ).otherwise(\"\").alias(\"hmctsCaseCategory\"),\n",
    "#     col(\"CaseNo\").alias(\"appealReferenceNumber\"),\n",
    "#     when(\n",
    "#         (col(\"representation\") == 'LR') | (col(\"representation\") == 'AIP'),\n",
    "#         coalesce(col(\"appealTypeDescription\"), lit(\"\"))\n",
    "#     ).otherwise(\"\").alias(\"appealTypeDescription\"),\n",
    "#     when(\n",
    "#         (col(\"representation\") == 'LR'),\n",
    "#         col(\"caseManagementCategory\")\n",
    "#     ).otherwise(lit(None)).alias(\"caseManagementCategory\"),\n",
    "#     lit(\"YES\").alias(\"isAppealReferenceNumberAvailable\"),\n",
    "#     lit(\"\").alias(\"ccdReferenceNumberForDisplay\")\n",
    "# )\n",
    "\n",
    "# df = df.groupBy(col(\"CaseNo\")).agg(\n",
    "#     collect_list(\n",
    "#         struct(\n",
    "#             'appealType', 'appealReferenceNumber', 'hmctsCaseCategory', 'appealTypeDescription', 'caseManagementCategory', 'isAppealReferenceNumberAvailable','ccdReferenceNumberForDisplay'\n",
    "#         )\n",
    "#     ).alias(\"appealType\")\n",
    "#     )\n",
    "\n",
    "# # .withColumn(\"caseManagementCategory\", \n",
    "# #     expr(\"\"\"\n",
    "# #     struct(\n",
    "# #         struct(\n",
    "# #             caseManagementCategory as code,\n",
    "# #             caseManagementCategory as label\n",
    "# #         ) as value,\n",
    "# #         array(\n",
    "# #             struct(\n",
    "# #                 caseManagementCategory as code,\n",
    "# #                 caseManagementCategory as label\n",
    "# #             )\n",
    "# #         ) as list_items\n",
    "# #     )\"\"\")\n",
    "\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca8fd74e-8a0b-4196-b1ff-7cfcc6eda700",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check for Duplicates in Silver Appeal Tables"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, count\n",
    "\n",
    "# # Reading tables into DataFrames and labeling as M1 to M9\n",
    "# M1 = spark.table(\"ariadm_active_appeals.silver_appealcase_detail\").distinct()\n",
    "# M2 = spark.table(\"ariadm_active_appeals.silver_caseapplicant_detail\")\n",
    "# M3 = spark.table(\"ariadm_active_appeals.silver_status_detail\")\n",
    "# M4 = spark.table(\"ariadm_active_appeals.silver_transaction_detail\")\n",
    "# M5 = spark.table(\"ariadm_active_appeals.silver_link_detail\")\n",
    "# M6 = spark.table(\"ariadm_active_appeals.silver_adjudicator_detail\")\n",
    "# M7 = spark.table(\"ariadm_active_appeals.silver_appealcategory_detail\")\n",
    "# M8 = spark.table(\"ariadm_active_appeals.silver_documentsreceived_detail\")\n",
    "# M9 = spark.table(\"ariadm_active_appeals.silver_history_detail\")\n",
    "\n",
    "# # Function to check for duplicates\n",
    "# def check_duplicates(df, table_name):\n",
    "#     duplicates = df.groupBy(\"caseno\").agg(count(\"*\").alias(\"count\")).filter(col(\"count\") > 1)\n",
    "#     if duplicates.count() > 0:\n",
    "#         displayHTML(f\"<span style='color:red;'>&#x274C; Table {table_name} has duplicates.</span>\")\n",
    "#     else:\n",
    "#         displayHTML(f\"<span style='color:green;'>&#x2705; Table {table_name} has no duplicates.</span>\")\n",
    "\n",
    "# # Check for duplicates in each table\n",
    "# check_duplicates(M1, \"silver_appealcase_detail\")\n",
    "# check_duplicates(M2, \"silver_caseapplicant_detail\")\n",
    "# check_duplicates(M3, \"silver_status_detail\")\n",
    "# check_duplicates(M4, \"silver_transaction_detail\")\n",
    "# check_duplicates(M5, \"silver_link_detail\")\n",
    "# check_duplicates(M6, \"silver_adjudicator_detail\")\n",
    "# check_duplicates(M7, \"silver_appealcategory_detail\")\n",
    "# check_duplicates(M8, \"silver_documentsreceived_detail\")\n",
    "# check_duplicates(M9, \"silver_history_detail\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4742786875901377,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "GOLD_PAYMENT_PENDING_JSON",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
