{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca145479-3655-4bf2-bafa-8c2894258a3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ARM Acknowledgment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81ef4be3-3c60-450c-91c7-9d09d193aea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Autoloader set up**  \n",
    "This Notebook sets up an Autoloader job that runs on a manual trigger to collect ack messages from the ack eventhubs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ab6f686-9bdf-4e7c-8537-52a7097d6569",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, LongType\n",
    "from pyspark.sql.functions import col,from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "822a9151-b14c-4ebe-a59d-13e01a0e8a8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"DatabricksWorkflow\")\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "if not logger.hasHandlers():\n",
    "    logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9e19871-35ae-4223-9b53-5be0335bf7f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "segment = 'SBAILS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c41be175-1d89-4a6f-9135-82e7f162705e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define ack schema"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "ack_schema = StructType([\n",
    "    StructField(\"filename\", StringType(), True),\n",
    "    StructField(\"http_response\", IntegerType(),True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"http_message\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bb97544-5eab-4a87-9ac1-9162caf100b7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set environment variables"
    }
   },
   "outputs": [],
   "source": [
    "#Load configuration JSON\n",
    "config_path = \"dbfs:/configs/config.json\"\n",
    "try:\n",
    "    config = spark.read.option(\"multiline\", \"true\").json(config_path)\n",
    "    logger.info(f\"Successfully read config file from {config_path}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not read config file at {config_path}: {e}\", exc_info=True)\n",
    "    raise FileNotFoundError(f\"Could not read config file at {config_path}: {e}\")\n",
    "\n",
    "#Extract environment and lz_key\n",
    "try:\n",
    "    first_row = config.first()\n",
    "    env = first_row[\"env\"].strip().lower()\n",
    "    lz_key = first_row[\"lz_key\"].strip().lower()\n",
    "    logger.info(f\"Extracted configs: env={env}, lz_key={lz_key}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Missing expected keys 'env' or 'lz_key' in config file: {e}\", exc_info=True)\n",
    "    raise KeyError(f\"Missing expected keys 'env' or 'lz_key' in config file: {e}\")\n",
    "\n",
    "#Construct keyvault name\n",
    "try:\n",
    "    keyvault_name = f\"ingest{lz_key}-meta002-{env}\"\n",
    "    logger.info(f\"Constructed keyvault name: {keyvault_name}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error constructing keyvault name: {e}\", exc_info=True)\n",
    "    raise ValueError(f\"Error constructing keyvault name: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18cc2dcb-1b4f-486f-b4a4-b1600210722e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get keyvault secrets"
    }
   },
   "outputs": [],
   "source": [
    "# Access the Service Principal secrets from Key Vault\n",
    "try:\n",
    "    client_secret = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-SECRET')\n",
    "    logger.info(\"Successfully retrieved SERVICE-PRINCIPLE-CLIENT-SECRET from Key Vault\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not retrieve 'SERVICE-PRINCIPLE-CLIENT-SECRET' from Key Vault '{keyvault_name}': {e}\", exc_info=True)\n",
    "    raise KeyError(f\"Could not retrieve 'SERVICE-PRINCIPLE-CLIENT-SECRET' from Key Vault '{keyvault_name}': {e}\")\n",
    "\n",
    "try:\n",
    "    tenant_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-TENANT-ID')\n",
    "    logger.info(\"Successfully retrieved SERVICE-PRINCIPLE-TENANT-ID from Key Vault\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not retrieve 'SERVICE-PRINCIPLE-TENANT-ID' from Key Vault '{keyvault_name}': {e}\", exc_info=True)\n",
    "    raise KeyError(f\"Could not retrieve 'SERVICE-PRINCIPLE-TENANT-ID' from Key Vault '{keyvault_name}': {e}\")\n",
    "\n",
    "try:\n",
    "    client_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-ID')\n",
    "    logger.info(\"Successfully retrieved SERVICE-PRINCIPLE-CLIENT-ID from Key Vault\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not retrieve 'SERVICE-PRINCIPLE-CLIENT-ID' from Key Vault '{keyvault_name}': {e}\", exc_info=True)\n",
    "    raise KeyError(f\"Could not retrieve 'SERVICE-PRINCIPLE-CLIENT-ID' from Key Vault '{keyvault_name}': {e}\")\n",
    "\n",
    "logger.info(\"✅ Successfully retrieved all Service Principal secrets from Key Vault\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6fb560f-ff5e-403e-9327-d776d0d57aae",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define eventhub namespace"
    }
   },
   "outputs": [],
   "source": [
    "EH_NAMESPACE = f\"ingest{lz_key}-integration-eventHubNamespace001-{env}\"\n",
    "EH_NAME = f\"evh-sbl-ack-{lz_key}-uks-dlrm-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8342a92c-962b-49b0-a1a4-5955b9a5fcf2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kafka options"
    }
   },
   "outputs": [],
   "source": [
    "connection_string = dbutils.secrets.get(keyvault_name, \"RootManageSharedAccessKey\")\n",
    "\n",
    "KAFKA_OPTIONS = {\n",
    "    \"kafka.bootstrap.servers\": f\"{EH_NAMESPACE}.servicebus.windows.net:9093\",\n",
    "    \"subscribe\": EH_NAME,\n",
    "    \"startingOffsets\": \"earliest\",\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"failOnDataLoss\": \"false\",\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.sasl.jaas.config\": f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{connection_string}\";'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dc97cc1-45e9-4b46-a9d4-fbfb9c26c456",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "set config"
    }
   },
   "outputs": [],
   "source": [
    "# --- Parameterise containers ---\n",
    "curated_storage_account = f\"ingest{lz_key}curated{env}\"\n",
    "curated_container = \"gold\"\n",
    "silver_curated_container = \"silver\"\n",
    "checkpoint_storage_account = f\"ingest{lz_key}xcutting{env}\"\n",
    "\n",
    "# --- Assign OAuth to storage accounts ---\n",
    "storage_accounts = [curated_storage_account, checkpoint_storage_account]\n",
    "\n",
    "for storage_account in storage_accounts:\n",
    "    try:\n",
    "        configs = {\n",
    "            f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\": \"OAuth\",\n",
    "            f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\":\n",
    "                \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "            f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\": client_id,\n",
    "            f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\": client_secret,\n",
    "            f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\":\n",
    "                f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "        }\n",
    "\n",
    "        for key, val in configs.items():\n",
    "            try:\n",
    "                spark.conf.set(key, val)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to set Spark config '{key}' for storage account '{storage_account}': {e}\", exc_info=True)\n",
    "                raise RuntimeError(f\"Failed to set Spark config '{key}' for storage account '{storage_account}': {e}\")\n",
    "\n",
    "        logger.info(f\"✅ Successfully configured OAuth for storage account: {storage_account}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error configuring OAuth for storage account '{storage_account}': {e}\", exc_info=True)\n",
    "        raise RuntimeError(f\"Error configuring OAuth for storage account '{storage_account}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85ec23a3-4f23-41b5-9697-6b40059bfda3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define path variables"
    }
   },
   "outputs": [],
   "source": [
    "# Container and path for storing Delta table (in curated storage account)\n",
    "data_path = f\"abfss://silver@ingest{lz_key}curated{env}.dfs.core.windows.net/ARIADM/ARM/AUDIT/{segment}/sbl_ack_audit_table\"\n",
    "\n",
    "# Container and path for checkpoint (in xcuttings storage account)\n",
    "checkpoint_path = f\"abfss://db-ack-checkpoint@ingest{lz_key}xcutting{env}.dfs.core.windows.net/{segment}/ACK/ack\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02662951-59fb-4fe3-a1cd-b1981b06f91f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_data_path = f\"abfss://gold@ingest{lz_key}curated{env}.dfs.core.windows.net/ARIADM/ARM/{segment}/\"\n",
    "# HTML Count\n",
    "html_path = f\"{gold_data_path}HTML/*.html\"\n",
    "html_df = spark.read.format(\"binaryFile\").load(html_path)\n",
    "\n",
    "json_path = f\"{gold_data_path}JSON/*.json\"\n",
    "json_df = spark.read.format(\"binaryFile\").load(json_path)\n",
    "\n",
    "a360_path = f\"{gold_data_path}A360/*.a360\"\n",
    "a360_df = spark.read.format(\"binaryFile\").load(a360_path)\n",
    "\n",
    "expected_html = html_df.count()\n",
    "expected_json = json_df.count()\n",
    "expected_a360 = a360_df.count()\n",
    "\n",
    "logger.info(f\"Expected HTML: {expected_html}\")\n",
    "logger.info(f\"Expected JSON: {expected_json}\")\n",
    "logger.info(f\"Expected A360: {expected_a360}\")\n",
    "\n",
    "#1a. how many records in source data? (prod aria db) or we can use outputs of segmentation tables eg gold tables\n",
    "#1b json content, html content should be the same as what we have established in the source\n",
    "#1c a360 is %% (mod) 250\n",
    "## expected pre-publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb341325-5340-4b6a-89d0-9aafdd47b1c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eventhubdf = spark.readStream.format(\"kafka\")\\\n",
    "    .options(**KAFKA_OPTIONS)\\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4cb5a90-4074-47c1-8c93-bbe76876bc0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parsed_df = (\n",
    "    eventhubdf\n",
    "    # 'body' is binary, so we cast to string (assuming UTF-8)\n",
    "    .select(col(\"value\").cast(\"string\").alias(\"json_str\"))\n",
    "    .select(from_json(col(\"json_str\"), ack_schema).alias(\"json_obj\"))\n",
    "    .select(\"json_obj.*\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "869db964-6118-467d-aa16-9762d77efed6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_processed_counts():\n",
    "    df = spark.read.format(\"delta\").load(data_path)\n",
    "    \n",
    "    html_count = df.filter(col(\"filename\").endswith(\".html\")).select(\"filename\").distinct().count()\n",
    "    json_count = df.filter((col(\"filename\")).endswith(\".json\")).select(\"filename\").distinct().count()\n",
    "    a360_count = df.filter((col(\"filename\")).endswith(\".a360\")).select(\"filename\").distinct().count()\n",
    "\n",
    "    return html_count, json_count, a360_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ea47fa2-e4ee-4f46-921e-dc56b46f4b64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = parsed_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c1d82b0-5f95-4bbe-bfef-d2ff0305cc99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# wait 60 seconds before the reconcilliaiton checks\n",
    "max_attempt = 5\n",
    "delay = 30 # seconds\n",
    "\n",
    "for attempt in range(1,max_attempt):\n",
    "    try:\n",
    "        html_count, json_count, a360_count = get_processed_counts()\n",
    "    except Exception as e:\n",
    "        if attempt < max_attempt:\n",
    "            print(f\"Attempt {attempt} failed: {e}. Retrying in {delay} seconds... \")\n",
    "            time.sleep(60)\n",
    "        else:\n",
    "            print(\"Failed to get processed counts after {max_attempt} attempts: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6d43378-9e76-4f38-9376-c23a09023204",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "while query.isActive:\n",
    "    html_count, json_count, a360_count = get_processed_counts()\n",
    "    logger.info(f\"\\nStatus HTML: {html_count}/{expected_html} \\nStatus JSON: {json_count}/{expected_json} \\nStatus A360: {a360_count}/{expected_a360}\")\n",
    "\n",
    "    if (\n",
    "        html_count >= expected_html\n",
    "        and json_count >= expected_json\n",
    "        and a360_count >= expected_a360\n",
    "    ):\n",
    "        logger.info(\"All files processed\")\n",
    "        query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc805612-4412-4c1e-bf53-4d975146be7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_container = \"silver\"\n",
    "df_sbails_ack_audit_data = spark.read.format(\"delta\").load(f\"abfss://{silver_container}@{curated_storage_account}.dfs.core.windows.net/ARIADM/ARM/AUDIT/{segment}/bl_ack_audit_table/\")\n",
    "df_sbails_ack_audit_data.createOrReplaceTempView(\"sbails_acknowledge_audit_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32c02164-124c-4541-ba82-fb3e3115836d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "A360 table"
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "with window_func as (\n",
    "SELECT filename,\n",
    "            CASE WHEN http_response = 201 THEN 'Success' ELSE 'Failure' END AS http_response_status,\n",
    "            timestamp,\n",
    "            ROW_NUMBER() OVER (PARTITION BY filename ORDER BY timestamp DESC) as rn\n",
    "\n",
    "FROM sbails_acknowledge_audit_data)\n",
    "SELECT filename, \n",
    "        http_response_status,\n",
    "        timestamp\n",
    "FROM window_func where rn=1 and filename LIKE '%.a360'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "967e6f43-13b5-4bf3-b997-cb9750e864eb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JSON table"
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "with window_func as (\n",
    "SELECT filename,\n",
    "            CASE WHEN http_response = 201 THEN 'Success' ELSE 'Failure' END AS http_response_status,\n",
    "            timestamp,\n",
    "            ROW_NUMBER() OVER (PARTITION BY filename ORDER BY timestamp DESC) as rn\n",
    "\n",
    "FROM sbails_acknowledge_audit_data)\n",
    "SELECT filename, \n",
    "        http_response_status,\n",
    "        timestamp\n",
    "FROM window_func where rn=1 and filename LIKE '%.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15020bf2-c78e-4d1e-9605-01ea380acc1d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "HMTL table"
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "with window_func as (\n",
    "SELECT filename,\n",
    "            CASE WHEN http_response = 201 THEN 'Success' ELSE 'Failure' END AS http_response_status,\n",
    "            timestamp,\n",
    "            ROW_NUMBER() OVER (PARTITION BY filename ORDER BY timestamp DESC) as rn\n",
    "\n",
    "FROM sbails_acknowledge_audit_data)\n",
    "SELECT filename, \n",
    "        http_response_status,\n",
    "        timestamp\n",
    "FROM window_func where rn=1 and filename LIKE '%.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0c8687c-3734-4dab-9291-917ef0574656",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "HTML"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    WITH results AS (\n",
    "        SELECT \n",
    "            split_part(filename, '.', -1) AS file_extension,\n",
    "            COUNT(CASE WHEN http_response = 201 AND http_message = 'Created' THEN 1 END) AS count_of_successful_eventhub_responses,\n",
    "            COUNT(CASE WHEN http_response <> 201 AND http_message <> 'Created' THEN 1 END) AS count_of_unsuccessful_eventhub_responses\n",
    "        FROM sbails_acknowledge_audit_data\n",
    "        \n",
    "        WHERE split_part(filename, '.', -1) = 'html'\n",
    "        GROUP BY split_part(filename, '.', -1)\n",
    "    )\n",
    "    SELECT\n",
    "        file_extension AS file_type,\n",
    "        {expected_html} AS total_expected_html_eventhub_responses,\n",
    "        CASE WHEN file_extension = 'html' THEN count_of_successful_eventhub_responses ELSE NULL END AS `total_sent_html_eventhub_responses`,\n",
    "        CASE WHEN file_extension = 'html' THEN concat(ROUND((count_of_successful_eventhub_responses / {expected_html}) * 100, 2), \"%\") ELSE NULL END AS `%total_expected_html_eventhub_responses`\n",
    "\n",
    "    FROM results\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09d45fc1-103e-4e9a-a145-101a136e085f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JSON"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    WITH results AS (\n",
    "        SELECT \n",
    "            split_part(filename, '.', -1) AS file_extension,\n",
    "            COUNT(CASE WHEN http_response = 201 AND http_message = 'Created' THEN 1 END) AS count_of_successful_eventhub_responses,\n",
    "            COUNT(CASE WHEN http_response <> 201 AND http_message <> 'Created' THEN 1 END) AS count_of_unsuccessful_eventhub_responses\n",
    "        FROM sbails_acknowledge_audit_data\n",
    "        WHERE split_part(filename, '.', -1) = 'json'\n",
    "        GROUP BY split_part(filename, '.', -1)\n",
    "    )\n",
    "    SELECT\n",
    "        file_extension AS file_type,\n",
    "        {expected_html} AS total_expected_html_eventhub_responses,\n",
    "        CASE WHEN file_extension = 'json' THEN count_of_successful_eventhub_responses ELSE NULL END AS `total_sent_json_eventhub_responses`,\n",
    "        CASE WHEN file_extension = 'json' THEN concat(ROUND((count_of_successful_eventhub_responses / {expected_html}) * 100, 2), \"%\") ELSE NULL END AS `%total_expected_json_eventhub_responses`\n",
    "    FROM results\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9dc4172-0061-451b-8214-55b86c1911d1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "A360"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    WITH results AS (\n",
    "        SELECT \n",
    "            split_part(filename, '.', -1) AS file_extension,\n",
    "            COUNT(CASE WHEN http_response = 201 AND http_message = 'Created' THEN 1 END) AS count_of_successful_eventhub_responses,\n",
    "            COUNT(CASE WHEN http_response <> 201 AND http_message <> 'Created' THEN 1 END) AS count_of_unsuccessful_eventhub_responses\n",
    "        FROM sbails_acknowledge_audit_data\n",
    "        WHERE split_part(filename, '.', -1) = 'a360'\n",
    "        GROUP BY split_part(filename, '.', -1)\n",
    "    )\n",
    "    SELECT\n",
    "        file_extension AS file_type,\n",
    "        {expected_a360} AS total_expected_a360_eventhub_responses,\n",
    "        CASE WHEN file_extension = 'a360' THEN CEIL(count_of_successful_eventhub_responses, {expected_a360}) ELSE NULL END AS `count_of_successful_a360_responses`,\n",
    "        CASE WHEN file_extension = 'a360' THEN concat(ROUND((count_of_successful_eventhub_responses / {expected_a360}) * 100, 2), \"%\") ELSE NULL END AS `%total_expected_a360_eventhub_responses`\n",
    "        \n",
    "        FROM results\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d599af1-e9e5-4b70-95a7-712a1e351918",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import create audit table"
    }
   },
   "outputs": [],
   "source": [
    "silver_container = \"silver\"\n",
    "sbl_cr_audit_table = spark.read.format(\"delta\").load(f\"abfss://{silver_container}@{curated_storage_account}.dfs.core.windows.net/ARIADM/ARM/AUDIT/{segment}/sbl_cr_audit_table/\")\n",
    "sbl_cr_audit_table.createOrReplaceTempView(\"sbl_cr_audit_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c7177ec-be08-4a4d-95b0-ed89554e228e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "WITH CTE AS (\n",
    "  SELECT \n",
    "    COUNT(Unique_Identifier) AS count_of_gold_records, \n",
    "    Table_Name, \n",
    "    Stage_Name \n",
    "  FROM sbl_cr_audit_table \n",
    "  WHERE Stage_Name IN ('segmentation_stage', 'gold_stage') \n",
    "    AND Table_Name IN (\n",
    "      'silver_normal_bail', \n",
    "      'create_bails_json_content', \n",
    "      'create_bails_html_content', \n",
    "      'gold_bails_a360',\n",
    "      'save_html_to_blob',\n",
    "      'save_json_to_blob'\n",
    "    )\n",
    "  GROUP BY Table_Name, Stage_Name\n",
    "  ORDER BY Stage_Name DESC\n",
    ")\n",
    "SELECT\n",
    "  count_of_gold_records,\n",
    "  Table_Name,\n",
    "  Stage_Name\n",
    "FROM CTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46dda17e-a48d-468b-ad65-0121b74311a7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Publish audit table"
    }
   },
   "outputs": [],
   "source": [
    "df_sbails_pub_audit_db_eh_audit_data = spark.read.format(\"delta\").load(f\"abfss://{silver_container}@{curated_storage_account}.dfs.core.windows.net/ARIADM/ARM/AUDIT/{segment}/bl_pub_audit_table/\")\n",
    "df_bails_pub_audit_db_eh_audit_data.createOrReplaceTempView(\"sbl_publish_audit_db_eh_data\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "      SELECT \n",
    "      split_part(file_name, '.', -1) as file_extension,\n",
    "      {expected_html} as total_expected_html_eventhub_responses,\n",
    "      {expected_json} as total_expected_json_eventhub_responses,\n",
    "      {expected_a360} as total_expected_a360_eventhub_responses,\n",
    "      COUNT(CASE WHEN status = 'success' THEN 1 END) AS count_of_successful_eventhub_responses,\n",
    "      COUNT(CASE WHEN status <> 'success' THEN 1 END) AS count_of_unsuccessful_eventhub_responses,\n",
    "      concat(((count_of_successful_eventhub_responses/total_expected_html_eventhub_responses) * 100), \"%\") as `%_of_successful_eventhub_responses`\n",
    "\n",
    "FROM sbl_publish_audit_db_eh_data\n",
    "GROUP BY file_extension\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "447f54e2-6704-49aa-96db-7da38fec1139",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "A360 Publish"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    WITH results AS (\n",
    "        SELECT \n",
    "            split_part(file_name, '.', -1) AS file_extension,\n",
    "            COUNT(CASE WHEN status = 'success' THEN 1 END) AS count_of_successful_eventhub_responses,\n",
    "            COUNT(CASE WHEN status != 'success' THEN 1 END) AS count_of_unsuccessful_eventhub_responses\n",
    "        FROM sbl_publish_audit_db_eh_data\n",
    "        WHERE split_part(file_name, '.', -1) = 'a360'\n",
    "        GROUP BY split_part(file_name, '.', -1)\n",
    "    )\n",
    "    SELECT\n",
    "        file_extension AS file_type,\n",
    "        {expected_a360} AS total_expected_a360_eventhub_responses,\n",
    "        count_of_successful_eventhub_responses AS count_of_successful_a360_responses,\n",
    "        concat(\n",
    "            ROUND(\n",
    "                (count_of_successful_eventhub_responses / {expected_a360}) * 100, 2\n",
    "            ), \"%\"\n",
    "        ) AS `%total_expected_a360_eventhub_responses`\n",
    "    FROM results\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90a87e77-0465-4940-8e37-f68d9d68e82c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "HTML Publish"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    WITH results AS (\n",
    "        SELECT \n",
    "            split_part(file_name, '.', -1) AS file_extension,\n",
    "            COUNT(CASE WHEN status = 'success' THEN 1 END) AS count_of_successful_eventhub_responses,\n",
    "            COUNT(CASE WHEN status != 'success' THEN 1 END) AS count_of_unsuccessful_eventhub_responses\n",
    "        FROM sbl_publish_audit_db_eh_data\n",
    "        WHERE split_part(file_name, '.', -1) = 'html'\n",
    "        GROUP BY split_part(file_name, '.', -1)\n",
    "    )\n",
    "    SELECT\n",
    "        file_extension AS file_type,\n",
    "        {expected_html} AS total_expected_html_eventhub_responses,\n",
    "        count_of_successful_eventhub_responses AS count_of_successful_html_responses,\n",
    "        concat(\n",
    "            ROUND(\n",
    "                (count_of_successful_eventhub_responses / {expected_html}) * 100, 2\n",
    "            ), \"%\"\n",
    "        ) AS `%total_expected_html_eventhub_responses`\n",
    "    FROM results\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b14def78-f9cd-48b9-a14b-fc93d7b88940",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JSON Acknowledge"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    WITH results AS (\n",
    "        SELECT \n",
    "            split_part(file_name, '.', -1) AS file_extension,\n",
    "            COUNT(CASE WHEN status = 'success' THEN 1 END) AS count_of_successful_eventhub_responses,\n",
    "            COUNT(CASE WHEN status != 'success' THEN 1 END) AS count_of_unsuccessful_eventhub_responses\n",
    "        FROM sbl_publish_audit_db_eh_data\n",
    "        WHERE split_part(file_name, '.', -1) = 'json'\n",
    "        GROUP BY split_part(file_name, '.', -1)\n",
    "    )\n",
    "    SELECT\n",
    "        file_extension AS file_type,\n",
    "        {expected_json} AS total_expected_json_eventhub_responses,\n",
    "        count_of_successful_eventhub_responses AS count_of_successful_json_responses,\n",
    "        concat(\n",
    "            ROUND(\n",
    "                (count_of_successful_eventhub_responses / {expected_json}) * 100, 2\n",
    "            ), \"%\"\n",
    "        ) AS `%total_expected_json_eventhub_responses`\n",
    "    FROM results\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb326d7a-d859-4bd5-af06-c034ae3ac1e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc06dc38-0b8d-4a22-8910-5f3d59e2720a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2932fe7e-1bad-428d-bf45-f4e281263f55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1640307-70bd-4699-8108-d07195bd4954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Notebook completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bails_ack_autoloader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
