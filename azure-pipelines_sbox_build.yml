trigger:
  branches:
    include:
      - main

pool:
  vmImage: 'ubuntu-latest'

parameters:
  - name: environment_components
    type: object
    default:
      - deployment: sbox_aria
        environment: sbox
        service_connection: DTS-DATAINGEST-SBOX
        pool_name: 'hmcts-sds-ptlsbox'
        segments: ['bails', 'apluta', 'td', 'joh', 'aplfpa', 'aplfta', 'sbails']
        landing_zones: 
          - landing_zone: '00'
            databricks_host: 'https://adb-3635282203417052.12.azuredatabricks.net'
            databricks_token: "$(DATABRICKS_PAT_SBOX00)" 
            Client_ID: $(SBOX_CLIENT_ID)
            Subscription_ID: $(SBOX_SUBSCRIPTION_ID)
            HTML_StorageAccount: 'ingest00landingsbox'
            Resource_Group: 'ingest00-main-sbox'
            Key_Vault_Name: 'ingest00-meta002-sbox'
          - landing_zone: '01'
            databricks_host: 'https://adb-376876256300083.3.azuredatabricks.net'
            databricks_token: "$(DATABRICKS_PAT_SBOX01)" 
            Client_ID: $(SBOX_CLIENT_ID)
            Subscription_ID: $(SBOX_SUBSCRIPTION_ID)
            HTML_StorageAccount: 'ingest01landingsbox'
            Resource_Group: 'ingest01-main-sbox'
            Key_Vault_Name: 'ingest01-meta002-sbox'
          - landing_zone: '02'
            databricks_host: 'https://adb-1879076228317698.18.azuredatabricks.net'
            databricks_token: "$(DATABRICKS_PAT_SBOX02)" 
            Client_ID: $(SBOX_CLIENT_ID)
            Subscription_ID: $(SBOX_SUBSCRIPTION_ID)
            HTML_StorageAccount: 'ingest02landingsbox'
            Resource_Group: 'ingest02-main-sbox'
            Key_Vault_Name: 'ingest02-meta002-sbox'

      # - deployment: stg_aria
      #   environment: stg
      #   service_connection: DTS-DATAINGEST-STG
      #   dependsOn: sbox_aria
      #   pool_name: 'hmcts-sds-ptl'
      #   landing_zones:
      #     - landing_zone: '00'
      #       HTML_StorageAccount: 'ingest00landingstg'
      #       Resource_Group: 'ingest00-main-stg'
      #       Client_ID: $(STG_CLIENT_ID)
      #       Subscription_ID: $(STG_SUBSCRIPTION_ID)
      #       databricks_host: 'https://adb-4305432441461530.10.azuredatabricks.net'
      #       databricks_token: "$(STG_DATABRICKS_TOKEN_WORKSPACE)"

      # - deployment: prod_aria
      #   environment: prod
      #   service_connection: DTS-DATAINGEST-PROD
      #   dependsOn: stg_aria
      #   pool_name: 'hmcts-sds-ptl'
      #   landing_zones: 
      #     - landing_zone: '00'
      #       HTML_StorageAccount: 'ingest00landingprod'
      #       Resource_Group: 'ingest00-main-prod'
      #       Client_ID: $(PROD_CLIENT_ID)
      #       Subscription_ID: $(PROD_SUBSCRIPTION_ID)
  
stages:
  - stage: Init
    displayName: 'No dependency stage'
    jobs:
      - job: BuildJob
        displayName: 'Build Job'
        steps:
          - script: echo 'Initial stage set-up'
            displayName: 'Initial stage set-up'

  - ${{ each deployment in parameters.environment_components }}:
    - ${{ if or(eq(deployment.environment, 'stg'), eq(deployment.environment, 'prod')) }}:
      - stage: Manual_Approval_${{ deployment.environment }}
        displayName: 'Manual Approval for ${{ deployment.environment }}'
        dependsOn: Init
        jobs:
          - job: waitForValidation
            displayName: Wait for Manual Validation
            pool: server
            steps:
              - task: ManualValidation@1
                timeoutInMinutes: 23160
                inputs:
                  approvers: andrew.mcdevitt@hmcts.net, ara.islam1@hmcts.net
                  instructions: Please validate the build configuration and resume

  # Build shared wheel first as a single operation per environment
  - ${{ each deployment in parameters.environment_components }}:
    - stage: ${{ format('{0}_Build_Wheel', deployment.deployment) }}
      displayName: 'Build Shared Wheel for ${{ deployment.deployment }}'
      ${{ if or(eq(deployment.environment, 'stg'), eq(deployment.environment, 'prod')) }}:
        dependsOn: Manual_Approval_${{ deployment.environment }}
      ${{ else }}:
        dependsOn: Init
      jobs:
        - job: BuildWheel
          displayName: 'Build Shared Python Wheel'
          steps:
            - task: UsePythonVersion@0
              displayName: 'Use Python 3.x'
              inputs:
                versionSpec: '3.x'

            - script: |
                python --version
                pip install --upgrade pip
                pip install build
                pip install databricks-cli

                echo "Listing dist contents:"
                ls -al $(Build.SourcesDirectory)/Databricks/SharedFunctionsLib/dist

                echo "removing files in dist/"
                rm -rf $(Build.SourcesDirectory)/Databricks/SharedFunctionsLib/dist

                echo "Listing dist contents after deletion:"
                ls -al $(Build.SourcesDirectory)/Databricks/SharedFunctionsLib/dist

                echo "Building wheel"
                python -m build --wheel --outdir $(Build.SourcesDirectory)/Databricks/SharedFunctionsLib/dist \
                  $(Build.SourcesDirectory)/Databricks/SharedFunctionsLib

                echo "Listing dist contents after building the wheel"
                ls -al $(Build.SourcesDirectory)/Databricks/SharedFunctionsLib/dist
              displayName: 'Build Shared Python Wheel'

            - task: PublishBuildArtifacts@1
              inputs:
                PathtoPublish: '$(Build.SourcesDirectory)/Databricks/SharedFunctionsLib/dist'
                ArtifactName: 'ariafunctionv2'
                publishLocation: 'Container'
              displayName: 'Publish Shared Wheel Artifact'

  # Build function app package for each segment
  - ${{ each deployment in parameters.environment_components }}:
    - ${{ each segment in deployment.segments }}:
      - stage: ${{ format('{0}_Package_{1}', deployment.deployment, segment) }}
        displayName: 'Package Azure Function: ${{ deployment.deployment }}-${{ segment }}'
        dependsOn: ${{ format('{0}_Build_Wheel', deployment.deployment) }}
        jobs:
          - job: BuildFunction
            displayName: 'Build Azure Function for ${{ segment }}'
            steps:
              - script: |
                  pip install --upgrade pip
                  pip install -r AzureFunctions/ARCHIVE/${{ segment }}/requirements.txt \
                    --target="./AzureFunctions/.python_packages_${{ segment }}/lib/site-packages"
                displayName: 'Install dependencies for ${{ segment }}'
              
              - task: PublishBuildArtifacts@1
                inputs:
                  PathtoPublish: '$(Build.SourcesDirectory)/AzureFunctions/ARCHIVE/${{ segment }}'
                  ArtifactName: 'functionapp-${{ segment }}'
                  publishLocation: 'Container'
                displayName: 'Publish Function App for ${{ segment }}'
  
  # Loop through for LZ 02 to deploy artifacts first
  - ${{ each deployment in parameters.environment_components }}:
    - ${{ each lz in deployment.landing_zones}}:
      - ${{ each segment in deployment.segments}}:
          - stage: Deploy_Functions_${{ deployment.environment }}${{lz.landing_zone}}_${{ segment }}
            displayName: 'Deploy Functions for ${{ deployment.environment }}${{lz.landing_zone}}_${{ segment }}'
            dependsOn: ${{ format('{0}_Package_{1}', deployment.deployment, segment) }}
            jobs:
              - job: Deploy_Function_${{ lz.landing_zone }}
                displayName: 'Deploy to LZ ${{ lz.landing_zone }}'
                steps:
                  - task: DownloadBuildArtifacts@1
                    inputs:
                      buildType: 'current'
                      downloadType: 'single'
                      artifactName: 'functionapp-${{ segment }}'
                      downloadPath: '$(Pipeline.Workspace)'
                    displayName: 'Download Azure Function Artifact for ${{ segment }}'

                  - task: AzureFunctionApp@2
                    inputs:
                      connectedServiceNameARM: ${{ deployment.service_connection }}
                      appType: 'functionAppLinux'
                      appName: 'af-${{ segment }}-${{ deployment.environment }}${{ lz.landing_zone }}-uks-dlrm-01'
                      package: '$(Pipeline.Workspace)/functionapp-${{ segment }}'
                      deploymentMethod: 'zipDeploy'
                    displayName: 'Deploy Azure Function for ${{ segment }}-${{ deployment.environment }}-${{ lz.landing_zone }}' ##vnet and kv access policies

  # Deploy HTML templates and Databricks code
  - ${{ each deployment in parameters.environment_components }}:
    - ${{ each lz in deployment.landing_zones}}:
      - stage: Deploy_HTML_Databricks_${{ deployment.environment }}_${{ lz.landing_zone }}
        displayName: 'Deploy HTML and Databricks code for ${{ deployment.environment }}${{ lz.landing_zone }}'
        dependsOn:
          - Deploy_Functions_${{ deployment.environment }}${{ lz.landing_zone }}_bails   
          - Deploy_Functions_${{ deployment.environment }}${{ lz.landing_zone }}_apluta
          - Deploy_Functions_${{ deployment.environment }}${{ lz.landing_zone }}_td
          - Deploy_Functions_${{ deployment.environment }}${{ lz.landing_zone }}_joh
          - Deploy_Functions_${{ deployment.environment }}${{ lz.landing_zone }}_aplfpa
          - Deploy_Functions_${{ deployment.environment }}${{ lz.landing_zone }}_aplfta
          - Deploy_Functions_${{ deployment.environment }}${{ lz.landing_zone }}_sbails

        jobs:
          - job: Deploy_Databricks_HTML_${{ deployment.environment }}_${{ lz.landing_zone }}
            displayName: 'Deploy to LZ ${{ lz.landing_zone }}'
            pool:
              vmImage: 'ubuntu-latest'
            steps:
              - task: UsePythonVersion@0
                displayName: 'Use Python 3.x'
                inputs:
                  versionSpec: '3.x'

              - script: |
                  pip install --upgrade pip
                  pip install databricks-cli
                displayName: 'Install Databricks CLI'

              - script: |
                  echo "[DEFAULT]" > $(HOME)/.databrickscfg
                  echo "host = ${{ lz.databricks_host }}" >> $(HOME)/.databrickscfg
                  echo "token = ${{ lz.databricks_token }}" >> $(HOME)/.databrickscfg
                displayName: "Configure Databricks CLI for ENV${{ deployment.environment }} LZ${{ lz.landing_zone }}"

              - script: |
                  databricks workspace delete /live --recursive || true
                  databricks workspace mkdirs /live
                  databricks workspace import_dir Databricks /live --overwrite
                displayName: "Publish to Databricks Workspace - /live for ${{ deployment.environment }}${{ lz.landing_zone }}"
            
          - job: Configure_HTML_Upload_${{ deployment.environment }}_${{ lz.landing_zone }}
            displayName: 'Upload HTML Templates to ${{ deployment.environment }}${{ lz.landing_zone }}'
            pool: 
              name: ${{ deployment.pool_name }}
            steps:
              - task: AzureCLI@2
                displayName: Configure Storage and Upload HTML Templates
                inputs:
                  azureSubscription: ${{ deployment.service_connection }}
                  scriptType: 'bash'
                  scriptLocation: 'inlineScript'
                  inlineScript: |
                    echo "Assigning SP Blob Contributor Access"
                    az role assignment create \
                      --assignee "${{ lz.Client_ID }}" \
                      --role "Storage Blob Data Contributor" \
                      --scope "/subscriptions/${{ lz.Subscription_ID }}/resourceGroups/${{ lz.Resource_Group }}/providers/Microsoft.Storage/storageAccounts/${{ lz.HTML_StorageAccount }}"

                    echo "Retrieving Storage Account Key"
                    storage_account_key=$(az storage account keys list \
                      --resource-group ${{ lz.Resource_Group }} \
                      --account-name ${{ lz.HTML_StorageAccount }} \
                      --query '[0].value' -o tsv)
                    echo "Success! ${{ lz.HTML_StorageAccount }} is $storage_account_key"

                    echo "Retrieving Blob Endpoint"
                    blob_endpoint=$(az storage account show \
                      --resource-group ${{ lz.Resource_Group }} \
                      --name ${{ lz.HTML_StorageAccount }} \
                      --query "primaryEndpoints.blob" -o tsv)
                    echo "Success! ${{ lz.HTML_StorageAccount }} is $blob_endpoint"

                    echo "Creating html-template container if not exists"
                    az storage container create \
                      --account-name ${{ lz.HTML_StorageAccount }} \
                      --name html-template \
                      --account-key $storage_account_key \
                      --blob-endpoint $blob_endpoint
                    echo "Success! html-template container created!"

                    echo "Uploading HTML Templates"
                    az storage blob upload-batch \
                      --account-name ${{ lz.HTML_StorageAccount }} \
                      --account-key $storage_account_key \
                      --destination html-template \
                      --source '$(Build.SourcesDirectory)/HTML_Templates/' \
                      --pattern '*.html' \
                      --overwrite true
                    echo "Success! HTML templates imported to ${{ lz.HTML_StorageAccount }}/html-template"
              
              # - task: AzureCLI@2
              #   displayName: Configure SAS Token generation for ${{ deployment.environment }}${{ lz.landing_zone }}
              #   inputs:
              #     azureSubscription: ${{ deployment.service_connection }}
              #     scriptType: 'bash'
              #     scriptLocation: 'inlineScript'
              #     inlineScript: |
              #       ENVIRONMENT="${{ deployment.environment }}"
              #       LZ="${{ lz.landing_zone }}"
              #       ENV_UPPER=$(echo "$ENVIRONMENT" | tr '[:lower:]' '[:upper:]')
                    
              #       STORAGE_ACCOUNT="ingest${LZ}curated${ENVIRONMENT}"
              #       KEYVAULT_NAME="ingest${LZ}-meta002-${ENVIRONMENT}"
                    
              #       echo "=== DEBUG INFORMATION ==="
              #       echo "Environment: $ENVIRONMENT"
              #       echo "Landing Zone: $LZ"
              #       echo "Storage Account Name: $STORAGE_ACCOUNT"
              #       echo "Key Vault Name: $KEYVAULT_NAME"
              #       echo "=========================="

              #       echo "Assigning SP Key Vault Secrets Officer Access"
              #       az role assignment create \
              #         --assignee "${{ lz.Client_ID }}" \
              #         --role "Key Vault Secrets Officer" \
              #         --scope "/subscriptions/${{ lz.Subscription_ID }}/resourceGroups/${{ lz.Resource_Group }}/providers/Microsoft.KeyVault/vaults/${{ lz.Key_Vault_Name }}"
              #       echo "Role Assigned"

              #       echo "Listing Key Vault Officer role assignments"
              #       az role assignment list \
              #         --assignee "${{ lz.Client_ID }}" \
              #         --scope "/subscriptions/${{ lz.Subscription_ID }}/resourceGroups/${{ lz.Resource_Group }}/providers/Microsoft.KeyVault/vaults/${{ lz.Key_Vault_Name }}" \
              #         --output table
                    
              #       # Test if storage account exists
              #       echo "Checking if storage account exists..." 
              #       if az storage account show --name "$STORAGE_ACCOUNT" --output table; then
              #         echo "✓ Storage account found"
              #       else
              #         echo "✗ Storage account not found or no access"
              #         exit 1
              #       fi
                    
              #       # Test if container exists
              #       echo "Checking if container 'dropzone' exists..."
              #       if az storage container show --name "dropzone" --account-name "$STORAGE_ACCOUNT" --auth-mode login --output table; then
              #         echo "✓ Container 'dropzone' found"
              #       else
              #         echo "✗ Container 'dropzone' not found"
              #         exit 1
              #       fi
                    
              #       echo "Generating SAS token for ${STORAGE_ACCOUNT}/dropzone"
                    
              #       curated_sas_token=$(az storage container generate-sas \
              #         --account-name "$STORAGE_ACCOUNT" \
              #         --name "dropzone" \
              #         --permissions rwdlacd \
              #         --expiry 2026-03-21T00:00:00Z \
              #         --start 2025-03-21T00:00:00Z \
              #         --https-only \
              #         --auth-mode key \
              #         --output tsv)
                    
              #       if [ -z "$curated_sas_token" ]; then
              #         echo "✗ Failed to generate SAS token"
              #         exit 1
              #       else
              #         echo "✓ SAS token generated successfully"
              #         echo "Token length: ${#curated_sas_token} characters"
              #       fi
                    
              #       # Store SAS token in Key Vault
              #       echo "Storing SAS token in Key Vault: $KEYVAULT_NAME"
                    
              #       if az keyvault secret set \
              #         --vault-name "$KEYVAULT_NAME" \
              #         --name "CURATED-${ENV_UPPER}-SAS-TOKEN-TEST" \
              #         --value "$curated_sas_token" \
              #         --output table; then
              #         echo "✓ SAS token stored successfully in Key Vault"
              #       else
              #         echo "✗ Failed to store SAS token in Key Vault"
              #         exit 1
              #       fi
                    
              #       echo "=== COMPLETION ==="
              #       echo "✓ Generated SAS token for ${STORAGE_ACCOUNT}/dropzone"
              #       echo "✓ Stored as: CURATED-${ENV_UPPER}-SAS-TOKEN-TEST"
              #       echo "=================="

  # Deploy Python wheel to Databricks as final stage
  - ${{ each deployment in parameters.environment_components }}:
    - ${{ each lz in deployment.landing_zones }}:
      - stage: Deploy_Wheel_To_Databricks_${{ deployment.environment }}_${{ lz.landing_zone }}_
        displayName: 'Deploy Wheel to Databricks for ${{ deployment.environment }}${{ lz.landing_zone }}'
        dependsOn: Deploy_HTML_Databricks_${{ deployment.environment }}_${{ lz.landing_zone }}
        jobs:
          - job:
            displayName: 'Deploy_Databricks_resources_to_${{ deployment.environment }}_${{ lz.landing_zone }}'
            pool: 
              vmImage: 'ubuntu-latest'
                
            steps:
            - task: DownloadBuildArtifacts@1
              inputs:
                buildType: 'current'
                downloadType: 'single'
                artifactName: 'ariafunctionv2'
                downloadPath: '$(Pipeline.Workspace)'
              displayName: 'Download Wheel Artifact'
            
            - script: |
                echo " Files contained within Pipeline.workspace: $(Pipeline.Workspace)"
                ls -al $(Pipeline.Workspace)

            - task: UsePythonVersion@0
              displayName: 'Use Python 3.x'
              inputs:
                versionSpec: '3.x'

            - script: |
                  /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
                  eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"
                  
                  brew install databricks/tap/databricks
                  databricks --version
              displayName: 'Install Databricks CLI'

            - script: |
                eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"
                export DATABRICKS_HOST="${{ lz.databricks_host }}"
                export DATABRICKS_TOKEN="${{ lz.databricks_token }}"

                databricks --version  
                echo "[DEFAULT]" > $(HOME)/.databrickscfg
                echo "host = ${{ lz.databricks_host }}" >> $(HOME)/.databrickscfg
                echo "token = ${{ lz.databricks_token }}" >> $(HOME)/.databrickscfg
                cat ~/.databrickscfg  # Print the contents for debugging
              displayName: 'Configure Databricks CLI'

            - script: |
                eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"

                set -e 

                echo "Searching for wheel files in: $(Pipeline.Workspace)" 
                find '$(Pipeline.Workspace)' -type f -name "*.whl"

                WHEEL_FILE=$(find '$(Pipeline.Workspace)' -type f -name "*.whl" | sort -V | head -n 1)

                if [ -n "$WHEEL_FILE" ]; then
                  echo "Found wheel file: $WHEEL_FILE"

                  FILENAME=$(basename "$WHEEL_FILE")
                  echo "Extracted filename: $FILENAME"

                  DEST_PATH="dbfs:/FileStore/shared_wheels/$FILENAME"
                  echo "Destination path: $DEST_PATH"

                  echo "creating directory dbfs:/FileStore/shared_wheels"
                  databricks fs mkdirs dbfs:/FileStore/shared_wheels    

                  echo "copying wheel file to DBFS..."
                  echo "copying $WHEEL_FILE to $DEST_PATH"
                  databricks fs cp "$WHEEL_FILE" "$DEST_PATH" --overwrite --debug

                  echo "Displaying content of dbfs:/FileStore/shared_wheels"
                  databricks fs ls dbfs:/FileStore/shared_wheels

                else
                  echo "No wheel file found"
                  exit 1
                fi
              displayName: 'Upload Wheel to Databricks DBFS'
                
            - script: |
                echo "Current dir: $(pwd)"
                ls -al /home/vsts/work/1/s
                eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"

                cd ARIA_DABs

                echo "Current dir: $(pwd)"
                ls -al 

                databricks bundle destroy -t ${{ deployment.environment }}${{ lz.landing_zone }}
                databricks bundle validate -t ${{ deployment.environment }}${{ lz.landing_zone }} --debug
                databricks bundle deploy -t ${{ deployment.environment }}${{ lz.landing_zone }} --debug
              displayName: "Deploy Databricks resources to ${{ deployment.environment }}-${{ lz.landing_zone }} workspace"