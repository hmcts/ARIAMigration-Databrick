{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a38346e-92cb-4fef-ad6d-f99b8906a10e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/configs/config.json\")\n",
    "env_name = config.first()[\"env\"].strip().lower()\n",
    "lz_key = config.first()[\"lz_key\"].strip().lower()\n",
    "\n",
    "print(f\"env_code: {lz_key}\")  # This won't be redacted\n",
    "print(f\"env_name: {env_name}\")  # This won't be redacted\n",
    "\n",
    "KeyVault_name = f\"ingest{lz_key}-meta002-{env_name}\"\n",
    "print(f\"KeyVault_name: {KeyVault_name}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0edc304a-bddd-40db-a487-f115ca0f68ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Service principal credentials\n",
    "client_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-ID\")\n",
    "client_secret = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-SECRET\")\n",
    "tenant_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-TENANT-ID\")\n",
    "\n",
    "# Storage account names\n",
    "curated_storage = f\"ingest{lz_key}curated{env_name}\"\n",
    "checkpoint_storage = f\"ingest{lz_key}xcutting{env_name}\"\n",
    "raw_storage = f\"ingest{lz_key}raw{env_name}\"\n",
    "landing_storage = f\"ingest{lz_key}landing{env_name}\"\n",
    "external_storage = f\"ingest{lz_key}external{env_name}\"\n",
    "\n",
    "\n",
    "# Spark config for curated storage (Delta table)\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{curated_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{curated_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{curated_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{curated_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{curated_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{checkpoint_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{checkpoint_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{checkpoint_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{checkpoint_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{checkpoint_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{raw_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{raw_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{raw_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{raw_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{raw_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{landing_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{landing_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{landing_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{landing_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{landing_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{external_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{external_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{external_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{external_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{external_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b29c0c2-575e-4a01-a94a-f5ce1bb9b77a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SAS_Token = dbutils.secrets.get(KeyVault_name, \"ARIATD-SAS-TOKEN\")\n",
    "sub_dir = 'ARIATD'\n",
    "account_url = \"https://a360c2x2555dz.blob.core.windows.net\"\n",
    "container_name = \"dropzone\"\n",
    "mnt_name = \"ariatd\"  # mnt name specified as ariatd\n",
    "mount_point = f\"/mnt/{mnt_name}\"\n",
    "\n",
    "# Drop mount if exists, then recreate\n",
    "if any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n",
    "    dbutils.fs.unmount(mount_point)\n",
    "\n",
    "dbutils.fs.mount(\n",
    "    source=f\"wasbs://{container_name}@a360c2x2555dz.blob.core.windows.net/{sub_dir}\",\n",
    "    mount_point=mount_point,\n",
    "    extra_configs={f\"fs.azure.sas.{container_name}.a360c2x2555dz.blob.core.windows.net\": SAS_Token}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "307ebfed-d366-475b-8b9f-3f04f475c086",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime, col\n",
    "\n",
    "df = spark.createDataFrame(dbutils.fs.ls(\"/mnt/ariatd/submission\"))\n",
    "df = df.withColumn(\"modificationTime_dt\", from_unixtime(col(\"modificationTime\") / 1000))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11014eb5-194c-4689-86bf-5380c876a2dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e506f1c3-cd03-41ed-a886-53dcc4772a5e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758798095722}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime, col, lower, when, max as spark_max, count\n",
    "\n",
    "df = spark.createDataFrame(dbutils.fs.ls(\"/mnt/ariatd/submission\"))\n",
    "df = df.withColumn(\"modificationTime_dt\", from_unixtime(col(\"modificationTime\") / 1000))\n",
    "\n",
    "# Count files by extension (.html, .json, .a360) and get max modification date per extension\n",
    "ext_col = lower(col(\"name\"))\n",
    "ext_counts = (\n",
    "    df.withColumn(\n",
    "        \"extension\",\n",
    "        when(ext_col.endswith(\".html\"), \".html\")\n",
    "        .when(ext_col.endswith(\".json\"), \".json\")\n",
    "        .when(ext_col.endswith(\".a360\"), \".a360\")\n",
    "    )\n",
    "    .filter(col(\"extension\").isNotNull())\n",
    "    .groupBy(\"extension\")\n",
    "    .agg(\n",
    "        spark_max(\"modificationTime_dt\").alias(\"max_modificationTime_UTC\"),\n",
    "        col(\"extension\"),\n",
    "        count(\"*\").alias(\"count\")\n",
    "    )\n",
    "    .select(\"extension\", \"count\", \"max_modificationTime_UTC\")\n",
    ")\n",
    "\n",
    "display(ext_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "054a8cc6-9a00-4f52-894a-c3524bb4b652",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read Submission"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "########################################################\n",
    "# Identify env and KeyVault_name\n",
    "#######################################################\n",
    "\n",
    "config = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/configs/config.json\")\n",
    "env_name = config.first()[\"env\"].strip().lower()\n",
    "lz_key = config.first()[\"lz_key\"].strip().lower()\n",
    "\n",
    "print(f\"env_code: {lz_key}\")  # This won't be redacted\n",
    "print(f\"env_name: {env_name}\")  # This won't be redacted\n",
    "\n",
    "KeyVault_name = f\"ingest{lz_key}-meta002-{env_name}\"\n",
    "print(f\"KeyVault_name: {KeyVault_name}\") \n",
    "######################################\n",
    "RECORD_CLASS = 'ARIATD'\n",
    "#######################################################\n",
    "\n",
    "SAS_Token = dbutils.secrets.get(KeyVault_name, f\"{RECORD_CLASS}-SAS-TOKEN\")\n",
    "storage_account_name = \"a360c2x2555dz\"\n",
    "container_name = \"dropzone\"\n",
    "sub_dir = f\"{RECORD_CLASS}/submission\"\n",
    "\n",
    "input_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{sub_dir}\"\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.{container_name}.{storage_account_name}.blob.core.windows.net\",\n",
    "    SAS_Token\n",
    ")\n",
    "\n",
    "df = spark.read.format(\"binaryFile\").load(input_path)\n",
    "display(df)\n",
    "\n",
    "# Count files by extension (.html, .json, .a360) and get max modification date per extension\n",
    "ext_col = lower(col(\"path\"))\n",
    "ext_counts = (\n",
    "    df.withColumn(\n",
    "        \"extension\",\n",
    "        when(ext_col.endswith(\".html\"), \".html\")\n",
    "        .when(ext_col.endswith(\".json\"), \".json\")\n",
    "        .when(ext_col.endswith(\".a360\"), \".a360\")\n",
    "    )\n",
    "    .filter(col(\"extension\").isNotNull())\n",
    "    .groupBy(\"extension\")\n",
    "    .agg(\n",
    "        max(\"modificationTime\").alias(\"max_modificationTime_UTC\"),\n",
    "        col(\"extension\"),\n",
    "        count(\"*\").alias(\"count\")\n",
    "    )\n",
    "    .select(\"extension\", \"count\", \"max_modificationTime_UTC\")\n",
    ")\n",
    "\n",
    "display(ext_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "827efec2-9053-4dda-bd0e-9c0849485d4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count files by extension (.html, .json, .a360) and get max modification date per extension\n",
    "ext_col = lower(col(\"path\"))\n",
    "ext_counts = (\n",
    "    df.withColumn(\n",
    "        \"extension\",\n",
    "        when(ext_col.endswith(\".html\"), \".html\")\n",
    "        .when(ext_col.endswith(\".json\"), \".json\")\n",
    "        .when(ext_col.endswith(\".a360\"), \".a360\")\n",
    "    )\n",
    "    .filter(col(\"extension\").isNotNull())\n",
    "    .groupBy(\"extension\")\n",
    "    .agg(\n",
    "        spark_max(\"modificationTime\").alias(\"max_modificationTime_UTC\"),\n",
    "        col(\"extension\"),\n",
    "        count(\"*\").alias(\"count\")\n",
    "    )\n",
    "    .select(\"extension\", \"count\", \"max_modificationTime_UTC\")\n",
    ")\n",
    "\n",
    "display(ext_counts)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-09-25 11_30_36",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
