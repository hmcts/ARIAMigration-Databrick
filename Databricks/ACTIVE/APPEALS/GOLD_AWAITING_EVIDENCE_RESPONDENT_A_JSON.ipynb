{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "253aca60-865e-47c6-acc7-8b66d9b7f07b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install /dbfs/FileStore/packages/shared_functions-0.5.2-py3-none-any.whl\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7831545c-1e19-4244-ab14-25869bbd78f7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import packages"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import *\n",
    "import uk_postcodes_parsing\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "from pyspark.sql.functions import col, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e251edd4-ffe0-47d2-9c90-2add21a1af59",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Assign configs"
    }
   },
   "outputs": [],
   "source": [
    "config = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/configs/config.json\")\n",
    "env_name = config.first()[\"env\"].strip().lower()\n",
    "lz_key = config.first()[\"lz_key\"].strip().lower()\n",
    "\n",
    "print(f\"env_code: {lz_key}\")  # This won't be redacted\n",
    "print(f\"env_name: {env_name}\")  # This won't be redacted\n",
    "\n",
    "KeyVault_name = f\"ingest{lz_key}-meta002-{env_name}\"\n",
    "print(f\"KeyVault_name: {KeyVault_name}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36f0c0bd-2026-4b2c-8800-f4732e142615",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Assign OAuth"
    }
   },
   "outputs": [],
   "source": [
    "# Service principal credentials\n",
    "client_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-ID\")\n",
    "client_secret = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-SECRET\")\n",
    "tenant_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-TENANT-ID\")\n",
    "\n",
    "# Storage account names\n",
    "curated_storage = f\"ingest{lz_key}curated{env_name}\"\n",
    "checkpoint_storage = f\"ingest{lz_key}xcutting{env_name}\"\n",
    "raw_storage = f\"ingest{lz_key}raw{env_name}\"\n",
    "landing_storage = f\"ingest{lz_key}landing{env_name}\"\n",
    "external_storage = f\"ingest{lz_key}external{env_name}\"\n",
    "\n",
    "\n",
    "# Spark config for curated storage (Delta table)\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{curated_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{curated_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{curated_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{curated_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{curated_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{checkpoint_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{checkpoint_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{checkpoint_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{checkpoint_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{checkpoint_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{raw_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{raw_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{raw_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{raw_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{raw_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{landing_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{landing_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{landing_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{landing_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{landing_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{external_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{external_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{external_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{external_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{external_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e169fc23-7079-4102-8017-e009060ec361",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Assign Paths"
    }
   },
   "outputs": [],
   "source": [
    "AppealState = \"awaitingRespondentEvidence(a)\"\n",
    "output_name = \"awaiting_respondent_evidence_a\"\n",
    "\n",
    "# Setting variables for use in subsequent cells\n",
    "bronze_path = f\"abfss://bronze@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/\"\n",
    "silver_path = f\"abfss://silver@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/\"\n",
    "audit_path = f\"abfss://silver@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/AUDIT/{AppealState}\"\n",
    "gold_outputs = f\"ARIADM/ACTIVE/CCD/APPEALS/{AppealState}\"\n",
    "\n",
    "# Print all variables\n",
    "variables = {\n",
    "    # \"read_hive\": read_hive,\n",
    "    \n",
    "    \"bronze_path\": bronze_path,\n",
    "    \"silver_path\": silver_path,\n",
    "    \"audit_path\": audit_path,\n",
    "    \"gold_outputs\": gold_outputs,\n",
    "    \"key_vault\": KeyVault_name,\n",
    "    \"AppealState\": AppealState\n",
    "\n",
    "}\n",
    "\n",
    "display(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf35cc30-e4f7-4bf8-aa28-451a231d48d1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Field group Functions"
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import shared_functions.paymentPending as PP\n",
    "import shared_functions.appealSubmitted as APS\n",
    "import shared_functions.AwaitingEvidenceRespondant_a as AERa\n",
    "import shared_functions.AwaitingEvidenceRespondant_b as AERb\n",
    "from shared_functions.DQRules import base_DQRules, build_rule_expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d30465e-ae52-4a05-9eab-bba6e0964456",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read in tables"
    }
   },
   "outputs": [],
   "source": [
    "silver_m1 = spark.table(\"ariadm_active_appeals.silver_appealcase_detail\").filter(col(\"dv_targetState\") == lit(AppealState)).distinct()\n",
    "silver_m2 = spark.table(\"ariadm_active_appeals.silver_caseapplicant_detail\") \n",
    "silver_m3 = spark.table(\"ariadm_active_appeals.silver_status_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "silver_m4 = spark.table(\"ariadm_active_appeals.silver_transaction_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "silver_m5 = spark.table(\"ariadm_active_appeals.silver_link_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "silver_c = spark.table(\"ariadm_active_appeals.silver_appealcategory_detail\")\n",
    "bronze_countryFromAddress = spark.table(\"ariadm_active_appeals.bronze_countries_countryFromAddress\").withColumn(\"lu_countryGovUkOocAdminJ\",col(\"countryGovUkOocAdminJ\"))\n",
    "bronze_HORef_cleansing = spark.table(\"ariadm_active_appeals.bronze_HORef_cleansing\")\n",
    "bronze_remission_lookup_df = spark.table(\"ariadm_active_appeals.bronze_remissions\").distinct()\n",
    "\n",
    "silver_h = spark.table(\"ariadm_active_appeals.silver_history_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "bronze_remissions_lookup_df = spark.table(\"ariadm_active_appeals.bronze_remissions\").distinct()\n",
    "bronze_countryFromAddress = spark.table(\"ariadm_active_appeals.bronze_countries_countryFromAddress\")\n",
    "bronze_HORef_cleansing = spark.table(\"ariadm_active_appeals.bronze_HORef_cleansing\")\n",
    "bronze_hearing_centres = spark.table(\"ariadm_active_appeals.bronze_hearing_centres\")\n",
    "bronze_derive_hearing_centres = spark.table(\"ariadm_active_appeals.bronze_derive_hearing_centres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "725183e4-68d6-4f35-b27f-07d7eee61392",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: appealType"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = PP.appealType(silver_m1)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d9d0d26-a1c2-4f16-83f5-a80b03efadfe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: CaseData"
    }
   },
   "outputs": [],
   "source": [
    "df,df_audit = PP.caseData(silver_m1, silver_m2, silver_m3, silver_h, bronze_hearing_centres, bronze_derive_hearing_centres)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09990ce8-21fa-4219-b5d0-13535ffb304d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: flagsLabels"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = PP.flagsLabels(silver_m1, silver_m2, silver_c)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f34c43f4-2863-49aa-a26b-59ce05ee4ae6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: legalRepDetails"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = PP.legalRepDetails(silver_m1)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad2db383-5eb6-45cb-b580-07e0b6c80327",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: appellantDetails"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = AERa.appellantDetails(silver_m1, silver_m2, silver_c, bronze_countryFromAddress,bronze_HORef_cleansing)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c07ba4a2-0028-4133-b7af-c80ca1359d98",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: homeOfficeDetails"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = PP.homeOfficeDetails(silver_m1, silver_m2, silver_c, bronze_HORef_cleansing)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17b67c33-e74d-4159-8ebd-d525cada317f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: paymentType"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = APS.paymentType(silver_m1, silver_m4)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c06653d4-f0de-4256-bc42-620efe620d81",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: PartyIDs"
    }
   },
   "outputs": [],
   "source": [
    "df ,df_audit = PP.partyID(silver_m1, silver_m3, silver_c)\n",
    "# display(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f606b4c-55c7-4c4d-9129-a48cf913d30a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: remissionTypes"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = APS.remissionTypes(silver_m1, bronze_remission_lookup_df, silver_m4)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3202d868-a71f-49e2-9571-5a28b812ed34",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: sponsorDetails"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = PP.sponsorDetails(silver_m1, silver_c)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78e872c0-0b0b-4259-a3b8-233ba36a9d31",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: GeneralFunctions"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = PP.general(silver_m1, silver_m2, silver_m3, silver_h, bronze_hearing_centres, bronze_derive_hearing_centres)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e159e0a2-163f-46f1-a491-59f9a8c75ba1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: GeneralDefault - DefaultValues"
    }
   },
   "outputs": [],
   "source": [
    "df = AERa.generalDefault(silver_m1)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79de0936-f579-432a-990c-dad2f6f3f74a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: Documents"
    }
   },
   "outputs": [],
   "source": [
    "documents_content, documents_audit = PP.documents(silver_m1)\n",
    "# display(documents_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e89be12-a1e2-48c7-a51a-d4be284c03c1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: caseState"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = PP.caseState(silver_m1,\"awaitingRespondentEvidence\")\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5552c83b-7c45-4994-a403-1b94dc87827f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: mainAwaitingRespondentEvidenceA"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, StructType, ArrayType, MapType\n",
    "from pyspark.sql.functions import col, lit, to_json, struct, concat, regexp_replace\n",
    "from datetime import datetime\n",
    "\n",
    "def mainawaitingRespondentEvidence(silver_segmentation, silver_m1, silver_m2, silver_m3, silver_c,silver_h, bronze_remissions, bronze_countryFromAddress, bronze_HORef_cleansing):\n",
    "\n",
    "    AppealState = \"awaitingRespondentEvidence(a)\"\n",
    "\n",
    "    # Aggregate details\n",
    "    AppealType_df, AppealType_df_audit = PP.appealType(silver_m1)\n",
    "    caseData_df, caseData_df_audit = PP.caseData(silver_m1, silver_m2, silver_m3, silver_h, bronze_hearing_centres, bronze_derive_hearing_centres)\n",
    "    flagsLabels_df, flagsLabels_df_audit = PP.flagsLabels(silver_m1, silver_m2, silver_c)\n",
    "    appellantDetails_df, appellantDetails_df_audit = AERa.appellantDetails(silver_m1, silver_m2, silver_c, bronze_countryFromAddress, bronze_HORef_cleansing)\n",
    "    legalRepDetails_df, legalRepDetails_df_audit = PP.legalRepDetails(silver_m1)\n",
    "    partyID_df, partyID_df_audit = PP.partyID(silver_m1, silver_m3, silver_c)\n",
    "    payment_df, payment_df_audit = APS.paymentType(silver_m1, silver_m4)\n",
    "    homeOfficeDetails_df, homeOfficeDetails_df_audit = PP.homeOfficeDetails(silver_m1, silver_m2, silver_c, bronze_HORef_cleansing)\n",
    "    remissionTypes_df, remissionTypes_df_audit = APS.remissionTypes(silver_m1, bronze_remissions, silver_m4)\n",
    "    sponsorDetails_df, sponsorDetails_df_audit = PP.sponsorDetails(silver_m1, silver_c)\n",
    "    general_df, general_df_audit = PP.general(silver_m1, silver_m2, silver_m3, silver_h, bronze_hearing_centres, bronze_derive_hearing_centres)\n",
    "    generalDefault_df = AERa.generalDefault(silver_m1)\n",
    "    documents_df, documents_df_audit = PP.documents(silver_m1)\n",
    "    caseState_df, caseState_df_audit = PP.caseState(silver_m1, \"awaitingRespondentEvidence\")\n",
    "    silver_segmentation_df = silver_segmentation\n",
    "\n",
    "    # Join all aggregated data with Appeal Case Details\n",
    "    df_combined = (\n",
    "        silver_segmentation_df.join(AppealType_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(caseData_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(legalRepDetails_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(appellantDetails_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(flagsLabels_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(partyID_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(homeOfficeDetails_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(remissionTypes_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(sponsorDetails_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(payment_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(general_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(generalDefault_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(documents_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(caseState_df, on=\"CaseNo\", how=\"left\")\n",
    "    \n",
    "    )\n",
    "\n",
    "    # Join all aggregated data with Appeal Case Details\n",
    "    df_combined_audit = (\n",
    "        silver_segmentation_df.join(AppealType_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(caseData_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(legalRepDetails_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(appellantDetails_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(flagsLabels_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(partyID_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(homeOfficeDetails_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(remissionTypes_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(sponsorDetails_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(payment_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(general_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(documents_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(caseState_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    Datetime_name = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "    # Create JSON and filename and omit columns that are with null values\n",
    "    df_final = df_combined.withColumn(\n",
    "        \"JSON_Content\", to_json(struct(*df_combined.drop(col(\"CaseNo\")).columns))\n",
    "    ).withColumn(\n",
    "        \"JSON_File_name\", concat(lit(f\"{gold_outputs}/{Datetime_name}/JSON/APPEALS_\"), regexp_replace(col(\"CaseNo\"), \"/\", \"_\"), lit(\".json\"))\n",
    "    ).distinct()\n",
    "    \n",
    "    return df_final, df_combined_audit\n",
    "\n",
    "# silver_m1 = spark.table(\"ariadm_active_appeals.silver_appealcase_detail\").filter(col(\"dv_targetState\") == lit(AppealState)).distinct()\n",
    "# silver_m2 =  spark.table(\"ariadm_active_appeals.silver_caseapplicant_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "# silver_m3 = spark.table(\"ariadm_active_appeals.silver_status_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "# silver_c = spark.table(\"ariadm_active_appeals.silver_appealcategory_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "# bronze_remission_lookup_df = spark.table(\"ariadm_active_appeals.bronze_remissions\").distinct()\n",
    "# silver_segmentation = spark.table(\"ariadm_active_appeals.stg_segmentation_states\").filter(col(\"TargetState\") == lit(AppealState))\n",
    "\n",
    "# bronze_countryFromAddress = spark.table(\"ariadm_active_appeals.bronze_countries_countryFromAddress\").withColumn(\"lu_countryGovUkOocAdminJ\",col(\"countryGovUkOocAdminJ\"))\n",
    "\n",
    "# bronze_HORef_cleansing = spark.table(\"ariadm_active_appeals.bronze_HORef_cleansing\")\n",
    "\n",
    "# df_final, df_audit = mainawaitingRespondentEvidence(silver_segmentation, silver_m1, silver_m2, silver_m3, silver_c,silver_h, bronze_remission_lookup_df,bronze_countryFromAddress,bronze_HORef_cleansing)\n",
    "\n",
    "# display(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1e06424-6966-4125-9563-3d73261b37c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Function: Upload  and Blob Client Connection Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bd38396-bc28-4971-84ae-805bfd07e775",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Secret Retrieval for Database Connection"
    }
   },
   "outputs": [],
   "source": [
    "secret = dbutils.secrets.get(KeyVault_name, f\"CURATED-{env_name}-SAS-TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5169b7b1-135d-4de4-a228-a25b02b66f97",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Azure Blob Storage Connection Setup in Python"
    }
   },
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "import os\n",
    "\n",
    "# Set up the BlobServiceClient with your connection string\n",
    "connection_string = secret\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Specify the container name\n",
    "container_name = \"gold\"\n",
    "container_client = blob_service_client.get_container_client(container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd8881aa-9b58-41e1-ad05-93d0230af4c5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: Format Dates for UploadBlob Storage"
    }
   },
   "outputs": [],
   "source": [
    "# Upload HTML to Azure Blob Storage\n",
    "def upload_to_blob(file_name, file_content):\n",
    "    try:\n",
    "        # blob_client = container_client.get_blob_client(f\"{gold_outputs}/HTML/{file_name}\")\n",
    "        blob_client = container_client.get_blob_client(f\"{file_name}\")\n",
    "        blob_client.upload_blob(file_content, overwrite=True)\n",
    "        return \"success\"\n",
    "    except Exception as e:\n",
    "        return f\"error: {str(e)}\"\n",
    "\n",
    "# Register the upload function as a UDF\n",
    "upload_udf = udf(upload_to_blob)\n",
    "\n",
    "# df_with_upload_status = df_final.withColumn(\n",
    "#     \"Status\", upload_udf(col(\"JSON_File_name\"), col(\"JSON_Content\"))\n",
    "# )\n",
    "\n",
    "# display(df_with_upload_status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db97feb8-762a-40b6-8cb9-5afa8f134fe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Gold Outputs and Tracking DLT Table Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71422b56-4880-4990-b444-34d3f288782b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Expectation collection and validation checks into string"
    }
   },
   "outputs": [],
   "source": [
    "checks = {}\n",
    "checks = base_DQRules()\n",
    "\n",
    "checks[\"valid_appellantFullName_not_null\"] = \"(appellantFullName IS NOT NULL)\"\n",
    "\n",
    "dq_rules = build_rule_expression(checks)\n",
    "# dq_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3d01276-8aa3-40b8-9143-07edd5e2f8bc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transformation: stg_main_awaitingRespondentEvidence_validation"
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, lit, expr\n",
    "\n",
    "@dlt.table(\n",
    "    name=f\"stg_main_{output_name}_validation\",\n",
    "    comment=\"DLT table running mainPaymentPending to generate a JSON_Content column for CCD validation. Applies DLT expectations on CCD, adding is_valid to flag validation results.\",\n",
    "    path=f\"{audit_path}/stg_main_{output_name}_validation\"\n",
    ")\n",
    "@dlt.expect_all(checks)\n",
    "def stg_main_awaitingRespondentEvidence_validation():\n",
    "    try:\n",
    "        silver_m1 = dlt.read(\"silver_appealcase_detail\").filter(col(\"dv_targetState\") == lit(AppealState)).distinct()\n",
    "        bronze_appealtype_lookup_df = dlt.read(\"bronze_appealtype\").distinct()\n",
    "        bronze_hearing_centres_lookup_df = dlt.read(\"bronze_hearing_centres\").distinct()\n",
    "        # stg_representation = dlt.read(\"stg_representation\").select(col(\"Representation\").alias(\"valid_representation\"))\n",
    "        silver_m2 = dlt.read(\"silver_caseapplicant_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_m3 = dlt.read(\"silver_status_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m4 = dlt.read(\"silver_transaction_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_m5 = dlt.read(\"silver_link_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m6 = dlt.read(\"silver_adjudicator_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_c = dlt.read(\"silver_appealcategory_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_h = dlt.read(\"silver_history_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m8 = dlt.read(\"silver_documentsreceived_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m9 = dlt.read(\"silver_history_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        bronze_countries_postal_lookup_df = dlt.read(\"bronze_countries_postal\").distinct()\n",
    "        bronze_remissions = dlt.read(\"bronze_remissions\").distinct()\n",
    "        bronze_countryFromAddress = dlt.read(\"bronze_countries_countryFromAddress\")\n",
    "        bronze_HORef_cleansing = dlt.read(\"bronze_HORef_cleansing\")\n",
    "        silver_segmentation = dlt.read(\"stg_segmentation_states\").filter(col(\"TargetState\") == lit(AppealState))\n",
    "    except:\n",
    "        silver_m1 = spark.table(\"ariadm_active_appeals.silver_appealcase_detail\").filter(col(\"dv_targetState\") == lit(AppealState)).distinct()\n",
    "        bronze_appealtype_lookup_df = spark.table(\"ariadm_active_appeals.bronze_appealtype\").distinct()\n",
    "        bronze_hearing_centres_lookup_df = spark.table(\"ariadm_active_appeals.bronze_hearing_centres\").distinct()\n",
    "        # stg_representation = spark.table(\"ariadm_active_appeals.stg_representation\").select(col(\"Representation\").alias(\"valid_representation\"))\n",
    "        silver_m2 = spark.table(\"ariadm_active_appeals.silver_caseapplicant_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_m3 = spark.table(\"ariadm_active_appeals.silver_status_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m4 = spark.table(\"ariadm_active_appeals.silver_transaction_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_m5 = spark.table(\"ariadm_active_appeals.silver_link_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m6 = spark.table(\"ariadm_active_appeals.silver_adjudicator_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_c = spark.table(\"ariadm_active_appeals.silver_appealcategory_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_h = spark.table(\"ariadm_active_appeals.silver_history_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m8 = spark.table(\"ariadm_active_appeals.silver_documentsreceived_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m9 = spark.table(\"ariadm_active_appeals.silver_history_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        bronze_countries_postal_lookup_df = spark.table(\"ariadm_active_appeals.bronze_countries_postal\").distinct() \n",
    "        bronze_remissions = spark.table(\"ariadm_active_appeals.bronze_remissions\").distinct()\n",
    "        bronze_countryFromAddress = spark.table(\"ariadm_active_appeals.bronze_countries_countryFromAddress\")\n",
    "        bronze_HORef_cleansing = spark.table(\"ariadm_active_appeals.bronze_HORef_cleansing\")\n",
    "        silver_segmentation = spark.table(\"ariadm_active_appeals.stg_segmentation_states\").filter(col(\"TargetState\") == lit(AppealState))\n",
    "\n",
    " \n",
    "    df_final,df_audit = mainawaitingRespondentEvidence(silver_segmentation, silver_m1,silver_m2, silver_m3,silver_c,silver_h, bronze_remissions,bronze_countryFromAddress,bronze_HORef_cleansing)\n",
    " \n",
    "    valid_representation = silver_m1.select(col(\"CaseNo\"), col(\"dv_representation\"),col(\"dv_CCDAppealType\"),col(\"CaseRep_Address5\"), col(\"CaseRep_Postcode\"),col(\"MainRespondentId\"), col(\"lu_appealType\"), col(\"HORef\"),col(\"Sponsor_Authorisation\"),col(\"Sponsor_Name\")) \n",
    "    valid_appealant_address = silver_m2.select(col(\"CaseNo\"), col(\"Appellant_Address1\"), col(\"Appellant_Address2\"),col(\"Appellant_Address3\"),(\"Appellant_Address4\"), col(\"Appellant_Address5\"), col(\"Appellant_Postcode\"),col(\"Appellant_Email\"),col(\"Appellant_Telephone\"), col(\"FCONumber\")).filter(col(\"Relationship\").isNull())\n",
    "    valid_country_list = bronze_countries_postal_lookup_df.select(col(\"countryGovUkOocAdminJ\").alias(\"valid_countryGovUkOocAdminJ\")).distinct()\n",
    "    valid_catagoryid_list = silver_c.groupBy(\"CaseNo\").agg(F.collect_list(\"CategoryId\").alias(\"valid_categoryIdList\"))\n",
    "    valid_HORef_cleansing = bronze_HORef_cleansing.select( col(\"CaseNo\"),coalesce(col(\"HORef\"), col(\"FCONumber\")).alias(\"lu_HORef\"))\n",
    "    valid_reasonDescription = silver_m1.alias(\"m1\").join(bronze_remissions, on=[\"PaymentRemissionReason\",\"PaymentRemissionRequested\"], how=\"left\").select(\"CaseNo\", \"ReasonDescription\",col(\"remissionClaim\").alias(\"lu_remissionClaim\"),col(\"feeRemissionType\").alias(\"lu_feeRemissionType\"))\n",
    " \n",
    "    df_final = df_final.join(valid_representation, on=\"CaseNo\", how=\"left\"\n",
    "                            ).join(valid_country_list, on=col(\"CaseRep_Address5\") == col(\"valid_countryGovUkOocAdminJ\"), how=\"left\"\n",
    "                            ).join(valid_catagoryid_list, on=\"CaseNo\", how=\"left\"\n",
    "                            ).join(valid_appealant_address, on=\"CaseNo\", how=\"left\"\n",
    "                            ).join(valid_HORef_cleansing, on=\"CaseNo\", how=\"left\"\n",
    "                            ).join(valid_reasonDescription, on=\"CaseNo\", how=\"left\")\n",
    "\n",
    " \n",
    "    df_final = df_final.withColumn(\"is_valid\", expr(dq_rules))\n",
    "\n",
    "    # df_final = df_final.drop(col(\"dv_representation\"), col(\"CaseRepAddress5\"), col(\"CaseRepPostcode\"), col(\"valid_countryGovUkOocAdminJ\"))\n",
    "\n",
    "    # columns_to_drop = [\"dv_representation\", \"CaseRepAddress5\", \"CaseRepPostcode\"]\n",
    "\n",
    "    # if all(col in df_final.columns for col in columns_to_drop): #If the columns exist - remove \n",
    "    #     df_final = df_final.drop(*columns_to_drop)\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3362b255-72e6-4a4f-8b12-170c233128ad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transformation: stg_valid_payment_pending_records"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=f\"stg_valid_{output_name}_records\",\n",
    "    comment=\"Delta Live Gold Table with JSON content.\",\n",
    "    path=f\"{audit_path}/stg_valid_{output_name}_records\"\n",
    ")\n",
    "def stg_valid_awaitingRespondentEvidence_records():\n",
    "    \"\"\"\n",
    "    Delta Live Table for creating and uploading JSON content for Appeals.\n",
    "    \"\"\"\n",
    "    # Load source data\n",
    "    df = dlt.read(f\"stg_main_{output_name}_validation\")\n",
    "\n",
    "    df_filtered = df.filter(\n",
    "        (col(\"is_valid\") == True)\n",
    "    )\n",
    "\n",
    "    # Repartition to optimize parallelism\n",
    "    repartitioned_df = df_filtered.repartition(64)\n",
    "\n",
    "    df_with_upload_status = repartitioned_df.filter(~col(\"JSON_content\").like(\"Error%\")).withColumn(\n",
    "            \"Status\", upload_udf(col(\"JSON_File_Name\"), col(\"JSON_content\"))\n",
    "        )\n",
    "\n",
    "    # Return the DataFrame for DLT table creation\n",
    "    return df_with_upload_status.select(\"CaseNo\", \"JSON_content\",col(\"JSON_File_Name\").alias(\"File_Name\"),\"Status\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60bfdf72-3653-4133-ae58-42f66d648a28",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transformation: stg_invalid_awaitingRespondentEvidenceA_quarantine_records"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=f\"stg_invalid_{output_name}_quarantine_records\",\n",
    "    comment=\"Quarantined records that failed data quality checks or JSON generation.\",\n",
    "    path=f\"{audit_path}/stg_invalid_{output_name}_quarantine_records\"\n",
    ")\n",
    "def stg_invalid_awaitingRespondentEvidence_quarantine_records():\n",
    "\n",
    "    df = dlt.read(f\"stg_main_{output_name}_validation\")\n",
    "\n",
    "    df_filtered = df.filter(\n",
    "        (col(\"is_valid\") != True)\n",
    "    ).withColumn(\"JSON_File_Name\", regexp_replace(col(\"JSON_File_Name\"), \"/JSON/\", \"/INVALID_JSON/\"))\n",
    "\n",
    "    # Repartition to optimize parallelism\n",
    "    repartitioned_df = df_filtered.repartition(64)\n",
    "\n",
    "    df_with_upload_status = repartitioned_df.filter(~col(\"JSON_content\").like(\"Error%\")).withColumn(\n",
    "            \"Status\", upload_udf(col(\"JSON_File_Name\"), col(\"JSON_content\"))\n",
    "        )\n",
    "\n",
    "    return df_with_upload_status.select(\"CaseNo\", \"JSON_content\",col(\"JSON_File_Name\").alias(\"File_Name\"),\"Status\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a81bf947-c500-4014-b228-71862de0a3fc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transformation: apl_active_awaitingRespondentEvidence_cr_audit_table"
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, lit, expr\n",
    "\n",
    "@dlt.table(\n",
    "    name=f\"apl_active_{output_name}_cr_audit_table\",\n",
    "    comment=\"DLT table Covers 4.2 Silver layer LLD requirements: Audits CCD attributes, input field values, derived values, and all columns for validation and traceability.\",\n",
    "    path=f\"{audit_path}/apl_active_{output_name}_cr_audit_table\"\n",
    ")\n",
    "def apl_active_awaitingRespondentEvidenceA_cr_audit_table():\n",
    "    try:\n",
    "        silver_m1 = dlt.read(\"silver_appealcase_detail\").filter(col(\"dv_targetState\") == lit(AppealState)).distinct()\n",
    "        silver_m2 = dlt.read(\"silver_caseapplicant_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        bronze_appealtype_lookup_df = dlt.read(\"bronze_appealtype\").distinct()\n",
    "        bronze_hearing_centres_lookup_df = dlt.read(\"bronze_hearing_centres\").distinct()\n",
    "        silver_m3 = dlt.read(\"silver_status_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_c = dlt.read(\"ariadm_active_appeals.silver_appealcategory_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_h = dlt.read(\"silver_history_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        bronze_remission_lookup_df = dlt.read(\"bronze_remissions\").distinct()\n",
    "        bronze_remissions_lookup_df = dlt.read(\"bronze_remissions\").distinct()\n",
    "        bronze_countryFromAddress = dlt.read(\"bronze_countries_countryFromAddress\")\n",
    "        bronze_HORef_cleansing = dlt.read(\"bronze_HORef_cleansing\")\n",
    "        silver_segmentation = dlt.read(\"stg_segmentation_states\").filter(col(\"TargetState\") == lit(AppealState))\n",
    "      \n",
    "    except:\n",
    "        silver_m1 = spark.table(\"ariadm_active_appeals.silver_appealcase_detail\").filter(col(\"dv_targetState\") == lit(AppealState)).distinct()\n",
    "        silver_m2 = spark.table(\"ariadm_active_appeals.silver_caseapplicant_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        bronze_appealtype_lookup_df = spark.table(\"ariadm_active_appeals.bronze_appealtype\").distinct()\n",
    "        bronze_hearing_centres_lookup_df = spark.table(\"ariadm_active_appeals.bronze_hearing_centres\").distinct()\n",
    "        silver_m3 = spark.table(\"ariadm_active_appeals.silver_status_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_c = spark.table(\"ariadm_active_appeals.silver_appealcategory_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_h = spark.table(\"ariadm_active_appeals.silver_history_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        bronze_remissions_lookup_df = spark.table(\"ariadm_active_appeals.bronze_remissions\").distinct()\n",
    "        bronze_countryFromAddress = spark.table(\"ariadm_active_appeals.bronze_countries_countryFromAddress\")\n",
    "        bronze_HORef_cleansing = spark.table(\"ariadm_active_appeals.bronze_HORef_cleansing\")\n",
    "        silver_segmentation = spark.table(\"ariadm_active_appeals.stg_segmentation_states\").filter(col(\"TargetState\") == lit(AppealState))\n",
    " \n",
    "    df_final,df_audit = mainawaitingRespondentEvidence(silver_segmentation, silver_m1,silver_m2, silver_m3,silver_c,silver_h, bronze_remissions_lookup_df,bronze_countryFromAddress,bronze_HORef_cleansing)\n",
    "\n",
    "    return df_audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c065211-f4d5-47d6-9f7f-757fedeaa149",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Exit Notebook"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Notebook completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6755913017508968,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "GOLD_AWAITING_EVIDENCE_RESPONDENT_A_JSON",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
