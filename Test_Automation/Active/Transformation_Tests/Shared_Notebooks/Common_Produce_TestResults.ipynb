{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e3a33e0-5b4e-4fcb-8702-228f16839a8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #Setup Notebook Parameters (Defaulting to Payment Pending and Running all tests)\n",
    "\n",
    "#Imports\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "%pip install openpyxl\n",
    "\n",
    "dbutils.widgets.text(\"all_test_results\", \"\")\n",
    "all_test_results_string = dbutils.widgets.get(\"all_test_results_string\")\n",
    "\n",
    "dbutils.widgets.text(\"state_under_test\", \"\")\n",
    "state_under_test = dbutils.widgets.get(\"state_under_test\")\n",
    "\n",
    "dbutils.widgets.text(\"base_path\", \"\")\n",
    "base_path = dbutils.widgets.get(\"base_path\")\n",
    "\n",
    "dbutils.widgets.text(\"test_results_path\", \"\")\n",
    "test_results_path = dbutils.widgets.get(\"test_results_path\")\n",
    "#new\n",
    "dbutils.widgets.text(\"run_user\", \"\")\n",
    "run_user = dbutils.widgets.get(\"run_user\")\n",
    "\n",
    "dbutils.widgets.text(\"run_tag\", \"\")\n",
    "run_tag = dbutils.widgets.get(\"run_tag\")\n",
    "\n",
    "dbutils.widgets.text(\"run_by_automation_name\", \"\")\n",
    "run_by_automation_name = dbutils.widgets.get(\"run_by_automation_name\")\n",
    "\n",
    "dbutils.widgets.text(\"run_start_datetime\", \"\")\n",
    "run_start_datetime_string = dbutils.widgets.get(\"run_start_datetime\")\n",
    "run_start_datetime = datetime.strptime(run_start_datetime_string, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "# Add base_path to sys.path to find models\n",
    "if base_path not in sys.path:\n",
    "    sys.path.append(base_path)\n",
    "# from models.test_result import TestResult\n",
    "\n",
    "#Add as short term solution to above that stopped working\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class TestResult:\n",
    "    test_field: str =\"\"     \n",
    "    status: str =\"\"\n",
    "    message: str = \"\"\n",
    "    test_from_state: str=\"\"\n",
    "    test_name: str=\"\"\n",
    "\n",
    "all_test_results = [TestResult(**d) for d in json.loads(all_test_results_string)]\n",
    "\n",
    "\n",
    "###############################\n",
    "#Update Central Results Table\n",
    "###############################\n",
    "import uuid\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#Define Schema\n",
    "def run_and_result_table_schemas():\n",
    "\n",
    "    runs_schema = StructType([\n",
    "        StructField(\"run_id\", StringType(), True),\n",
    "        StructField(\"run_user\", StringType(), True),\n",
    "        StructField(\"run_start_datetime\", TimestampType(), True),\n",
    "        StructField(\"run_end_datetime\", TimestampType(), True),    \n",
    "        StructField(\"run_by_automation_name\", StringType(), True),\n",
    "        StructField(\"run_tag\", StringType(), True),\n",
    "        StructField(\"run_status\", StringType(), True),\n",
    "        StructField(\"state_under_test\", StringType(), True), \n",
    "        StructField(\"total_passed\", IntegerType(), True), \n",
    "        StructField(\"total_failed\", IntegerType(), True), \n",
    "        StructField(\"total_tests\", IntegerType(), True), \n",
    "    ])\n",
    "\n",
    "    results_schema = StructType([\n",
    "        StructField(\"result_id\", StringType(), True),\n",
    "        StructField(\"run_id\", StringType(), True),  # FK to test_runs\n",
    "        StructField(\"test_name\", StringType(), True),\n",
    "        StructField(\"test_field\", StringType(), True),\n",
    "        StructField(\"test_from_state\", StringType(), True),\n",
    "        StructField(\"status\", StringType(), True),\n",
    "        StructField(\"message\", StringType(), True)\n",
    "    ])\n",
    "    return runs_schema, results_schema\n",
    "\n",
    "\n",
    "###############################\n",
    "#Create Run First (so can get ID to register results against)\n",
    "###############################\n",
    "def create_run(run_user, run_start_datetime, run_end_datetime, run_by_automation_name, run_tag, run_status,state_under_test,pass_count, fail_count, total_count):        \n",
    "    try:\n",
    "        run_id = str(uuid.uuid4())\n",
    "        runs_schema, results_schema = run_and_result_table_schemas()\n",
    "        df = spark.createDataFrame(\n",
    "            [(run_id, run_user, run_start_datetime, run_end_datetime, run_by_automation_name, run_tag, run_status,state_under_test,pass_count, fail_count, total_count)],runs_schema)\n",
    "        df.show()\n",
    "        print(\"After Create dataframe)\")\n",
    "        df.write.option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(\"test_automation_runs\")    \n",
    "        print(\"After Write dataframe)\")\n",
    "        return run_id\n",
    "    except Exception as e:\n",
    "        error_message = str(e)  \n",
    "        print(f\"Failed to Create New Run Record. error : {message}\")  \n",
    "        return None\n",
    "          \n",
    "###############################\n",
    "#Create Each Test Result\n",
    "###############################\n",
    "def create_result(run_id):\n",
    "    try:\n",
    "        runs_schema, results_schema = run_and_result_table_schemas()\n",
    "        rows = []        \n",
    "        for result in all_test_results:                               \n",
    "                rows.append(\n",
    "                    (\n",
    "                    str(uuid.uuid4()),\n",
    "                    run_id,\n",
    "                    str(getattr(result, \"test_name\", \"\") or \"\"),\n",
    "                    str(getattr(result, \"test_field\", \"\") or \"\"),\n",
    "                    str(getattr(result, \"test_from_state\", \"\") or \"\"),\n",
    "                    str(getattr(result, \"status\", \"\") or \"\"),\n",
    "                    str(getattr(result, \"message\", \"\") or \"\")\n",
    "                    )\n",
    "                )        \n",
    "        df = spark.createDataFrame(rows, results_schema)\n",
    "        df.write.option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(\"test_automation_results\")    \n",
    "    except Exception as e:\n",
    "        error_message = str(e)   \n",
    "        print(f\"failed to Update Record : {result.test_name} - error : {message}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfc910ea-3cac-4f71-bfa6-0937a1881788",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "able"
    }
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "#Produce File Test Reports\n",
    "#################################\n",
    "\n",
    "# Count passed and failed\n",
    "pass_count = sum(1 for r in all_test_results if r.status.upper() == \"PASS\")\n",
    "fail_count = sum(1 for r in all_test_results if r.status.upper() == \"FAIL\")\n",
    "run_status = \"PASS\" if fail_count == 0 and pass_count >= 1 else \"FAIL\"\n",
    "print(f\"OVERALL TEST RESULTS - Status :  {run_status} - FOR STATE : {state_under_test} -- Pass: {pass_count}, Fail: {fail_count}\")\n",
    "\n",
    "#Display / Output Test results\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "#convert list of dicts string into df\n",
    "df_results = pd.DataFrame(eval(all_test_results_string))\n",
    "display(df_results)  # Databricks display\n",
    "\n",
    "#Export Results\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "#Create Results Folder\n",
    "test_results_path = f\"{test_results_path}/{timestamp}-{state_under_test}/\"\n",
    "os.makedirs(test_results_path, exist_ok=True)\n",
    "\n",
    "#TODO - Choose output format\n",
    "\n",
    "# Export to CSV\n",
    "file_path = f\"{test_results_path}/test_results_{state_under_test}_{timestamp}.csv\"\n",
    "df_results.to_csv(file_path, index=False)\n",
    "print(f\"Test results saved to {file_path}\")\n",
    "\n",
    "# Export to XLS\n",
    "file_path = f\"{test_results_path}/test_results_{state_under_test}_{timestamp}.xlsx\"\n",
    "df_results.to_excel(file_path, engine=\"openpyxl\", index=False )\n",
    "print(f\"Test results saved to {file_path}\")\n",
    "\n",
    "#Push Results into Spark Table\n",
    "# from table_utils import create_run, create_result\n",
    "print(\"Starting Pushing Run/Results into Tables\")\n",
    "run_end_datetime = datetime.now()\n",
    "run_id = create_run(run_user, run_start_datetime, run_end_datetime, run_by_automation_name, run_tag, run_status,state_under_test,pass_count, fail_count, pass_count + fail_count)\n",
    "print(f\"Finsihed creating Run -- Run_id = {str(run_id)}\")\n",
    "if run_id != None:\n",
    "    create_result(run_id)\n",
    "    print(f\"Finsihed creating Results\")\n",
    "else:\n",
    "    print(\"Failed to Create a Run, No results have been submitted to spark tables\")\n",
    "    \n",
    "\n",
    "# Export to HTML\n",
    "file_path = f\"{test_results_path}/test_results_{state_under_test}_{timestamp}.html\"\n",
    "def color_status(val):\n",
    "    color = 'green' if val == 'PASS' else 'red'\n",
    "    return f'color: {color}; font-weight: bold'\n",
    "\n",
    "# Convert DataFrame to styled HTML\n",
    "styled_html = df_results.style.applymap(color_status, subset=['status']) \\\n",
    "                    .set_table_styles([\n",
    "                        {'selector': 'table', 'props': [('border-collapse', 'collapse'), \n",
    "                                                        ('width', '80%')]},\n",
    "                        {'selector': 'th, td', 'props': [('border', '1px solid black'),\n",
    "                                                        ('padding', '8px'),\n",
    "                                                        ('text-align', 'left')]}\n",
    "                    ]) \\\n",
    "                    .render()\n",
    "\n",
    "# Wrap in basic HTML tags\n",
    "html_content = f\"\"\"\n",
    "<html>\n",
    "<head>\n",
    "    <title>Test Results - {state_under_test} - {timestamp}</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h2>Test Results - {state_under_test}- {timestamp}</h2>\n",
    "    <p><strong>Total PASS:</strong> {str(pass_count)} &nbsp;&nbsp; <strong>Total FAIL:</strong> {str(fail_count)}</p>\n",
    "    {styled_html}\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "with open(file_path, \"w\") as f:\n",
    "    f.write(html_content)\n",
    "print(f\"HTML report saved to {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Common_Produce_TestResults",
   "widgets": {
    "all_test_results": {
     "currentValue": "",
     "nuid": "e744db53-b487-47b1-8e2b-fc560b5b2671",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "all_test_results",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "all_test_results",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "param_1": {
     "currentValue": "all_test_results",
     "nuid": "62f6bcdd-de1d-4bc4-b804-a715e59ce739",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "param_1",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "param_1",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "param_2": {
     "currentValue": "state_under_test",
     "nuid": "52a0039c-2cc9-4b99-9f3d-c4f916e19eb1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "param_2",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "param_2",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "param_3": {
     "currentValue": "base_path",
     "nuid": "80c2004d-3b35-4716-8ecc-02d3ea8867f8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "param_3",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "param_3",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
