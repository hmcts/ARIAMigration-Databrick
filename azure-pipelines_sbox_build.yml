trigger:
  branches:
    include:
    - main

pool:
  vmImage: 'ubuntu-latest' 

variables:
  - name: env
    value: 'sandbox'
  - name: DATABRICKS_HOST_WORKSPACE_MAIN00
    value: 'https://adb-3635282203417052.12.azuredatabricks.net'
  - name: DATABRICKS_HOST_WORKSPACE_MAIN01
    value: 'https://adb-376876256300083.3.azuredatabricks.net/'
  - name: DATABRICKS_HOST_WORKSPACE_MAIN02
    value: 'https://adb-1879076228317698.18.azuredatabricks.net'

stages:
- stage: 'Build'
  displayName: 'Build Stage'
  jobs:
  - job: BuildJob
    displayName: 'Run Build and deploy'
    steps:
    - task: UsePythonVersion@0
      displayName: 'Use Python 3.x'
      inputs:
        versionSpec: '3.x'

    - script: |
        python --version
        pip install --upgrade pip
        pip install build
        pip install databricks-cli
        python -m build --wheel --outdir $(Build.SourcesDirectory)/Databricks/SharedFunctionsLib/dist \
          $(Build.SourcesDirectory)/Databricks/SharedFunctionsLib

        ls -la $(Build.SourcesDirectory)/Databricks/SharedFunctionsLib/dist
      displayName: 'Install Python dependencies & build Python wheel!'
    
    - script: |
       pip install --upgrade pip
       pip install -r AzureFunctions/requirements.txt --target="./AzureFunctions/.python_packages/lib/site-packages"
      displayName: 'Install AzureFunctions dependencies into package'

    # Configure and deploy to Workspace MAIN00
    - script: |
        echo "[DEFAULT]" > $(HOME)/.databrickscfg
        echo "host = $(DATABRICKS_HOST_WORKSPACE_MAIN00)" >> $(HOME)/.databrickscfg
        echo "token = $(DATABRICKS_TOKEN_WORKSPACE_MAIN00)" >> $(HOME)/.databrickscfg
      displayName: 'Configure Databricks CLI for Workspace MAIN00'

    - script: |
        databricks workspace delete /live --recursive 
        databricks workspace mkdirs /live
        databricks workspace import_dir Databricks /live --overwrite
      displayName: 'Publish Databricks folder to /live in Databricks MAIN00 Workspace'

    # Configure and deploy to Workspace MAIN01
    - script: |
        echo "[DEFAULT]" > $(HOME)/.databrickscfg
        echo "host = $(DATABRICKS_HOST_WORKSPACE_MAIN01)" >> $(HOME)/.databrickscfg
        echo "token = $(DATABRICKS_TOKEN_WORKSPACE_MAIN01)" >> $(HOME)/.databrickscfg
      displayName: 'Configure Databricks CLI for Workspace MAIN01'

    - script: |
        databricks workspace delete /live --recursive
        databricks workspace mkdirs /live
        databricks workspace import_dir Databricks /live --overwrite
      displayName: 'Publish Databricks folder to /live in Databricks MAIN01 Workspace'

    # Configure and deploy to Workspace MAIN02
    - script: |
        echo "[DEFAULT]" > $(HOME)/.databrickscfg
        echo "host = $(DATABRICKS_HOST_WORKSPACE_MAIN02)" >> $(HOME)/.databrickscfg
        echo "token = $(DATABRICKS_TOKEN_WORKSPACE_MAIN02)" >> $(HOME)/.databrickscfg
      displayName: 'Configure Databricks CLI for Workspace MAIN02'

    - script: |
        databricks workspace delete /live --recursive
        databricks workspace mkdirs /live
        databricks workspace import_dir Databricks /live --overwrite
      displayName: 'Publish Databricks folder to /live in Databricks MAIN02 Workspace'
    
    - task: PublishBuildArtifacts@1
      inputs:
        PathtoPublish: '$(Build.SourcesDirectory)/Databricks/SharedFunctionsLib/dist'
        ArtifactName: 'ariafunction'
        publishLocation: 'Container'
      displayName: 'Publish Wheel Artifact'
    
    - task: PublishBuildArtifacts@1
      inputs:
        PathtoPublish: '$(Build.SourcesDirectory)/AzureFunctions'
        ArtifactName: 'functionapp'
        publishLocation: 'Container'
      displayName: 'Publish Azure Function'
    
- stage: 'Deploy'
  displayName: 'Deploy Stage'
  jobs:
  - job: DeployDatabricksResources
    displayName: 'Run Build and deploy'
    steps:

    - task: DownloadBuildArtifacts@1
      inputs:
        buildType: 'current'
        downloadType: 'single'
        artifactName: 'ariafunction'
        downloadPath: '$(Pipeline.Workspace)'
      displayName: 'Download wheel Artifact'
    
    - task: DownloadBuildArtifacts@1
      inputs:
        buildType: 'current'
        downloadType: 'single'
        artifactName: 'functionapp'
        downloadPath: '$(Pipeline.Workspace)'
      displayName: 'Download Azure Function Artifact'
    
    - task: AzureFunctionApp@2
      inputs: 
        connectedServiceNameARM: 'DTS-DATAINGEST-SBOX'
        appType: 'functionAppLinux'
        appName: 'af-joh-dev-uks-dlrm-02'
        package: '$(Pipeline.Workspace)/functionapp'
        deploymentMethod: 'auto'
      displayName: 'Deploy Azure Function to af-joh-dev-uks-dlrm-02'
    
    - task: AzureCLI@2
      inputs:
      azureSubscription: 'DTS-DATAINGEST-SBOX' 
      scriptType: 'bash'
      scriptLocation: 'inlineScript'
      inlineScript: |
        AAD_TOKEN=$(az account get-access-token --resource https://databricks.azure.net --query accessToken -o tsv)
        echo "AAD Token: $AAD_TOKEN"
      displayName: 'Authenticate Azure using Service Principal to Retrieve AAD Token'

    - task: UsePythonVersion@0
      displayName: 'Use Python 3.x'
      inputs:
        versionSpec: '3.x'

    - script: |
          /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
          eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"
          
          brew install databricks/tap/databricks
          databricks --version
      displayName: 'Install Databricks CLI'

      #Ensure databrcks CLI is available in current session (loading homebrew shell)
    - script: |
        eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"  
        databricks --version  
        echo "[DEFAULT]" > $(HOME)/.databrickscfg
        echo "host = $(DATABRICKS_HOST_WORKSPACE_MAIN02)" >> $(HOME)/.databrickscfg
        echo "token = $(DATABRICKS_TOKEN_WORKSPACE_MAIN02)" >> $(HOME)/.databrickscfg
        cat ~/.databrickscfg  # Print the contents for debugging
      displayName: 'Configure Databricks CLI for Workspace MAIN02'

    - script: |
        eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"  

        set -e 

        WHEEL_FILE=$(find '$(Pipeline.Workspace)' -name "*.whl" | tail -n 1)

        if [ -n "$WHEEL_FILE" ]; then
          echo "Found wheel file: $WHEEL_FILE"

          FILENAME=$(basename "$WHEEL_FILE")
          echo "Extracted filename: $FILENAME"

          DEST_PATH="dbfs:/FileStore/shared_wheels/$FILENAME"
          echo "Destination path: $DEST_PATH"

          echo "creating directory dbfs:/FileStore/shared_wheels"
          databricks fs mkdirs dbfs:/FileStore/shared_wheels    

          echo "copying wheel file to DBFS..."
          echo "copying $WHEEL_FILE to $DEST_PATH"
          databricks fs cp "$WHEEL_FILE" "$DEST_PATH" --overwrite --debug

          echo "Displaying content of dbfs:/FileStore/shared_wheels"
          databricks fs ls dbfs:/FileStore/shared_wheels

        else
          echo "No wheel file found"
          exit 1
        fi
      displayName: 'Upload Wheel to Databricks DBFS'

    - script: |
        eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)" 

        SCOPE_NAME="ingest02-meta002-sbox"
        KEYVAULT_NAME="ingest02-meta002-sbox"
        RESOURCE_GROUP="ingest02-main-sbox"
        RESOURCE_ID="/subscriptions/df72bb30-d6fb-47bd-82ee-5eb87473ddb3/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.KeyVault/vaults/$KEYVAULT_NAME"
        DNS_NAME="https://$KEYVAULT_NAME.vault.azure.net/"

         echo "Test using AAD_TOKEN: $AAD_TOKEN"

        if databricks secrets list-scopes | grep -q "\"scope\": \"$SCOPE_NAME\""; then
          echo "Secret scope '$SCOPE_NAME' already exists. Skipping creation."
        else
          echo "Creating Azure Key Vault backed secret scope '$SCOPE_NAME'..."
          databricks secrets create-scope --json "{
            \"scope\": \"$SCOPE_NAME\",
            \"scope_backend_type\": \"AZURE_KEYVAULT\",
            \"initial_manage_principal\": \"users\",
            \"backend_azure_keyvault\": {
              \"resource_id\": \"$RESOURCE_ID\",
              \"dns_name\": \"$DNS_NAME\",
              \"userAADToken\": \"$AAD_TOKEN\"
            }
        }"
        fi

        echo "{
        \"scope\": \"$SCOPE_NAME\",
        \"scope_backend_type\": \"AZURE_KEYVAULT\",
        \"initial_manage_principal\": \"users\",
        \"backend_azure_keyvault\": {
          \"resource_id\": \"$RESOURCE_ID\",
          \"dns_name\": \"$DNS_NAME\"
          }
        }"
      displayName: 'Creating Azure Key Vault backed secret scope'
        
    - script: |
        eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)" 
        KEYVAULT_NAME="ingest02-meta002-sbox"

        echo "Query databricks secrets inside KeyVault backed scope (ingest00-meta002-sbox)"
        databricks secrets list-secrets $KEYVAULT_NAME
        databricks secrets list-secrets ingest00-meta002-sbox
      displayName: 'View KeyVault backed secret keys in Databricks'
    
    - script: |
        echo "Current dir: $(pwd)"
        ls -al /home/vsts/work/1/s
        eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)" 

        cd ARIA_DABs

        echo "Current dir: $(pwd)"
        ls -al  

        databricks bundle destroy -t sandbox 
        databricks bundle validate -t sandbox --debug
        databricks bundle deploy -t sandbox --debug

        databricks bundle run OAuth_job_v1
      displayName: "Deploy Databricks resources to ingest02 workspace"
