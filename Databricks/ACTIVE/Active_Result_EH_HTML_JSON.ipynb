{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c49e7bb7-2c7b-4c3c-ad32-8517eb0e4ada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "import json\n",
    "from  itertools import islice\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import col, decode, split, element_at, udf, lit, reduce, from_json, to_timestamp\n",
    "import logging\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark import SparkContext\n",
    "import os\n",
    "from functools import reduce\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0935e987-2a28-4579-ae26-db913d5e668b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Assign configs\n",
    "config = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/configs/config.json\")\n",
    "env = config.first()[\"env\"].strip().lower()\n",
    "lz_key = config.first()[\"lz_key\"].strip().lower()\n",
    "\n",
    "keyvault_name = f\"ingest{lz_key}-meta002-{env}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1e8a640-61a6-40b3-b678-fb80b412e13a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Access the Service Principle secrets from keyvaults\n",
    "client_secret = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-SECRET')\n",
    "tenant_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-TENANT-ID')\n",
    "client_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e576cd5-907b-45a0-a862-688c02899b67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Paramaterise containers\n",
    "curated_storage_account = f\"ingest{lz_key}curated{env}\"\n",
    "curated_container = \"gold\"\n",
    "silver_curated_container = \"silver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eb0229b-691c-4656-b8e2-70e6fa0697ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "curated_storage_account = f\"ingest{lz_key}curated{env}\"\n",
    "checkpoint_storage_account = f\"ingest{lz_key}xcutting{env}\"\n",
    "\n",
    "##Assign OAuth to curated storage account\n",
    "storage_accounts = [curated_storage_account, checkpoint_storage_account]\n",
    "\n",
    "for storage_account in storage_accounts:\n",
    "    configs = {\n",
    "            f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\": \"OAuth\",\n",
    "            f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\":\n",
    "                \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "            f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\": client_id,\n",
    "            f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\": client_secret,\n",
    "            f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\":\n",
    "                f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "        }\n",
    "    for key,val in configs.items():\n",
    "        spark.conf.set(key,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb1b57dc-27a6-41cc-b3be-bec7f5f3cde7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print out the auth config for each storage account to confirm\n",
    "for storage_account in storage_accounts:\n",
    "    key = f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\"\n",
    "    print(f\"{key}: {spark.conf.get(key, 'MISSING')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bf43b26-2a7f-4381-a50c-d5c54a673ad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve the state parameter from the Databricks Workflow\n",
    "dbutils.widgets.text(\"state\", \"paymentPending\", \"State to Process\")\n",
    "state = dbutils.widgets.get(\"state\")\n",
    "print(f\"ðŸ”„ Processing state: {state}\")\n",
    "\n",
    "EH_NAMESPACE = f\"ingest{lz_key}-integration-eventHubNamespace001-{env}\"\n",
    "EH_NAME = f\"evh-active-pub-{env}-{lz_key}-uks-dlrm-01\" #To create this Eventhub in the UI\n",
    "\n",
    "connection_string = dbutils.secrets.get(keyvault_name, \"RootManageSharedAccessKey\")\n",
    "\n",
    "KAFKA_OPTIONS = {\n",
    "    \"kafka.bootstrap.servers\": f\"{EH_NAMESPACE}.servicebus.windows.net:9093\",\n",
    "    \"subscribe\": EH_NAME,\n",
    "    \"consumer.group.id\": state,\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"failOnDataLoss\": \"false\",\n",
    "    \"startingOffsets\": \"latest\",\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.sasl.jaas.config\": f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{connection_string}\";'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6795adc-c3d9-4f10-b9bc-6b98e42b045d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paths specific to this state\n",
    "data_path = f\"abfss://silver@ingest{lz_key}curated{env}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/{current_state}/publish_audit_db_eh/\"\n",
    "checkpoint_path = f\"abfss://db-ack-checkpoint@ingest{lz_key}xcutting{env}.dfs.core.windows.net/{current_state}/ACK/\"\n",
    "\n",
    "print(f\"ðŸ“‚ Data path: {data_path}\")\n",
    "print(f\"ðŸ“‚ Checkpoint path: {checkpoint_path}\")\n",
    "\n",
    "# Keep schema exactly as it exists in Pub notebook\n",
    "schema = StructType([\n",
    "    StructField(\"RunID\", StringType(), True),\n",
    "    StructField(\"CaseNo\", StringType(), True),\n",
    "    StructField(\"Filename\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"PublishingDateTime\", StringType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"Error\", StringType(), True)\n",
    "])\n",
    "\n",
    "#Read data stored in the EH\n",
    "eventhubdf = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .options(**KAFKA_OPTIONS)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "#Parse the Kafka message for values (e.g., schema above)\n",
    "parsed_df = (\n",
    "    eventhubdf\n",
    "    .select(col(\"value\").cast(\"string\").alias(\"json_str\"))\n",
    "    .select(from_json(col(\"json_str\"), schema).alias(\"json_obj\"))\n",
    "    .select(\"json_obj.*\")\n",
    ")\n",
    "\n",
    "#Stream data into publish_audit_db_eh. Checkpoing into xcutting\n",
    "query = (\n",
    "    parsed_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .outputMode(\"append\")\n",
    "    .start(data_path)\n",
    ")\n",
    "\n",
    "#Wait 15seconds if no data has been received then complete\n",
    "query.awaitTermination(timeout=15)\n",
    "query.stop()\n",
    "\n",
    "#Read df \n",
    "df = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(data_path)\n",
    "    .filter(col(\"Status\").isNotNull())\n",
    ")\n",
    "\n",
    "display(df)\n",
    "#Return total number of rows per state\n",
    "display(df.groupBy(\"State\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fba554c0-2d7f-438b-8c68-c3acd95e9f14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(f\"{current_state} notebook completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Active_Result_EH_HTML_JSON",
   "widgets": {
    "current_state": {
     "currentValue": "paymentPending",
     "nuid": "d2962a90-100c-40ca-a2c9-1c40f002650f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "paymentPending",
      "label": "State to Process",
      "name": "current_state",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "paymentPending",
      "label": "State to Process",
      "name": "current_state",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
