{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65cb672e-fad4-41c9-955b-9e447b8ff7f6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Python Wheel"
    }
   },
   "outputs": [],
   "source": [
    "!pip install /dbfs/FileStore/packages/shared_functions-0.5.2-py3-none-any.whl\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shared_functions.paymentPending as PP\n",
    "from shared_functions.DQRules import base_DQRules, build_rule_expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7831545c-1e19-4244-ab14-25869bbd78f7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import packages"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import *\n",
    "import uk_postcodes_parsing\n",
    "from pyspark.sql import functions as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e251edd4-ffe0-47d2-9c90-2add21a1af59",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Assign configs"
    }
   },
   "outputs": [],
   "source": [
    "config = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/configs/config.json\")\n",
    "env_name = config.first()[\"env\"].strip().lower()\n",
    "lz_key = config.first()[\"lz_key\"].strip().lower()\n",
    "\n",
    "print(f\"env_code: {lz_key}\")  # This won't be redacted\n",
    "print(f\"env_name: {env_name}\")  # This won't be redacted\n",
    "\n",
    "KeyVault_name = f\"ingest{lz_key}-meta002-{env_name}\"\n",
    "print(f\"KeyVault_name: {KeyVault_name}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36f0c0bd-2026-4b2c-8800-f4732e142615",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Assign OAuth"
    }
   },
   "outputs": [],
   "source": [
    "# Service principal credentials\n",
    "client_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-ID\")\n",
    "client_secret = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-SECRET\")\n",
    "tenant_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-TENANT-ID\")\n",
    "\n",
    "# Storage account names\n",
    "curated_storage = f\"ingest{lz_key}curated{env_name}\"\n",
    "checkpoint_storage = f\"ingest{lz_key}xcutting{env_name}\"\n",
    "raw_storage = f\"ingest{lz_key}raw{env_name}\"\n",
    "landing_storage = f\"ingest{lz_key}landing{env_name}\"\n",
    "external_storage = f\"ingest{lz_key}external{env_name}\"\n",
    "\n",
    "\n",
    "# Spark config for curated storage (Delta table)\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{curated_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{curated_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{curated_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{curated_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{curated_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{checkpoint_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{checkpoint_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{checkpoint_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{checkpoint_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{checkpoint_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{raw_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{raw_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{raw_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{raw_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{raw_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{landing_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{landing_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{landing_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{landing_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{landing_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{external_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{external_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{external_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{external_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{external_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e169fc23-7079-4102-8017-e009060ec361",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Assign Paths"
    }
   },
   "outputs": [],
   "source": [
    "AppealState = \"reasonsForAppealSubmitted\"\n",
    "output_name = \"reasons_for_appeal_submitted\"\n",
    "\n",
    "# Setting variables for use in subsequent cells\n",
    "bronze_path = f\"abfss://bronze@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/\"\n",
    "silver_path = f\"abfss://silver@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/\"\n",
    "audit_path = f\"abfss://silver@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/AUDIT/{AppealState}\"\n",
    "gold_outputs = f\"ARIADM/ACTIVE/CCD/APPEALS/{AppealState}\"\n",
    "\n",
    "# Print all variables\n",
    "variables = {\n",
    "    # \"read_hive\": read_hive,\n",
    "    \n",
    "    \"bronze_path\": bronze_path,\n",
    "    \"silver_path\": silver_path,\n",
    "    \"audit_path\": audit_path,\n",
    "    \"gold_outputs\": gold_outputs,\n",
    "    \"key_vault\": KeyVault_name,\n",
    "    \"AppealState\": AppealState\n",
    "\n",
    "}\n",
    "\n",
    "display(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa39248b-be0a-4f70-96fe-5b6bb4713cf3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Field group functions"
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import shared_functions.paymentPending as PP\n",
    "import shared_functions.appealSubmitted as APS\n",
    "import shared_functions.AwaitingEvidenceRespondant_a as AERa\n",
    "import shared_functions.AwaitingEvidenceRespondant_b as AERb\n",
    "import shared_functions.reasonsForAppealSubmitted as RFPAS\n",
    "import shared_functions.caseUnderReview as CUR\n",
    "from shared_functions.DQRules import base_DQRules, build_rule_expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d30465e-ae52-4a05-9eab-bba6e0964456",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read in tables"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "silver_m1 = spark.table(\"ariadm_active_appeals.silver_appealcase_detail\").filter(col(\"dv_targetState\") == lit(AppealState)).distinct()\n",
    "silver_m2 = spark.table(\"ariadm_active_appeals.silver_caseapplicant_detail\") \n",
    "silver_m3 = spark.table(\"ariadm_active_appeals.silver_status_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "silver_m4 = spark.table(\"ariadm_active_appeals.silver_transaction_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "silver_m5 = spark.table(\"ariadm_active_appeals.silver_link_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "silver_m6 = spark.table(\"ariadm_active_appeals.silver_adjudicator_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "silver_c = spark.table(\"ariadm_active_appeals.silver_appealcategory_detail\")\n",
    "bronze_countryFromAddress = spark.table(\"ariadm_active_appeals.bronze_countries_countryFromAddress\").withColumn(\"lu_countryGovUkOocAdminJ\",col(\"countryGovUkOocAdminJ\"))\n",
    "bronze_HORef_cleansing = spark.table(\"ariadm_active_appeals.bronze_HORef_cleansing\")\n",
    "bronze_remission_lookup_df = spark.table(\"ariadm_active_appeals.bronze_remissions\").distinct()\n",
    "\n",
    "silver_h = spark.table(\"ariadm_active_appeals.silver_history_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "bronze_remissions_lookup_df = spark.table(\"ariadm_active_appeals.bronze_remissions\").distinct()\n",
    "bronze_countryFromAddress = spark.table(\"ariadm_active_appeals.bronze_countries_countryFromAddress\")\n",
    "bronze_HORef_cleansing = spark.table(\"ariadm_active_appeals.bronze_HORef_cleansing\")\n",
    "bronze_hearing_centres = spark.table(\"ariadm_active_appeals.bronze_hearing_centres\")\n",
    "bronze_derive_hearing_centres = spark.table(\"ariadm_active_appeals.bronze_derive_hearing_centres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "659794bc-4ee7-4c06-9b49-c722683888ff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: AppealType"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = RFPAS.appealType(silver_m1)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1bfe0c7-33ea-435f-a6ac-6a0c1ea44dca",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: caseData"
    }
   },
   "outputs": [],
   "source": [
    "df,df_audit = RFPAS.caseData(silver_m1, silver_m2, silver_m3, silver_h, bronze_hearing_centres, bronze_derive_hearing_centres)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7e0ce82-5fc6-447e-8766-1ed43cbc3885",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = RFPAS.flagsLabels(silver_m1, silver_m2, silver_c)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d148c5d9-d290-4ffd-9659-73b56c3d0d94",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: legalRepDetails"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = RFPAS.legalRepDetails(silver_m1)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4280437-d43a-4e88-8612-c1815ea9ff9b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: appellantDetails"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = RFPAS.appellantDetails(silver_m1, silver_m2, silver_c, bronze_countryFromAddress,bronze_HORef_cleansing)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25f86a21-304d-47ab-a385-5a4d6162ca2f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: homeOfficeDetails"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = RFPAS.homeOfficeDetails(silver_m1, silver_m2, silver_c, bronze_HORef_cleansing)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd6c6c7d-0b83-4885-aa92-67a57792bbda",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: paymentType"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = RFPAS.paymentType(silver_m1, silver_m4)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89d0182e-789e-41ed-9ab9-332fafb77900",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: PartyID"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = RFPAS.partyID(silver_m1, silver_m3, silver_c)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eab929a0-fcdf-49d9-9132-ac3f17a7b407",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: remissionTypes"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = RFPAS.remissionTypes(silver_m1, bronze_remission_lookup_df,silver_m4)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d80dfab-2797-4893-8f3a-e3bcb57bb3d3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: sponsorDetails"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = RFPAS.sponsorDetails(silver_m1, silver_c)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79f4a42f-6964-446d-b41d-cf2c2de6c56e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: hearingResponse"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = RFPAS.hearingResponse(silver_m1, silver_m3, silver_m6)\n",
    "# display(df)\n",
    "# display(df_audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19921c7b-4c43-4878-96a8-3f8738309915",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: GeneralFunctions"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = RFPAS.general(silver_m1, silver_m2, silver_m3, silver_h, bronze_hearing_centres, bronze_derive_hearing_centres)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcef7ae8-c0da-4d0a-aee4-1137a3a15e12",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: GeneralDefault - Default Values"
    }
   },
   "outputs": [],
   "source": [
    "df = RFPAS.generalDefault(silver_m1)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d457be5-e5be-485d-8603-197482ea366e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: Documents"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = RFPAS.documents(silver_m1)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cf99f49-6cf0-45b8-9cfd-8077c057bd86",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: caseState"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = PP.caseState(silver_m1,\"reasonsForAppealSubmitted\")\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5552c83b-7c45-4994-a403-1b94dc87827f",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757579036014}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Function: mainReasonForAppealSubmitted"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, StructType, ArrayType, MapType\n",
    "from pyspark.sql.functions import col, lit, to_json, struct, concat, regexp_replace\n",
    "from datetime import datetime\n",
    "\n",
    "def mainPaymentPending(silver_segmentation, silver_m1, silver_m2, silver_m3, silver_c,silver_h, bronze_remissions, bronze_countryFromAddress, bronze_HORef_cleansing,bronze_hearing_centres,bronze_derive_hearing_centres):\n",
    "    AppealState = \"reasonsForAppealSubmitted\"\n",
    "\n",
    "    # Aggregate details\n",
    "    AppealType_df, AppealType_df_audit = RFPAS.appealType(silver_m1)\n",
    "    caseData_df, caseData_df_audit = RFPAS.caseData(silver_m1, silver_m2, silver_m3, silver_h, bronze_hearing_centres, bronze_derive_hearing_centres)\n",
    "    flagsLabels_df, flagsLabels_df_audit = RFPAS.flagsLabels(silver_m1, silver_m2, silver_c)\n",
    "    appellantDetails_df, appellantDetails_df_audit = RFPAS.appellantDetails(silver_m1, silver_m2, silver_c, bronze_countryFromAddress,bronze_HORef_cleansing)\n",
    "    legalRepDetails_df, legalRepDetails_df_audit = RFPAS.legalRepDetails(silver_m1)\n",
    "    partyID_df, partyID_df_audit = RFPAS.partyID(silver_m1, silver_m3, silver_c)\n",
    "    payment_df, payment_df_audit = RFPAS.paymentType(silver_m1, silver_m4)\n",
    "    homeOfficeDetails_df, homeOfficeDetails_df_audit = RFPAS.homeOfficeDetails(silver_m1, silver_m2, silver_c, bronze_HORef_cleansing)\n",
    "    remissionTypes_df, remissionTypes_df_audit = RFPAS.remissionTypes(silver_m1, bronze_remissions, silver_m4)\n",
    "    sponsorDetails_df, sponsorDetails_df_audit = RFPAS.sponsorDetails(silver_m1, silver_c)\n",
    "    general_df, general_df_audit = RFPAS.general(silver_m1, silver_m2, silver_m3, silver_h, bronze_hearing_centres, bronze_derive_hearing_centres)\n",
    "    generalDefault_df = RFPAS.generalDefault(silver_m1)\n",
    "    documents_df, documents_df_audit = RFPAS.documents(silver_m1)\n",
    "    caseState_df, caseState_df_audit = PP.caseState(silver_m1, \"reasonsForAppealSubmitted\")\n",
    "    hearingResponse_df, hearingResponse_df_audit = RFPAS.hearingResponse(silver_m1, silver_m3, silver_m6)\n",
    "    silver_segmentation_df = silver_segmentation\n",
    "\n",
    "    # Join all aggregated data with Appeal Case Details\n",
    "    df_combined = (\n",
    "        silver_segmentation_df.join(AppealType_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(caseData_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(flagsLabels_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(appellantDetails_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(legalRepDetails_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(sponsorDetails_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(partyID_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(payment_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(remissionTypes_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(homeOfficeDetails_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(caseState_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(hearingResponse_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(general_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(generalDefault_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(documents_df, on=\"CaseNo\", how=\"left\")\n",
    "    \n",
    "    )\n",
    "\n",
    "    # Join all aggregated data with Appeal Case Details\n",
    "    df_combined_audit = (\n",
    "        silver_segmentation_df.join(AppealType_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(caseData_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(flagsLabels_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(appellantDetails_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(legalRepDetails_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(sponsorDetails_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(partyID_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(payment_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(remissionTypes_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(homeOfficeDetails_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(caseState_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(hearingResponse_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(general_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(documents_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    Datetime_name = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "    # Create JSON and filename and omit columns that are with null values\n",
    "    df_final = df_combined.withColumn(\n",
    "        \"JSON_Content\", to_json(struct(*df_combined.drop(col(\"CaseNo\")).columns))\n",
    "    ).withColumn(\n",
    "        \"JSON_File_name\", concat(lit(f\"{gold_outputs}/{Datetime_name}/JSON/APPEALS_\"), regexp_replace(col(\"CaseNo\"), \"/\", \"_\"), lit(\".json\"))\n",
    "    )\n",
    "    \n",
    "    return df_final, df_combined_audit\n",
    "\n",
    "########### Test ##########\n",
    "\n",
    "silver_m1 = spark.table(\"ariadm_active_appeals.silver_appealcase_detail\").filter(col(\"dv_targetState\") == lit(AppealState)).distinct()\n",
    "silver_m2 =  spark.table(\"ariadm_active_appeals.silver_caseapplicant_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "silver_m3 = spark.table(\"ariadm_active_appeals.silver_status_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "silver_c = spark.table(\"ariadm_active_appeals.silver_appealcategory_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "bronze_remissions = spark.table(\"ariadm_active_appeals.bronze_remissions\").distinct()\n",
    "silver_segmentation = spark.table(\"ariadm_active_appeals.stg_segmentation_states\").filter(col(\"TargetState\") == lit(AppealState))\n",
    "\n",
    "bronze_countryFromAddress = spark.table(\"ariadm_active_appeals.bronze_countries_countryFromAddress\").withColumn(\"lu_countryGovUkOocAdminJ\",col(\"countryGovUkOocAdminJ\"))\n",
    "\n",
    "bronze_HORef_cleansing = spark.table(\"ariadm_active_appeals.bronze_HORef_cleansing\")\n",
    "\n",
    "df_final, df_audit = mainPaymentPending(silver_segmentation, silver_m1, silver_m2, silver_m3, silver_c,silver_h, bronze_remissions, bronze_countryFromAddress, bronze_HORef_cleansing,bronze_hearing_centres,bronze_derive_hearing_centres)\n",
    "\n",
    "display(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1e06424-6966-4125-9563-3d73261b37c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Function: Upload  and Blob Client Connection Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bd38396-bc28-4971-84ae-805bfd07e775",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Secret Retrieval for Database Connection"
    }
   },
   "outputs": [],
   "source": [
    "secret = dbutils.secrets.get(KeyVault_name, f\"CURATED-{env_name}-SAS-TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5169b7b1-135d-4de4-a228-a25b02b66f97",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Azure Blob Storage Connection Setup in Python"
    }
   },
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "import os\n",
    "\n",
    "# Set up the BlobServiceClient with your connection string\n",
    "connection_string = secret\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Specify the container name\n",
    "container_name = \"gold\"\n",
    "container_client = blob_service_client.get_container_client(container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd8881aa-9b58-41e1-ad05-93d0230af4c5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: Format Dates for UploadBlob Storage"
    }
   },
   "outputs": [],
   "source": [
    "# Upload HTML to Azure Blob Storage\n",
    "def upload_to_blob(file_name, file_content):\n",
    "    try:\n",
    "        # blob_client = container_client.get_blob_client(f\"{gold_outputs}/HTML/{file_name}\")\n",
    "        blob_client = container_client.get_blob_client(f\"{file_name}\")\n",
    "        blob_client.upload_blob(file_content, overwrite=True)\n",
    "        return \"success\"\n",
    "    except Exception as e:\n",
    "        return f\"error: {str(e)}\"\n",
    "\n",
    "# Register the upload function as a UDF\n",
    "upload_udf = udf(upload_to_blob)\n",
    "\n",
    "# df_with_upload_status = df_final.withColumn(\n",
    "#     \"Status\", upload_udf(col(\"JSON_File_name\"), col(\"JSON_Content\"))\n",
    "# )\n",
    "\n",
    "# display(df_with_upload_status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db97feb8-762a-40b6-8cb9-5afa8f134fe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Gold Outputs and Tracking DLT Table Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68b88c16-e4c6-4ba2-81c9-7f755c89ae30",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Combine Validation Checks into String"
    }
   },
   "outputs": [],
   "source": [
    "checks = {}\n",
    "checks = base_DQRules()\n",
    "\n",
    "checks[\"valid_additionalInstructionsTribunalResponse\"] = (\n",
    "  \"((additionalInstructionsTribunalResponse IS NOT NULL and CaseStatus = '26') OR (additionalInstructionsTribunalResponse IS NULL and CaseStatus != '26'))\"\n",
    ")\n",
    "\n",
    "dq_rules = build_rule_expression(checks)\n",
    "dq_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3d01276-8aa3-40b8-9143-07edd5e2f8bc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transformation: stg_main_payment_pending_validation"
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, lit, expr\n",
    "from pyspark.sql import Window\n",
    "\n",
    "@dlt.table(\n",
    "    name=f\"stg_main_{output_name}_validation\",\n",
    "    comment=\"DLT table running mainPaymentPending to generate a JSON_Content column for CCD validation. Applies DLT expectations on CCD, adding is_valid to flag validation results.\",\n",
    "    path=f\"{audit_path}/stg_main_{output_name}_validation\"\n",
    ")\n",
    "@dlt.expect_all(checks)\n",
    "def stg_main_reasonsForAppealSubmitted_validation():\n",
    "    try:\n",
    "        silver_m1 = dlt.read(\"silver_appealcase_detail\").filter(col(\"dv_targetState\") == lit(AppealState)).distinct()\n",
    "        bronze_appealtype_lookup_df = dlt.read(\"bronze_appealtype\").distinct()\n",
    "        bronze_hearing_centres_lookup_df = dlt.read(\"bronze_hearing_centres\").distinct()\n",
    "        # stg_representation = dlt.read(\"stg_representation\").select(col(\"Representation\").alias(\"valid_representation\"))\n",
    "        silver_m2 = dlt.read(\"silver_caseapplicant_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_m3 = dlt.read(\"silver_status_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_m4 = dlt.read(\"silver_transaction_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_m5 = dlt.read(\"silver_link_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_m6 = dlt.read(\"silver_adjudicator_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_c = dlt.read(\"silver_appealcategory_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m8 = dlt.read(\"silver_documentsreceived_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_h = dlt.read(\"silver_history_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        bronze_countries_postal_lookup_df = dlt.read(\"bronze_countries_postal\").distinct()\n",
    "        bronze_remissions = dlt.read(\"bronze_remissions\").distinct()\n",
    "        bronze_countryFromAddress = dlt.read(\"bronze_countries_countryFromAddress\")\n",
    "        bronze_HORef_cleansing = dlt.read(\"bronze_HORef_cleansing\")\n",
    "        bronze_hearing_centres = dlt.read(\"bronze_hearing_centres\")\n",
    "        bronze_derive_hearing_centres = dlt.read(\"bronze_derive_hearing_centres\")\n",
    "        silver_segmentation = dlt.read(\"stg_segmentation_states\").filter(col(\"TargetState\") == lit(AppealState))\n",
    "        \n",
    "    except:\n",
    "        silver_m1 = spark.table(\"ariadm_active_appeals.silver_appealcase_detail\").filter(col(\"dv_targetState\") == lit(AppealState)).distinct()\n",
    "        bronze_appealtype_lookup_df = spark.table(\"ariadm_active_appeals.bronze_appealtype\").distinct()\n",
    "        bronze_hearing_centres_lookup_df = spark.table(\"ariadm_active_appeals.bronze_hearing_centres\").distinct()\n",
    "        # stg_representation = spark.table(\"ariadm_active_appeals.stg_representation\").select(col(\"Representation\").alias(\"valid_representation\"))\n",
    "        silver_m2 = spark.table(\"ariadm_active_appeals.silver_caseapplicant_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_m3 = spark.table(\"ariadm_active_appeals.silver_status_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_m4 = spark.table(\"ariadm_active_appeals.silver_transaction_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_m5 = spark.table(\"ariadm_active_appeals.silver_link_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_m6 = spark.table(\"ariadm_active_appeals.silver_adjudicator_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_c = spark.table(\"ariadm_active_appeals.silver_appealcategory_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m8 = spark.table(\"ariadm_active_appeals.silver_documentsreceived_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_h = spark.table(\"ariadm_active_appeals.silver_history_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        bronze_countries_postal_lookup_df = spark.table(\"ariadm_active_appeals.bronze_countries_postal\").distinct() \n",
    "        bronze_remissions = spark.table(\"ariadm_active_appeals.bronze_remissions\").distinct()\n",
    "        bronze_countryFromAddress = spark.table(\"ariadm_active_appeals.bronze_countries_countryFromAddress\")\n",
    "        bronze_HORef_cleansing = spark.table(\"ariadm_active_appeals.bronze_HORef_cleansing\")\n",
    "        bronze_hearing_centres = spark.table(\"ariadm_active_appeals.bronze_hearing_centres\")\n",
    "        bronze_derive_hearing_centres = spark.table(\"ariadm_active_appeals.bronze_derive_hearing_centres\")\n",
    "        silver_segmentation = spark.table(\"ariadm_active_appeals.stg_segmentation_states\").filter(col(\"TargetState\") == lit(AppealState))\n",
    "\n",
    "    df_final,df_audit = mainPaymentPending(silver_segmentation, silver_m1, silver_m2, silver_m3, silver_c,silver_h, bronze_remissions, bronze_countryFromAddress, bronze_HORef_cleansing,bronze_hearing_centres,bronze_derive_hearing_centres)\n",
    "\n",
    "    valid_representation = silver_m1.select(col(\"CaseNo\"), col(\"dv_representation\"),col(\"dv_CCDAppealType\"),col(\"CaseRep_Address5\"), col(\"CaseRep_Postcode\"),col(\"MainRespondentId\"), col(\"lu_appealType\"), col(\"HORef\"),col(\"Sponsor_Authorisation\"),col(\"Sponsor_Name\")) \n",
    "    valid_appealant_address = silver_m2.select(col(\"CaseNo\"), col(\"Appellant_Address1\"), col(\"Appellant_Address2\"),col(\"Appellant_Address3\"),(\"Appellant_Address4\"), col(\"Appellant_Address5\"), col(\"Appellant_Postcode\"),col(\"Appellant_Email\"),col(\"Appellant_Telephone\"), col(\"FCONumber\")).filter(col(\"Relationship\").isNull())\n",
    "    valid_country_list = bronze_countries_postal_lookup_df.select(col(\"countryGovUkOocAdminJ\").alias(\"valid_countryGovUkOocAdminJ\")).distinct()\n",
    "    valid_catagoryid_list = silver_c.groupBy(\"CaseNo\").agg(F.collect_list(\"CategoryId\").alias(\"valid_categoryIdList\"))\n",
    "    valid_HORef_cleansing = bronze_HORef_cleansing.select( col(\"CaseNo\"),coalesce(col(\"HORef\"), col(\"FCONumber\")).alias(\"lu_HORef\"))\n",
    "    valid_reasonDescription = silver_m1.alias(\"m1\").join(bronze_remissions, on=[\"PaymentRemissionReason\",\"PaymentRemissionRequested\"], how=\"left\").select(\"CaseNo\", \"ReasonDescription\",col(\"remissionClaim\").alias(\"lu_remissionClaim\"),col(\"feeRemissionType\").alias(\"lu_feeRemissionType\"))\n",
    "    window_spec = Window.partitionBy(\"CaseNo\").orderBy(desc(\"StatusId\"))\n",
    "    valid_caseStatus = (silver_m3.select(col(\"CaseNo\"),col(\"CaseStatus\"), col(\"StatusId\"), row_number().over(window_spec).alias(\"rn\")\n",
    "                                         ).filter(col(\"rn\") == 1)  # keep only the latest row per CaseNo\n",
    "    .drop(\"rn\",\"StatusId\"))\n",
    "    valid_statusId = silver_m3.select(col(\"CaseNo\"), col(\"StatusId\"), col('CaseStatus'), col('Outcome'))\n",
    " \n",
    "    df_final = df_final.join(valid_representation, on=\"CaseNo\", how=\"left\"\n",
    "                            ).join(valid_country_list, on=col(\"CaseRep_Address5\") == col(\"valid_countryGovUkOocAdminJ\"), how=\"left\"\n",
    "                            ).join(valid_catagoryid_list, on=\"CaseNo\", how=\"left\"\n",
    "                            ).join(valid_appealant_address, on=\"CaseNo\", how=\"left\"\n",
    "                            ).join(valid_HORef_cleansing, on=\"CaseNo\", how=\"left\"\n",
    "                            ).join(valid_reasonDescription, on=\"CaseNo\", how=\"left\"\n",
    "                            ).join(valid_statusId, on=\"CaseNo\", how=\"left\")\n",
    " \n",
    "    df_final = df_final.withColumn(\"is_valid\", expr(dq_rules))\n",
    "\n",
    "    # df_final = df_final.drop(col(\"dv_representation\"), col(\"CaseRepAddress5\"), col(\"CaseRepPostcode\"), col(\"valid_countryGovUkOocAdminJ\"))\n",
    "\n",
    "    # columns_to_drop = [\"dv_representation\", \"CaseRepAddress5\", \"CaseRepPostcode\"]\n",
    "\n",
    "    # if all(col in df_final.columns for col in columns_to_drop): #If the columns exist - remove \n",
    "    #     df_final = df_final.drop(*columns_to_drop)\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3362b255-72e6-4a4f-8b12-170c233128ad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transformation: stg_valid_payment_pending_records"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=f\"stg_valid_{output_name}_records\",\n",
    "    comment=\"Delta Live Gold Table with JSON content.\",\n",
    "    path=f\"{audit_path}/stg_valid_{output_name}_records\"\n",
    ")\n",
    "def stg_valid_reasonsforAppealSubmitted_records():\n",
    "    \"\"\"\n",
    "    Delta Live Table for creating and uploading JSON content for Appeals.\n",
    "    \"\"\"\n",
    "    # Load source data\n",
    "    df = dlt.read(f\"stg_main_{output_name}_validation\")\n",
    "\n",
    "    df_filtered = df.filter(\n",
    "        (col(\"is_valid\") == True)\n",
    "    )\n",
    "\n",
    "    # Repartition to optimize parallelism\n",
    "    repartitioned_df = df_filtered.repartition(64)\n",
    "\n",
    "    df_with_upload_status = repartitioned_df.filter(~col(\"JSON_content\").like(\"Error%\")).withColumn(\n",
    "            \"Status\", upload_udf(col(\"JSON_File_Name\"), col(\"JSON_content\"))\n",
    "        )\n",
    "\n",
    "    # Return the DataFrame for DLT table creation\n",
    "    return df_with_upload_status.select(\"CaseNo\", \"JSON_content\",col(\"JSON_File_Name\").alias(\"File_Name\"),\"Status\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60bfdf72-3653-4133-ae58-42f66d648a28",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transformation: stg_invalid_payment_pending_quarantine_records"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=f\"stg_invalid_{output_name}_quarantine_records\",\n",
    "    comment=\"Quarantined records that failed data quality checks or JSON generation.\",\n",
    "    path=f\"{audit_path}/stg_invalid_{output_name}_quarantine_records\"\n",
    ")\n",
    "def stg_invalid_reasonsForAppealSubmitted_quarantine_records():\n",
    "\n",
    "    df = dlt.read(f\"stg_main_{output_name}_validation\")\n",
    "\n",
    "    df_filtered = df.filter(\n",
    "        (col(\"is_valid\") != True)\n",
    "    ).withColumn(\"JSON_File_Name\", regexp_replace(col(\"JSON_File_Name\"), \"/JSON/\", \"/INVALID_JSON/\"))\n",
    "\n",
    "    # Repartition to optimize parallelism\n",
    "    repartitioned_df = df_filtered.repartition(64)\n",
    "\n",
    "    df_with_upload_status = repartitioned_df.filter(~col(\"JSON_content\").like(\"Error%\")).withColumn(\n",
    "            \"Status\", upload_udf(col(\"JSON_File_Name\"), col(\"JSON_content\"))\n",
    "        )\n",
    "\n",
    "    return df_with_upload_status.select(\"CaseNo\", \"JSON_content\",col(\"JSON_File_Name\").alias(\"File_Name\"),\"Status\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a81bf947-c500-4014-b228-71862de0a3fc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transformation: apl_active_payment_pending_cr_audit_table"
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, lit, expr\n",
    "\n",
    "@dlt.table(\n",
    "    name=f\"apl_active_{output_name}_cr_audit_table\",\n",
    "    comment=\"DLT table Covers 4.2 Silver layer LLD requirements: Audits CCD attributes, input field values, derived values, and all columns for validation and traceability.\",\n",
    "    path=f\"{audit_path}/apl_active_{output_name}_cr_audit_table\"\n",
    ")\n",
    "def apl_active_reasonsForAppealSubmitted_cr_audit_table():\n",
    "    try:\n",
    "        silver_m1 = dlt.read(\"silver_appealcase_detail\").filter(col(\"dv_targetState\") == lit(AppealState)).distinct()\n",
    "        silver_m2 = dlt.read(\"silver_caseapplicant_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        bronze_appealtype_lookup_df = dlt.read(\"bronze_appealtype\").distinct()\n",
    "        bronze_hearing_centres_lookup_df = dlt.read(\"bronze_hearing_centres\").distinct()\n",
    "        silver_m3 = dlt.read(\"silver_status_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_c = dlt.read(\"ariadm_active_appeals.silver_appealcategory_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_h = dlt.read(\"silver_history_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        bronze_remission_lookup_df = dlt.read(\"bronze_remissions\").distinct()\n",
    "        bronze_remissions_lookup_df = dlt.read(\"bronze_remissions\").distinct()\n",
    "        bronze_countryFromAddress = dlt.read(\"bronze_countries_countryFromAddress\")\n",
    "        bronze_HORef_cleansing = dlt.read(\"bronze_HORef_cleansing\")\n",
    "        bronze_hearing_centres = dlt.read(\"bronze_hearing_centres\")\n",
    "        bronze_derive_hearing_centres = dlt.read(\"bronze_derive_hearing_centres\")\n",
    "        silver_segmentation = dlt.read(\"stg_segmentation_states\").filter(col(\"TargetState\") == lit(AppealState))\n",
    "      \n",
    "    except:\n",
    "        silver_m1 = spark.table(\"ariadm_active_appeals.silver_appealcase_detail\").filter(col(\"dv_targetState\") == lit(AppealState)).distinct()\n",
    "        silver_m2 = spark.table(\"ariadm_active_appeals.silver_caseapplicant_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        bronze_appealtype_lookup_df = spark.table(\"ariadm_active_appeals.bronze_appealtype\").distinct()\n",
    "        bronze_hearing_centres_lookup_df = spark.table(\"ariadm_active_appeals.bronze_hearing_centres\").distinct()\n",
    "        silver_m3 = spark.table(\"ariadm_active_appeals.silver_status_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_c = spark.table(\"ariadm_active_appeals.silver_appealcategory_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_h = spark.table(\"ariadm_active_appeals.silver_history_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        bronze_remissions_lookup_df = spark.table(\"ariadm_active_appeals.bronze_remissions\").distinct()\n",
    "        bronze_countryFromAddress = spark.table(\"ariadm_active_appeals.bronze_countries_countryFromAddress\")\n",
    "        bronze_HORef_cleansing = spark.table(\"ariadm_active_appeals.bronze_HORef_cleansing\")\n",
    "        bronze_hearing_centres = spark.table(\"ariadm_active_appeals.bronze_hearing_centres\")\n",
    "        bronze_derive_hearing_centres = spark.table(\"ariadm_active_appeals.bronze_derive_hearing_centres\")\n",
    "        silver_segmentation = spark.table(\"ariadm_active_appeals.stg_segmentation_states\").filter(col(\"TargetState\") == lit(AppealState))\n",
    "\n",
    " \n",
    "    df_final,df_audit = mainPaymentPending(silver_segmentation, silver_m1, silver_m2, silver_m3, silver_c,silver_h, bronze_remissions_lookup_df, bronze_countryFromAddress, bronze_HORef_cleansing,bronze_hearing_centres,bronze_derive_hearing_centres)\n",
    "\n",
    "    return df_audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c065211-f4d5-47d6-9f7f-757fedeaa149",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Exit Notebook"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Notebook completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4671000946785512,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "GOLD_REASON_FOR_APPEAL_SUBMITTED_JSON",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
