{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe8b8579-1f8c-419e-a16b-c2aa7b9b32d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "from pyspark.sql.functions import col, decode, split, element_at,udf,input_file_name,regexp_extract,expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3064dc6-ae9b-4e63-8339-26ed44160a42",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define Schema for ARIAB Transaction Logs"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "expected_schema = StructType([\n",
    "    StructField(\"operation\", StringType(), True),\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"relation_id\", StringType(), True),\n",
    "    StructField(\"a360_record_id\", StringType(), True),\n",
    "    StructField(\"process_time\", TimestampType(), True),\n",
    "    StructField(\"status\", IntegerType(), True),\n",
    "    StructField(\"input\", StringType(), True),  # Contains nested JSON as a string\n",
    "    StructField(\"exception_description\", StringType(), True),\n",
    "    StructField(\"error_status\", StringType(), True),\n",
    "    StructField(\"a360_file_id\", StringType(), True),\n",
    "    StructField(\"file_size\", LongType(), True),\n",
    "    StructField(\"s_md5\", StringType(), True),\n",
    "    StructField(\"s_sha256\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),  # may be used as process_time\n",
    "    StructField(\"filename\", StringType(), True),\n",
    "    StructField(\"submission_folder\", StringType(), True),\n",
    "    StructField(\"file_hash\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2de9ecc-5dae-46bb-a899-5c7bd845b1ba",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Up Auto Loader for ARIAB Response Files"
    }
   },
   "outputs": [],
   "source": [
    "#### Set up Auto Loader job\n",
    "\n",
    "sas_token = dbutils.secrets.get(scope=\"ingest00-meta002-sbox\", key=\"ARIAJR-SAS-TOKEN\")\n",
    "storage_account_name = \"a360c2x2555dz\"\n",
    "container_name = \"dropzone\"\n",
    "sub_dir = \"ARIATD/response\"\n",
    "\n",
    "input_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{sub_dir}\"\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.{container_name}.{storage_account_name}.blob.core.windows.net\",\n",
    "    sas_token\n",
    ")\n",
    "\n",
    "# schema_location = \"/mnt/autoLoaderSchema/ARMJOH/response/read_stream\"\n",
    "\n",
    "# Define a file regex that matches files ending with .rsp\n",
    "file_regex = \".*\\\\.rsp$\"\n",
    "\n",
    "output_container_name = \"silver\" \n",
    "\n",
    "output_storage_account_name = \"ingest00curatedsbox\"\n",
    "\n",
    "output_subdir_amalgamated_responses = \"ARIADM/ARM/response/TD/amalgamated_responses\"\n",
    "amalgamated_responses_path = f\"wasbs://{output_container_name}@{output_storage_account_name}.blob.core.windows.net/{output_subdir_amalgamated_responses}\"\n",
    "\n",
    "\n",
    "\n",
    "output_subdir_input_upload = \"ARIADM/ARM/response/TD/input_upload\"\n",
    "input_upload_responses_path = f\"wasbs://{output_container_name}@{output_storage_account_name}.blob.core.windows.net/{output_subdir_input_upload}\"\n",
    "\n",
    "\n",
    "output_subdir_create_record_upload = \"ARIADM/ARM/response/TD/create_record\"\n",
    "create_record_responses_path = f\"wasbs://{output_container_name}@{output_storage_account_name}.blob.core.windows.net/{output_subdir_create_record_upload}\"\n",
    "\n",
    "\n",
    "output_subdir_upload_file_upload = \"ARIADM/ARM/response/TD/upload_file\"\n",
    "upload_file_responses_path = f\"wasbs://{output_container_name}@{output_storage_account_name}.blob.core.windows.net/{output_subdir_upload_file_upload}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6771dfbc-cbc4-413b-a3e3-2557cec9cee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Service principal credentials\n",
    "client_id = dbutils.secrets.get(\"ingest00-meta002-sbox\", \"SERVICE-PRINCIPLE-CLIENT-ID\")\n",
    "client_secret = dbutils.secrets.get(\"ingest00-meta002-sbox\", \"SERVICE-PRINCIPLE-CLIENT-SECRET\")\n",
    "tenant_id = dbutils.secrets.get(\"ingest00-meta002-sbox\", \"SERVICE-PRINCIPLE-TENANT-ID\")\n",
    "\n",
    "# Storage account names\n",
    "\n",
    "checkpoint_storage = \"ingest00xcuttingsbox\"\n",
    "\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{checkpoint_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{checkpoint_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{checkpoint_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{checkpoint_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{checkpoint_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{output_storage_account_name}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{output_storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{output_storage_account_name}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{output_storage_account_name}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{output_storage_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "\n",
    "\n",
    "check_point_path = \"abfss://db-rsp-checkpoint@ingest00xcuttingsbox.dfs.core.windows.net/ARMTD/RSP/\"\n",
    "\n",
    "schema_location = \"abfss://db-rsp-checkpoint@ingest00xcuttingsbox.dfs.core.windows.net/ARMTD/RSP/schema\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0134ddf8-abb7-48de-8944-b270a66098c9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load JSON Files with Autoloader and Add UUID Column"
    }
   },
   "outputs": [],
   "source": [
    "### Run autoloader read stream\n",
    "\n",
    "\n",
    "df = (spark.readStream.format(\"cloudFiles\")\n",
    "      .schema(expected_schema)\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", schema_location)\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"none\")\n",
    "    .option(\"checkpointLocation\", f\"{check_point_path}/rsp_readStream\")\n",
    "             #\"/mnt/autoLoaderSchema/ARMBail/response/read_stream\")\n",
    "    .load(input_path)\n",
    "    .select( \"*\",col(\"_metadata.file_path\").alias(\"_file_path\"),\n",
    "             col(\"_metadata.file_modification_time\").alias(\"_file_modification_time\")\n",
    ")\n",
    "    # .withColumn(\"file_name\", regexp_extract(input_file_name(),\"response\\/(.*)\",1))\n",
    "    .withColumn(\"id\", expr(\"uuid()\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cb48889-ec0d-4629-860c-7b27398de93e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.rm(\"/mnt/autoLoaderSchema/ARMTD/response\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b1344b-bb6a-478f-ab75-faf44f8d3c0b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Save Complete Table to Silver Container in Blob Storage"
    }
   },
   "outputs": [],
   "source": [
    "### 4 write streams to Silver container in blob storage\n",
    "\n",
    "output_sas = dbutils.secrets.get(scope=\"ingest00-meta002-sbox\", key=\"CURATED-SAS-TOKEN\")\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.{output_container_name}.{output_storage_account_name}.blob.core.windows.net\",\n",
    "    output_sas\n",
    ")\n",
    "\n",
    "### first save the complete table\n",
    "\n",
    "df_complete = df.writeStream \\\n",
    "    .queryName(\"amalgamated\") \\\n",
    "    .format(\"delta\")\\\n",
    "        .option(\"checkpointLocation\", f\"{check_point_path}/amalgamated\")\\\n",
    "            .outputMode(\"append\")\\\n",
    "                    .start(amalgamated_responses_path)                #.trigger(availableNow=True)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af78918f-0723-48ca-874c-2236cd09a5df",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Stream Delta Table for ARMAppeals Create Record"
    }
   },
   "outputs": [],
   "source": [
    "### Create Record table\n",
    "df_create_upload = df.select(\"id\",\"operation\",\"transaction_id\",\"relation_id\",\"a360_record_id\",\"process_time\",\"status\",\"input\",\"exception_description\",\"error_status\",\"_file_path\",\"_file_modification_time\"\n",
    ").filter(col(\"operation\")==\"create_record\")\\\n",
    "    .writeStream \\\n",
    "    .queryName(\"create_record\") \\\n",
    "    .format(\"delta\")\\\n",
    "        .option(\"checkpointLocation\", f\"{check_point_path}/create_record\")\\\n",
    "            .outputMode(\"append\")\\\n",
    "                .start(create_record_responses_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca92cd23-a518-4a3d-b975-2de5a46f9e56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### upload file table\n",
    "df_upload = df.select(\"id\",\"operation\",\"transaction_id\",\"relation_id\",\"a360_record_id\",\"process_time\",\"status\",\"input\",\"exception_description\",\"error_status\",\"a360_file_id\",\"file_size\",\"s_md5\",\"s_sha256\",\"_file_path\",\"_file_modification_time\"\n",
    ").filter(col(\"operation\")==\"upload_new_file\")\\\n",
    "    .writeStream \\\n",
    "    .queryName(\"upload_new_file\") \\\n",
    "    .format(\"delta\")\\\n",
    "        .option(\"checkpointLocation\", f\"{check_point_path}/upload_file\")\\\n",
    "            .outputMode(\"append\")\\\n",
    "                .start(upload_file_responses_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6e31917-b32a-4df5-bd2f-4cf55de6c135",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Stream Delta Table for ARMAppeals Input Upload"
    }
   },
   "outputs": [],
   "source": [
    "### Input upload table\n",
    "\n",
    "df_input_upload = df.select(\"id\",\"operation\",\"timestamp\",\"status\",\"exception_description\",\"error_status\",\"filename\",\"submission_folder\",\"file_hash\",\"_file_path\",\"_file_modification_time\"\n",
    ").filter(col(\"operation\")==\"input_upload\")\\\n",
    "    .writeStream \\\n",
    "    .queryName(\"input_upload\") \\\n",
    "    .format(\"delta\")\\\n",
    "        .option(\"checkpointLocation\", f\"{check_point_path}/input_upload\")\\\n",
    "            .outputMode(\"append\")\\\n",
    "                .start(input_upload_responses_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait 15 minutes for any stream to stop itself, or auto stop after 15mins.\n",
    "\n",
    "try:\n",
    "    spark.streams.awaitAnyTermination(900)\n",
    "finally:\n",
    "    # Stop all streams\n",
    "    df_complete.stop()\n",
    "    df_input_upload.stop()\n",
    "    df_create_upload.stop()\n",
    "    df_upload.stop()\n",
    "    print(\"All streams have been stopped\")\n",
    "\n",
    "\n",
    "df_input_upload = spark.read.format(\"delta\").load(input_upload_responses_path)\n",
    " \n",
    "# # Read the response data for create_record_upload\n",
    "df_create_record_upload = spark.read.format(\"delta\").load(create_record_responses_path)\n",
    " \n",
    "# # Read the response data for upload_file\n",
    "df_upload_file_upload = spark.read.format(\"delta\").load(upload_file_responses_path) \n",
    " \n",
    "# Read the response data for df_amalgamated_responses\n",
    "df_amalgamated_responses = spark.read.format(\"delta\").load(amalgamated_responses_path)\n",
    "\n",
    "print(f\"Records in df_input_upload: {df_input_upload.count()}\")\n",
    "print(f\"Records in df_create_record_upload: {df_create_record_upload.count()}\")\n",
    "print(f\"Records in df_upload_file_upload: {df_upload_file_upload.count()}\")\n",
    "print(f\"Records in df_amalgamated_responses: {df_amalgamated_responses.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba0d3129-bc06-457f-8f7d-b4042b0456b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Notebook execution completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8428073b-7e4c-46a1-9559-9a3f33c72f08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9d5e3eb-f302-43b3-8e48-fcfbd3556310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# from pyspark.sql.functions import col, from_unixtime\n",
    "\n",
    "# files_df = spark.createDataFrame(dbutils.fs.ls(\"/mnt/dropzoneariatd/ARIATD/submission/\"))\n",
    "# files_df = files_df.withColumn(\"modificationTime\", from_unixtime(col(\"modificationTime\") / 1000).cast(\"timestamp\"))\n",
    "\n",
    "# display(files_df.orderBy(col(\"modificationTime\").desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "898a9135-f4c6-43cc-9cb0-6ff2ac93fd25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, from_unixtime\n",
    "\n",
    "# files_df = spark.createDataFrame(dbutils.fs.ls(\"/mnt/dropzoneariatd/ARIATD/submission/\"))\n",
    "# files_df = files_df.withColumn(\"modificationTime\", from_unixtime(col(\"modificationTime\") / 1000).cast(\"timestamp\"))\n",
    "\n",
    "# display(files_df.orderBy(col(\"modificationTime\").desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef9f6b65-0867-4406-bd19-c7b87e5d064f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, from_unixtime\n",
    "\n",
    "# files_df = spark.createDataFrame(dbutils.fs.ls(\"/mnt/dropzoneariatd/ARIATD/response/\"))\n",
    "# files_df = files_df.withColumn(\"modificationTime\", from_unixtime(col(\"modificationTime\") / 1000).cast(\"timestamp\"))\n",
    "\n",
    "# display(files_df.orderBy(col(\"modificationTime\").desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8e6b892-d7c0-4e4d-b9ea-9038ad7ac134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# json_count = files_df.filter(col(\"path\").endswith(\".json\")).count()\n",
    "# html_count = files_df.filter(col(\"path\").endswith(\".html\")).count()\n",
    "# a360_count = files_df.filter(col(\"path\").endswith(\".a360\")).count()\n",
    "\n",
    "# display(json_count)\n",
    "# display(html_count)\n",
    "# display(a360_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aa23faa-35e2-4b49-8c32-8fa7f3970544",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.ls(\"/mnt/dropzoneariatd/ARIATD/response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2248bba9-77db-4129-8709-101ac98d4406",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.rm(\"/mnt/autoLoaderSchema/ARMTD/response\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5870a49b-b181-4954-a19e-2551043ceabd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.ls(\"/mnt/ingest00curatedsboxsilver/ARIADM/ARM/response/APPEALS/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b038814-2a48-4d26-93e1-901497dbc3c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.rm(\"/mnt/ingest00curatedsboxsilver/ARIADM/ARM/response/APPEALS/\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d87d743c-8899-4de3-bb51-4edeacc9013d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(dbutils.fs.ls(\"/mnt/dropzoneariatd/ARIATD/response/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f99e1fe-080a-4039-8219-639dd996547c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.secrets.list(scope=\"ingest00-meta002-sbox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "789bf5c9-b66f-4fc2-b836-7595007c9ead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# from azure.storage.blob import ContainerClient\n",
    "\n",
    "# sas_token = dbutils.secrets.get(scope=\"ingest00-meta002-sbox\", key=\"ARIATD-SAS-TOKEN\")\n",
    "# storage_account_name = \"a360c2x2555dz\"\n",
    "# container_name = \"dropzone\"\n",
    "# sub_dir = \"ARIATD/submission\"\n",
    "# account_url = f\"https://{storage_account_name}.blob.core.windows.net\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d09b5a77-bed4-4f8c-a6c6-b34adbb36a4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# html_df = spark.sql(\"\"\"\n",
    "# select CaseNo, HTMLContent as content_str, substring_index(HTMLFileName, '/', -1) as file_path from ariadm_arm_td.gold_td_iris_with_html\n",
    "# where CaseNo = 'FY/46144/2015'\n",
    "# \"\"\")\n",
    "\n",
    "# json_df = spark.sql(\"\"\"\n",
    "#  select CaseNo, JSONcollection  as content_str, substring_index(JSONFileName, '/', -1) as file_path from ariadm_arm_td.gold_td_iris_with_Json\n",
    "#   where CaseNo = 'FY/46144/2015';\n",
    "# \"\"\")\n",
    "\n",
    "# a360_df = spark.sql(\"\"\"\n",
    "# select CaseNo, A360Content  as content_str, substring_index(A360FileName, '/', -1) as file_path from ariadm_arm_td.gold_td_iris_with_a360\n",
    "#  where CaseNo = 'FY/46144/2015';\n",
    "# \"\"\")\n",
    "\n",
    "# display(html_df)\n",
    "# display(json_df)\n",
    "# display(a360_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d62d3f3f-7bd0-464d-8a93-13e5c9b7dc53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# from datetime import datetime\n",
    "\n",
    "# results = []\n",
    "\n",
    "# df = a360_df\n",
    "\n",
    "\n",
    "# # Callback generator to capture responses for each file upload\n",
    "# def capture_response(result):\n",
    "#     def hook(response):\n",
    "#         http_response = response.http_response\n",
    "#         result[\"http_response\"] = http_response.status_code\n",
    "#         result[\"http_message\"] = getattr(http_response, \"reason\", \"No reason provided\")\n",
    "#         result[\"timestamp\"] = datetime.now().isoformat()\n",
    "#     return hook\n",
    "\n",
    "# try:\n",
    "#     # Create a ContainerClient using a container-level SAS token\n",
    "#     container_client = ContainerClient(\n",
    "#         account_url=account_url,\n",
    "#         container_name=container_name,\n",
    "#         credential=sas_token\n",
    "#     )\n",
    "#     print(\"Container client created\")\n",
    "\n",
    "#     for row in df.collect():\n",
    "#         message = row.content_str\n",
    "#         file_path = row.file_path\n",
    "\n",
    "#         file_name= f\"{sub_dir}/{file_path}\"\n",
    "\n",
    "#         blob_client = container_client.get_blob_client(file_name)\n",
    "\n",
    "#         result = {\n",
    "#             \"filename\": file_name,\n",
    "#             \"http_response\": None,\n",
    "#             \"timestamp\": None,\n",
    "#             \"http_message\": None\n",
    "#         }\n",
    "\n",
    "#         print(\"uploading message to blob storage\",file_name)\n",
    "#         blob_client.upload_blob(message,overwrite=True,raw_response_hook=capture_response(result))\n",
    "#         print(\"upload complete\")\n",
    "\n",
    "#         results.append(result)\n",
    "\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(\"Failed to connect to blob storage:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d996daf-af80-42d9-8c9c-b8e6856a95a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(dbutils.fs.ls(\"/mnt/dropzoneariatd/ARIATD/submission/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c577c1da-a905-4d42-8caa-673d0a46fc79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.ls(\"/mnt/dropzoneariatd/ARIATD/response\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9852733a-e6b2-4949-8b91-000b8c044fc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.ls(\"/mnt/dropzoneariatd/ARIATD/submission/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3913d9ff-eb68-4a94-8061-5c0ffcc2d9cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, from_unixtime\n",
    "\n",
    "# files_df = spark.createDataFrame(dbutils.fs.ls(\"/mnt/dropzoneariajr/ARIAJR/submission/\"))\n",
    "# files_df = files_df.withColumn(\"modificationTime\", from_unixtime(col(\"modificationTime\") / 1000).cast(\"timestamp\"))\n",
    "\n",
    "# display(files_df.orderBy(col(\"modificationTime\").desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5cd189f-7199-4666-9cda-096941400014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# files_count = files_df.filter(col(\"path\").like(\"%.json\") | col(\"path\").like(\"%.html\") | col(\"path\").like(\"%.a360\")).count()\n",
    "# display(files_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6822f564-ac64-4233-9e26-744c27ca0ed4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, from_unixtime\n",
    "\n",
    "# files_df = spark.createDataFrame(dbutils.fs.ls(\"/mnt/dropzoneariajr/ARIAJR/response\"))\n",
    "# files_df = files_df.withColumn(\"modificationTime\", from_unixtime(col(\"modificationTime\") / 1000).cast(\"timestamp\"))\n",
    "\n",
    "# display(files_df.orderBy(col(\"modificationTime\").desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c870427-83c7-4cbf-af10-6d59fadd2cd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the paths to the tables\n",
    "audit_delta_path = \"/mnt/ingest00curatedsboxsilver/ARIADM/ARM/AUDIT/TD/td_cr_audit_table\"\n",
    "ack_path = \"/mnt/ingest00curatedsboxsilver/ARIADM/ARM/AUDIT/TD/td_ack_audit\"\n",
    "output_subdir_input_upload = \"/mnt/ingest00curatedsboxsilver/ARIADM/ARM/response/TD/input_upload\"\n",
    "output_subdir_create_record_upload = \"/mnt/ingest00curatedsboxsilver/ARIADM/ARM/response/TD/create_record\"\n",
    "output_subdir_upload_file_upload = \"/mnt/ingest00curatedsboxsilver/ARIADM/ARM/response/TD/upload_file\"\n",
    "output_subdir_amalgamated_responses = \"/mnt/ingest00curatedsboxsilver/ARIADM/ARM/response/TD/amalgamated_responses\"\n",
    "\n",
    "# Read the Delta table for joh_cr_audit_table\n",
    "df_audit = spark.read.format(\"delta\").load(audit_delta_path)\n",
    "\n",
    "# Read the Delta table for joh_ack_audit\n",
    "df_ack = spark.read.format(\"delta\").load(ack_path)\n",
    "\n",
    "# Read the response data for input_upload\n",
    "# df_input_upload = spark.read.format(\"parquet\").load(output_subdir_input_upload)\n",
    "\n",
    "# # Read the response data for create_record_upload\n",
    "# df_create_record_upload = spark.read.format(\"parquet\").load(output_subdir_create_record_upload)\n",
    "\n",
    "# # Read the response data for upload_file\n",
    "# df_upload_file_upload = spark.read.format(\"parquet\").load(output_subdir_upload_file_upload)\n",
    "\n",
    "\n",
    "# Read the response data for df_amalgamated_responses\n",
    "# df_amalgamated_responses = spark.read.format(\"delta\").load(output_subdir_amalgamated_responses)\n",
    "\n",
    "# Display the DataFrames using Databricks display\n",
    "display(df_audit)\n",
    "display(df_ack)\n",
    "# display(df_input_upload)\n",
    "# display(df_create_record_upload)\n",
    "# display(df_upload_file_upload)\n",
    "# display(df_amalgamated_responses)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8492011652829405,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ARIADM_ARM_RESPONSE_AUTOLOADER_TD",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
