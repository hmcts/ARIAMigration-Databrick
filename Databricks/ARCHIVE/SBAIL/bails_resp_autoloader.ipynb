{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8821ae9-478a-4045-ae52-afc7d315405f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "from pyspark.sql.functions import col, decode, split, element_at,udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, LongType\n",
    "from pyspark.sql.functions import date_format, col, when, count, lit, concat, round as spark_round\n",
    "import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6ce55bd-2c34-4e4c-ad0c-04314816588a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"DatabricksWorkflow\")\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "if not logger.hasHandlers():\n",
    "    logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b241e2f-1f28-4c95-b00e-850adcb4fff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Load configuration JSON\n",
    "config_path = \"dbfs:/configs/config.json\"\n",
    "try:\n",
    "    config = spark.read.option(\"multiline\", \"true\").json(config_path)\n",
    "    logger.info(f\"Successfully read config file from {config_path}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not read config file at {config_path}: {e}\", exc_info=True)\n",
    "    raise FileNotFoundError(f\"Could not read config file at {config_path}: {e}\")\n",
    "\n",
    "#Extract environment and lz_key\n",
    "try:\n",
    "    first_row = config.first()\n",
    "    env = first_row[\"env\"].strip().lower()\n",
    "    lz_key = first_row[\"lz_key\"].strip().lower()\n",
    "    logger.info(f\"Extracted configs: env={env}, lz_key={lz_key}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Missing expected keys 'env' or 'lz_key' in config file: {e}\", exc_info=True)\n",
    "    raise KeyError(f\"Missing expected keys 'env' or 'lz_key' in config file: {e}\")\n",
    "\n",
    "#Construct keyvault name\n",
    "try:\n",
    "    keyvault_name = f\"ingest{lz_key}-meta002-{env}\"\n",
    "    logger.info(f\"Constructed keyvault name: {keyvault_name}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error constructing keyvault name: {e}\", exc_info=True)\n",
    "    raise ValueError(f\"Error constructing keyvault name: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da8cbd9e-ed0b-4f69-95d0-ee1f4b90ef7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Access the Service Principal secrets from Key Vault\n",
    "try:\n",
    "    client_secret = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-SECRET')\n",
    "    logger.info(\"Successfully retrieved SERVICE-PRINCIPLE-CLIENT-SECRET from Key Vault\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not retrieve 'SERVICE-PRINCIPLE-CLIENT-SECRET' from Key Vault '{keyvault_name}': {e}\", exc_info=True)\n",
    "    raise KeyError(f\"Could not retrieve 'SERVICE-PRINCIPLE-CLIENT-SECRET' from Key Vault '{keyvault_name}': {e}\")\n",
    "\n",
    "try:\n",
    "    tenant_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-TENANT-ID')\n",
    "    logger.info(\"Successfully retrieved SERVICE-PRINCIPLE-TENANT-ID from Key Vault\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not retrieve 'SERVICE-PRINCIPLE-TENANT-ID' from Key Vault '{keyvault_name}': {e}\", exc_info=True)\n",
    "    raise KeyError(f\"Could not retrieve 'SERVICE-PRINCIPLE-TENANT-ID' from Key Vault '{keyvault_name}': {e}\")\n",
    "\n",
    "try:\n",
    "    client_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-ID')\n",
    "    logger.info(\"Successfully retrieved SERVICE-PRINCIPLE-CLIENT-ID from Key Vault\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not retrieve 'SERVICE-PRINCIPLE-CLIENT-ID' from Key Vault '{keyvault_name}': {e}\", exc_info=True)\n",
    "    raise KeyError(f\"Could not retrieve 'SERVICE-PRINCIPLE-CLIENT-ID' from Key Vault '{keyvault_name}': {e}\")\n",
    "\n",
    "logger.info(\"✅ Successfully retrieved all Service Principal secrets from Key Vault\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95406a5a-69ef-4e25-bc8c-49a5b8e6a19d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Parameterise containers ---\n",
    "curated_storage_account = f\"ingest{lz_key}curated{env}\"\n",
    "curated_container = \"gold\"\n",
    "silver_curated_container = \"silver\"\n",
    "checkpoint_storage_account = f\"ingest{lz_key}xcutting{env}\"\n",
    "\n",
    "# --- Assign OAuth to storage accounts ---\n",
    "storage_accounts = [curated_storage_account, checkpoint_storage_account]\n",
    "\n",
    "for storage_account in storage_accounts:\n",
    "    try:\n",
    "        configs = {\n",
    "            f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\": \"OAuth\",\n",
    "            f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\":\n",
    "                \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "            f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\": client_id,\n",
    "            f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\": client_secret,\n",
    "            f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\":\n",
    "                f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "        }\n",
    "\n",
    "        for key, val in configs.items():\n",
    "            try:\n",
    "                spark.conf.set(key, val)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to set Spark config '{key}' for storage account '{storage_account}': {e}\", exc_info=True)\n",
    "                raise RuntimeError(f\"Failed to set Spark config '{key}' for storage account '{storage_account}': {e}\")\n",
    "\n",
    "        logger.info(f\"✅ Successfully configured OAuth for storage account: {storage_account}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error configuring OAuth for storage account '{storage_account}': {e}\", exc_info=True)\n",
    "        raise RuntimeError(f\"Error configuring OAuth for storage account '{storage_account}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3b1dee5-37a1-45e9-8d52-81300c16fac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, LongType\n",
    "\n",
    "expected_schema = StructType([\n",
    "    StructField(\"operation\", StringType(), True),\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"relation_id\", StringType(), True),\n",
    "    StructField(\"a360_record_id\", StringType(), True),\n",
    "    StructField(\"process_time\", TimestampType(), True),\n",
    "    StructField(\"status\", IntegerType(), True),\n",
    "    StructField(\"input\", StringType(), True),  # Contains nested JSON as a string\n",
    "    StructField(\"exception_description\", StringType(), True),\n",
    "    StructField(\"error_status\", StringType(), True),\n",
    "    StructField(\"a360_file_id\", StringType(), True),\n",
    "    StructField(\"file_size\", LongType(), True),\n",
    "    StructField(\"s_md5\", StringType(), True),\n",
    "    StructField(\"s_sha256\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),  # may be used as process_time\n",
    "    StructField(\"filename\", StringType(), True),\n",
    "    StructField(\"submission_folder\", StringType(), True),\n",
    "    StructField(\"file_hash\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d09477b4-8ff2-4e7c-86d9-df97b53d42fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ARM_segment = \"SBDEV\" if env == \"sbox\" else \"SB\"\n",
    "ARIA_segment = \"SBAILS\"\n",
    "segment_short = \"sbl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e0e8c05-c5b7-4cfb-a743-6cb05c512fe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### Set up Auto Loader job\n",
    "from pyspark.sql.functions import input_file_name,regexp_extract,col,expr\n",
    "\n",
    "sas_token = dbutils.secrets.get(scope=f\"ingest{lz_key}-meta002-{env}\", key=f\"ARIA{ARM_segment}-SAS-TOKEN\")\n",
    "storage_account_name = \"a360c2x2555dz\"\n",
    "container_name = \"dropzone\"\n",
    "sub_dir = f\"ARIA{ARM_segment}/response\"\n",
    "\n",
    "input_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{sub_dir}\"\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.{container_name}.{storage_account_name}.blob.core.windows.net\",\n",
    "    sas_token\n",
    ")\n",
    "\n",
    "schema_location = f\"/mnt/autoLoaderSchema/ARM{ARIA_segment}/response/read_stream\"\n",
    "\n",
    "# Define a file regex that matches files ending with .rsp\n",
    "file_regex = \".*\\\\.rsp$\"\n",
    "\n",
    "output_container_name = \"silver\" \n",
    "\n",
    "output_storage_account_name = f\"ingest{lz_key}curated{env}\"\n",
    "\n",
    "output_subdir_amalgamated_responses = f\"ARIADM/ARM/response/{ARIA_segment}/amalgamated_responses\"\n",
    "amalgamated_responses_path = f\"abfss://{output_container_name}@{output_storage_account_name}.dfs.core.windows.net/{output_subdir_amalgamated_responses}\"\n",
    "\n",
    "output_subdir_input_upload = f\"ARIADM/ARM/response/{ARIA_segment}/input_upload\"\n",
    "input_upload_responses_path = f\"abfss://{output_container_name}@{output_storage_account_name}.dfs.core.windows.net/{output_subdir_input_upload}\"\n",
    "\n",
    "\n",
    "output_subdir_create_record_upload = f\"ARIADM/ARM/response/{ARIA_segment}/create_record\"\n",
    "create_record_responses_path = f\"abfss://{output_container_name}@{output_storage_account_name}.dfs.core.windows.net/{output_subdir_create_record_upload}\"\n",
    "\n",
    "\n",
    "output_subdir_upload_file_upload = f\"ARIADM/ARM/response/{ARIA_segment}/upload_file\"\n",
    "upload_file_responses_path = f\"abfss://{output_container_name}@{output_storage_account_name}.dfs.core.windows.net/{output_subdir_upload_file_upload}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8f7de5f-cdd8-47ba-a7f5-d406acfb6015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "check_point_path = f\"abfss://db-rsp-checkpoint@ingest{lz_key}xcutting{env}.dfs.core.windows.net/ARM{ARIA_segment}/RSP/\"\n",
    "schema_location = f\"abfss://db-rsp-checkpoint@ingest{lz_key}xcutting{env}.dfs.core.windows.net/ARM{ARIA_segment}/RSP/schema\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9dcb690-20e0-4c4b-ad64-6a70096a0e91",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reconcilliation count"
    }
   },
   "outputs": [],
   "source": [
    "## expected counts\n",
    "silver_path = f\"abfss://silver@ingest{lz_key}curated{env}.dfs.core.windows.net/ARIADM/ARM/AUDIT/{ARIA_segment}/{segment_short}_ack_audit_table\"\n",
    "\n",
    "filtered_df = spark.read.format(\"delta\").load(silver_path)\\\n",
    "    .filter(col(\"http_response\")==201).select(col(\"filename\")).distinct()\n",
    "\n",
    "html_count = filtered_df.filter(col(\"filename\").contains(\"html\")).count()\n",
    "json_count = filtered_df.filter(col(\"filename\").contains(\"json\")).count()\n",
    "a360_count = filtered_df.filter(col(\"filename\").contains(\"a360\")).count()\n",
    "\n",
    "expected_created_records = html_count\n",
    "\n",
    "expected_input_upload = a360_count\n",
    "expected_upload_file = html_count + json_count\n",
    "\n",
    "logger.info(f\"\"\"\n",
    "expected_created_records = {expected_created_records}\n",
    "expected_input_upload = {expected_input_upload}\n",
    "expected_upload_file = {expected_upload_file}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -5424487702560051,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd0c6fef-980f-498f-9318-75465f7e721b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Stream Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd6a514d-a2d8-4d0b-a27e-3bd3bbf9b9f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Run autoloader read stream\n",
    "\n",
    "df = (spark.readStream.format(\"cloudFiles\")\n",
    "      .schema(expected_schema)\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", schema_location)\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"none\")\n",
    "    .option(\"checkpointLocation\", f\"{check_point_path}/rsp_readStream\")\n",
    "    .load(input_path)\n",
    "    .select( \"*\",col(\"_metadata.file_path\").alias(\"_file_path\"),\n",
    "             col(\"_metadata.file_modification_time\").alias(\"_file_modification_time\")\n",
    "    )\n",
    "    .withColumn(\"id\", expr(\"uuid()\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bb92655-619a-44b2-b68c-e71b6541362c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_am = df.filter(col(\"operation\").isNotNull())\n",
    "df_cr = df.filter(col(\"operation\")==\"create_record\")\n",
    "df_iu = df.filter(col(\"operation\")==\"input_upload\")\n",
    "df_uf = df.filter(col(\"operation\")==\"upload_new_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "733dbec8-dcaa-44e5-bbc4-e715e16cae1b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Amalgamated"
    }
   },
   "outputs": [],
   "source": [
    "df_complete = df_am.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", f\"{check_point_path}/amalgamated\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .queryName(\"amalgamated\") \\\n",
    "    .start(amalgamated_responses_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a46df715-3a49-446d-bc69-dda3ab2a7af2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Input Upload Stream"
    }
   },
   "outputs": [],
   "source": [
    "### Input upload table\n",
    "\n",
    "df_input_upload_query = df_iu.select(\"id\",\"operation\",\"timestamp\",\"status\",\"exception_description\",\"error_status\",\"filename\",\"submission_folder\",\"file_hash\",\"_file_path\",\"_file_modification_time\"\n",
    "                ).writeStream \\\n",
    "                .format(\"delta\") \\\n",
    "                .option(\"checkpointLocation\", f\"{check_point_path}/input_upload\") \\\n",
    "                .outputMode(\"append\") \\\n",
    "                .queryName(\"input_upload\") \\\n",
    "                .start(input_upload_responses_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6849428b-017b-4cd3-9ab3-2be1449b9ad6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Record Stream"
    }
   },
   "outputs": [],
   "source": [
    "## Create record table\n",
    "df_create_record_query = df_cr.select(\"id\",\"operation\",\"transaction_id\",\"relation_id\",\"a360_record_id\",\"process_time\",\"status\",\"input\",\"exception_description\",\"error_status\",\"_file_path\",\"_file_modification_time\"\n",
    "    ).writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", f\"{check_point_path}/create_record\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"create_record\") \\\n",
    "    .start(create_record_responses_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1610c23a-edc8-4cf4-90a7-533b29150cc1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Upload File Stream"
    }
   },
   "outputs": [],
   "source": [
    "### upload file table\n",
    "df_upload_file_query = df_uf.select(\"id\",\"operation\",\"transaction_id\",\"relation_id\",\"a360_record_id\",\"process_time\",\"status\",\"input\",\"exception_description\",\"error_status\",\"a360_file_id\",\"file_size\",\"s_md5\",\"s_sha256\",\"_file_path\",\"_file_modification_time\")\\\n",
    "                .writeStream \\\n",
    "                .format(\"delta\") \\\n",
    "                .option(\"checkpointLocation\", f\"{check_point_path}/response/upload_file\") \\\n",
    "                .outputMode(\"append\") \\\n",
    "                .queryName(\"upload_file\") \\\n",
    "                .start(upload_file_responses_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f07fa1b9-842e-4160-961f-bdf467504c10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_current_counts():\n",
    "    df_amalgamated_responses = spark.read.format(\"delta\").load(amalgamated_responses_path)\n",
    "    input_upload_count = df_amalgamated_responses.filter(col(\"operation\") == \"input_upload\").count()\n",
    "    create_record_count = df_amalgamated_responses.filter(col(\"operation\") == \"create_record\").count()\n",
    "    upload_file_count = df_amalgamated_responses.filter(col(\"operation\") == \"upload_new_file\").count()\n",
    "    return input_upload_count, create_record_count, upload_file_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e557ff63-c8b3-41e0-b049-6b7462f11406",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# wait 60 seconds before the reconcilliaiton checks\n",
    "max_attempt = 5\n",
    "delay = 30 # seconds\n",
    "\n",
    "for attempt in range(1,max_attempt):\n",
    "    try:\n",
    "        input_upload_count, create_record_count, upload_file_count = get_current_counts()\n",
    "    except Exception as e:\n",
    "        if attempt < max_attempt:\n",
    "            print(f\"Attempt {attempt} failed: {e}. Retrying in {delay} seconds... \")\n",
    "            time.sleep(60)\n",
    "        else:\n",
    "            print(\"Failed to get processed counts after {max_attempt} attempts: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89eab5b8-0f44-458b-8121-b425a8ee3271",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for q in spark.streams.active:\n",
    "    logger.info(q.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c22596ae-c193-4f12-a316-b13bdf1c52f7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reconcilliation"
    }
   },
   "outputs": [],
   "source": [
    "while any(q.isActive for q in [df_input_upload_query,df_create_record_query,df_upload_file_query]):\n",
    "    input_upload_count, create_record_count, upload_file_count = get_current_counts()\n",
    "    logger.info(f\"\"\"Current counts:\\n\n",
    "                    input_upload_count={input_upload_count}/{expected_input_upload}\\n\n",
    "                    create_record_count={create_record_count}/{expected_created_records}\\n\n",
    "                    upload_file_count={upload_file_count}/{expected_upload_file}\"\"\")\n",
    "    if input_upload_count >= expected_input_upload:\n",
    "        df_input_upload_query.stop()\n",
    "        logger.info(f\"Stopping input_upload stream\")\n",
    "    if create_record_count is not None and create_record_count >= expected_created_records:\n",
    "        df_create_record_query.stop()\n",
    "        logger.info(f\"Stopping create_record stream\")\n",
    "    if upload_file_count >= expected_upload_file:\n",
    "        df_upload_file_query.stop()\n",
    "        logger.info(f\"Stopping upload_file stream\")\n",
    "    time.sleep(5)\n",
    "\n",
    "if any([q.isActive for q in spark.streams.active]):\n",
    "    df_complete.stop()\n",
    "    logger.info(\"All streams have been stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6c4d74f-8e1d-4f12-b753-fc9464cdd5b4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read in Ack, Pub, Create Record Tables"
    }
   },
   "outputs": [],
   "source": [
    "silver_container = \"silver\"\n",
    "df_sbails_ack_audit_db_eh_audit_data = spark.read.format(\"delta\").load(f\"abfss://{silver_container}@{curated_storage_account}.dfs.core.windows.net/ARIADM/ARM/AUDIT/{ARIA_segment}/{segment_short}_ack_audit_table/\")\n",
    "df_sbails_ack_audit_db_eh_audit_data.createOrReplaceTempView(f\"{segment_short}_ack_audit_db_eh_data\")\n",
    "\n",
    "df_sbails_pub_audit_db_eh_audit_data = spark.read.format(\"delta\").load(f\"abfss://{silver_container}@{curated_storage_account}.dfs.core.windows.net/ARIADM/ARM/AUDIT/{ARIA_segment}/{segment_short}_pub_audit_table/\")\n",
    "df_sbails_pub_audit_db_eh_audit_data.createOrReplaceTempView(f\"{segment_short}_pub_audit_db_eh_data\")\n",
    "\n",
    "df_sbails_cr_audit_db_eh_audit_data = spark.read.format(\"delta\").load(f\"abfss://{silver_container}@{curated_storage_account}.dfs.core.windows.net/ARIADM/ARM/AUDIT/{ARIA_segment}/{segment_short}_cr_audit_table/\")\n",
    "df_sbails_cr_audit_db_eh_audit_data.createOrReplaceTempView(f\"{segment_short}_cr_audit_db_eh_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43e2f880-6a3a-46ea-bf17-abb00d5cd784",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Amalgamated Table"
    }
   },
   "outputs": [],
   "source": [
    "df_amalgamated_output = spark.read.format(\"delta\").load(amalgamated_responses_path)\n",
    "df_amalgamated_output.createOrReplaceTempView(\"sbails_amalgamated_response_data\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "        SELECT \n",
    "        *\n",
    "FROM sbails_amalgamated_response_data\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94c25e84-851c-4b95-bb18-25ff1f414155",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Input Upload Table"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "df_input_upload = spark.read.format(\"delta\").load(input_upload_responses_path)\ndf_input_upload.createOrReplaceTempView(\"sbl_input_upload\")\n\nspark.sql(f\"\"\"\nSELECT  DATE_FORMAT(timestamp, 'ddMMMyyyy') as date,\n        operation,\n        {expected_input_upload} as expected_input_upload,\n        COUNT(CASE WHEN status = 1 THEN 1 END) as count_successful_input_upload,\n        COUNT(CASE WHEN status != 1 THEN 1 END) as count_unsuccessful_input_upload,\n        concat(((count_successful_input_upload/expected_input_upload) * 100), \"%\") as `%_of_successful_input_upload`\n\nFROM sbl_input_upload\nGROUP BY date, operation\nORDER BY date DESC\n\"\"\").display()",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "DETAILS"
         },
         {
          "key": "options",
          "value": {
           "columns": [
            {
             "name": "date",
             "title": "date",
             "type": "string"
            },
            {
             "name": "operation",
             "title": "operation",
             "type": "string"
            },
            {
             "name": "expected_input_upload",
             "title": "expected_input_upload",
             "type": "integer"
            },
            {
             "name": "count_successful_input_upload",
             "title": "count_successful_input_upload",
             "type": "integer"
            },
            {
             "name": "count_unsuccessful_input_upload",
             "title": "count_unsuccessful_input_upload",
             "type": "integer"
            },
            {
             "name": "%_of_successful_input_upload",
             "title": "%_of_successful_input_upload",
             "type": "string"
            }
           ],
           "version": 1
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "66aec723-c658-4a8f-9efb-7a04d3431435",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 23.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {},
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_input_upload = spark.read.format(\"delta\").load(input_upload_responses_path)\n",
    "df_input_upload.createOrReplaceTempView(\"sbl_input_upload\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "SELECT  DATE_FORMAT(timestamp, 'ddMMMyyyy') as date,\n",
    "        operation,\n",
    "        {expected_input_upload} as expected_input_upload,\n",
    "        COUNT(CASE WHEN status = 1 THEN 1 END) as count_successful_input_upload,\n",
    "        COUNT(CASE WHEN status != 1 THEN 1 END) as count_unsuccessful_input_upload,\n",
    "        concat(((count_successful_input_upload/expected_input_upload) * 100), \"%\") as `%_of_successful_input_upload`\n",
    "\n",
    "FROM sbl_input_upload\n",
    "GROUP BY date, operation\n",
    "ORDER BY date DESC\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "983eb680-904e-411b-9706-681f8b45930b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ack, Pub, Input Upload Reconcilliation (A360)"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "with sbl_ack_table as (\n",
    "    SELECT filename,\n",
    "    http_response,\n",
    "        case when http_response = '201' then 'success' else 'failure' end as acknowledge_status,\n",
    "        timestamp,\n",
    "        split_part(filename,\".\",1) as a360_filename FROM sbl_ack_audit_db_eh_data\n",
    "        where http_message = 'Created'\n",
    "    ), bails_pub_table as (\n",
    "    SELECT file_name,\n",
    "        status as publish_status,\n",
    "        error_message,\n",
    "        timestamp,\n",
    "        split_part(file_name,\".\",1) as a360_filename FROM sbl_pub_audit_db_eh_data\n",
    "    ) \n",
    "    SELECT t1.filename,\n",
    "    t3.publish_status,\n",
    "    t3.error_message as publish_error_message,\n",
    "    t3.timestamp as publish_timestamp, \n",
    "    t1.acknowledge_status,\n",
    "    t1.http_response as acknowledge_http_response, \n",
    "    t1.timestamp as acknowledge_timestamp,\n",
    "    t2.status as input_upload_status,\n",
    "    t2.exception_description as input_upload_exception_description,\n",
    "    t2.error_status as input_upload_error_message,\n",
    "    t2.timestamp as input_upload_timestamp\n",
    "\n",
    "FROM sbl_ack_table t1 left join sbl_input_upload t2 on t1.a360_filename = t2.filename\n",
    "left join bails_pub_table t3 on t3.a360_filename = t2.filename\n",
    "where t1.filename LIKE '%.a360'\n",
    "ORDER BY filename ASC\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70c9bedd-e5a8-4ad8-8bd2-9b47c637c3d9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Upload File Table"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "df_upload_file = spark.read.format(\"delta\").load(upload_file_responses_path)\ndf_upload_file.createOrReplaceTempView(f\"{segment_short}_upload_file\")\n\nspark.sql(f\"\"\"\nSELECT  DATE_FORMAT(process_time, 'ddMMMyyyy') as date,\n        operation,\n        {expected_upload_file} as expected_upload_file,\n        COUNT(CASE WHEN status = 1 THEN 1 END) as count_successful_upload_file,\n        COUNT(CASE WHEN status != 1 THEN 1 END) as count_unsuccessful_upload_file,\n        concat(((count_successful_upload_file/expected_upload_file) * 100), \"%\") as `%_of_successful_upload_file`\n\n\nFROM sbl_upload_file\nGROUP BY date, operation\nORDER BY date DESC\n\"\"\").display()",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "DETAILS"
         },
         {
          "key": "options",
          "value": {
           "columns": [
            {
             "name": "date",
             "title": "date",
             "type": "string"
            },
            {
             "name": "operation",
             "title": "operation",
             "type": "string"
            },
            {
             "name": "expected_upload_file",
             "title": "expected_upload_file",
             "type": "integer"
            },
            {
             "name": "count_successful_upload_file",
             "title": "count_successful_upload_file",
             "type": "integer"
            },
            {
             "name": "count_unsuccessful_upload_file",
             "title": "count_unsuccessful_upload_file",
             "type": "integer"
            },
            {
             "name": "%_of_successful_upload_file",
             "title": "%_of_successful_upload_file",
             "type": "string"
            }
           ],
           "version": 1
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "f935f302-285f-41be-88ef-51b64d66f63f",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 23.875,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {},
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_upload_file = spark.read.format(\"delta\").load(upload_file_responses_path)\n",
    "df_upload_file.createOrReplaceTempView(f\"{segment_short}_upload_file\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "SELECT  DATE_FORMAT(process_time, 'ddMMMyyyy') as date,\n",
    "        operation,\n",
    "        {expected_upload_file} as expected_upload_file,\n",
    "        COUNT(CASE WHEN status = 1 THEN 1 END) as count_successful_upload_file,\n",
    "        COUNT(CASE WHEN status != 1 THEN 1 END) as count_unsuccessful_upload_file,\n",
    "        concat(((count_successful_upload_file/expected_upload_file) * 100), \"%\") as `%_of_successful_upload_file`\n",
    "\n",
    "\n",
    "FROM sbl_upload_file\n",
    "GROUP BY date, operation\n",
    "ORDER BY date DESC\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52952625-7e0c-4f33-b220-92b7860e3f68",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764779976464}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Ack, Pub, Upload File Reconcilliation (HTML, JSON)"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "with sbl_ack_table as (\n",
    "    SELECT filename,\n",
    "    http_response,\n",
    "        case when http_response = '201' then 'success' else 'failure' end as acknowledge_status,\n",
    "        timestamp,\n",
    "        regexp_replace(regexp_replace(split_part(filename,\".\",1), \"sbails_\", \"\"), \"_\", \"/\") as ack_filename FROM sbl_ack_audit_db_eh_data\n",
    "        WHERE http_message = 'Created'\n",
    "    ), sbl_pub_table as (\n",
    "    SELECT file_name,\n",
    "        status as publish_status,\n",
    "        error_message,\n",
    "        timestamp,\n",
    "        regexp_replace(regexp_replace(split_part(file_name,\".\",1), \"sbails_\", \"\"), \"_\", \"/\") as pub_filename FROM sbl_pub_audit_db_eh_data\n",
    "    ), sbl_upload_file_new as (\n",
    "SELECT regexp_replace(relation_id, \"     \", \"\") as new_relation_id,\n",
    "      CASE WHEN status = 1 then 'success' else 'failure' END as upload_file_status,\n",
    "      process_time,\n",
    "      error_status,\n",
    "      exception_description\n",
    "      FROM sbl_upload_file\n",
    "      \n",
    "      ), deduped as (\n",
    "              \n",
    "              SELECT t1.file_name,\n",
    "                t1.publish_status,\n",
    "                t1.error_message as publish_error_message,\n",
    "                t1.timestamp as publish_timestamp,\n",
    "                t3.acknowledge_status,\n",
    "                t3.http_response as acknowledge_http_response,\n",
    "                t3.timestamp as acknowledge_timestamp,\n",
    "                t2.upload_file_status,\n",
    "                t2.error_status as input_upload_error_message,\n",
    "                t2.exception_description as input_upload_exception_description,\n",
    "                t2.process_time as input_upload_timestamp,\n",
    "                ROW_NUMBER() OVER(PARTITION BY t1.file_name ORDER BY t2.process_time DESC) as rn \n",
    "      \n",
    "\n",
    " FROM sbl_pub_table t1 \n",
    "inner join sbl_upload_file_new t2\n",
    "on t1.pub_filename = t2.new_relation_id\n",
    "inner join sbl_ack_table t3 on t3.filename = t1.file_name\n",
    "where t1.file_name LIKE \"%.html\" or t1.file_name LIKE \"%.json\"\n",
    "      )\n",
    "\n",
    "      SELECT * FROM deduped where deduped.rn = 1\n",
    "      ORDER BY deduped.file_name ASC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb8f06a0-5ec2-4a21-8ad3-dc2471827cbd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Record Table"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "df_create_record = spark.read.format(\"delta\").load(create_record_responses_path)\ndf_create_record.createOrReplaceTempView(\"sbl_create_record\")\n\nspark.sql(f\"\"\"\nSELECT \n    DATE_FORMAT(process_time, 'ddMMMyyyy') as date,\n    operation,\n    {expected_created_records} as expected_created_records,\n    COUNT(CASE WHEN status = 1 THEN 1 END) as count_of_successful_created_records,\n    COUNT(CASE WHEN status != 1 THEN 1 END) as count_of_unsuccessful_created_records,\n    CONCAT(ROUND((COUNT(CASE WHEN status = 1 THEN 1 END) * 100.0 / {expected_created_records}), 2), '%') as `%_of_successful_created_records`\nFROM sbl_create_record\nGROUP BY date, operation\nORDER BY date DESC\n\"\"\").display()",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "DETAILS"
         },
         {
          "key": "options",
          "value": {
           "columns": [
            {
             "name": "date",
             "title": "date",
             "type": "string"
            },
            {
             "name": "operation",
             "title": "operation",
             "type": "string"
            },
            {
             "name": "expected_created_records",
             "title": "expected_created_records",
             "type": "integer"
            },
            {
             "name": "count_of_successful_created_records",
             "title": "count_of_successful_created_records",
             "type": "integer"
            },
            {
             "name": "count_of_unsuccessful_created_records",
             "title": "count_of_unsuccessful_created_records",
             "type": "integer"
            },
            {
             "name": "%_of_successful_created_records",
             "title": "%_of_successful_created_records",
             "type": "string"
            }
           ],
           "version": 1
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "b1a1a227-dcb9-4fa7-b763-912c31c4427d",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 25.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {},
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_create_record = spark.read.format(\"delta\").load(create_record_responses_path)\n",
    "df_create_record.createOrReplaceTempView(\"sbl_create_record\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    DATE_FORMAT(process_time, 'ddMMMyyyy') as date,\n",
    "    operation,\n",
    "    {expected_created_records} as expected_created_records,\n",
    "    COUNT(CASE WHEN status = 1 THEN 1 END) as count_of_successful_created_records,\n",
    "    COUNT(CASE WHEN status != 1 THEN 1 END) as count_of_unsuccessful_created_records,\n",
    "    CONCAT(ROUND((COUNT(CASE WHEN status = 1 THEN 1 END) * 100.0 / {expected_created_records}), 2), '%') as `%_of_successful_created_records`\n",
    "FROM sbl_create_record\n",
    "GROUP BY date, operation\n",
    "ORDER BY date DESC\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f177e1c1-112e-446c-99aa-4eda234e821f",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764781595110}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Create Record Reconcilliation (HTML, JSON)"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "with window as (\n",
    "SELECT *, regexp_replace(Unique_Identifier, \"     \", \"\") as new_unique_identifier,\n",
    "split_part(Unique_Identifier, \".\", 1) as unique_identifier1,\n",
    "ROW_NUMBER() OVER (PARTITION BY split_part(Unique_Identifier, \".\", 1) ORDER BY Unique_Identifier_Desc DESC) as rn,\n",
    "file_name\n",
    "\n",
    "FROM sbl_cr_audit_db_eh_data where table_name LIKE '%json%' or table_name LIKE '%html%'),\n",
    "\n",
    "cr_audit as (\n",
    "        SELECT * FROM window where window.rn = 1\n",
    "\n",
    "    ), sbl_create_record_new as (\n",
    "\n",
    "    select *, concat(\"sbails_\", regexp_replace(regexp_replace(relation_id, \"     \", \"\"), \"/\", \"_\")) as new_relation_id,\n",
    "            case when status = 1 then 'success' else 'failure' end as create_record_status_arm\n",
    "           FROM sbl_create_record\n",
    "\n",
    ")\n",
    "SELECT unique_identifier1 as `Unique Identifier`,\n",
    "        t1.Run_DateTime as created_timestamp,\n",
    "        case when t1.Record_Count = 1 then 'success' else 'failure' end as successful_created_file_gold_layer,\n",
    "        t2.create_record_status_arm,\n",
    "        t2.error_status as create_record_error_message,\n",
    "        t2.process_time as create_record_timestamp\n",
    " FROM cr_audit t1 \n",
    " LEFT JOIN sbl_create_record_new t2 \n",
    "   ON t1.unique_identifier1 = t2.new_relation_id\n",
    " ORDER BY 1 ASC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faa75cab-e72d-4661-88d0-049bb991378a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_input_upload = spark.read.format(\"delta\").load(input_upload_responses_path)\n",
    " \n",
    "# # Read the response data for create_record_upload\n",
    "df_create_record_upload = spark.read.format(\"delta\").load(create_record_responses_path)\n",
    " \n",
    "# # Read the response data for upload_file\n",
    "df_upload_file_upload = spark.read.format(\"delta\").load(upload_file_responses_path) \n",
    " \n",
    "# Read the response data for df_amalgamated_responses\n",
    "df_amalgamated_responses = spark.read.format(\"delta\").load(amalgamated_responses_path)\n",
    "\n",
    "logger.info(f\"Records in df_input_upload: {df_input_upload.count()}\")\n",
    "logger.info(f\"Records in df_create_record_upload: {df_create_record_upload.count()}\")\n",
    "logger.info(f\"Records in df_upload_file_upload: {df_upload_file_upload.count()}\")\n",
    "logger.info(f\"Records in df_amalgamated_responses: {df_amalgamated_responses.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a00b3160-6cfd-4185-b90d-60abfe33790e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " dbutils.notebook.exit(\"Notebook completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [
    {
     "elements": [
      {
       "dashboardResultIndex": 0,
       "elementNUID": "43e2f880-6a3a-46ea-bf17-abb00d5cd784",
       "elementType": "command",
       "guid": "14809fbb-a314-49ec-9cf6-a99f106bc841",
       "options": null,
       "position": {
        "height": 5,
        "width": 24,
        "x": 0,
        "y": 18,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": 0,
       "elementNUID": "983eb680-904e-411b-9706-681f8b45930b",
       "elementType": "command",
       "guid": "1d061d85-aa76-4b80-8589-b30513e1a994",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 12,
        "y": 12,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": null,
       "elementNUID": "66aec723-c658-4a8f-9efb-7a04d3431435",
       "elementType": "command",
       "guid": "34c3332c-7ba0-435d-af6e-70bd2db63ccc",
       "options": {
        "autoScaleImg": false,
        "scale": 0,
        "showTitle": true,
        "title": "Count of Input Upload",
        "titleAlign": "center"
       },
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 12,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": 0,
       "elementNUID": "f177e1c1-112e-446c-99aa-4eda234e821f",
       "elementType": "command",
       "guid": "43e2cd06-e0e8-43b3-8439-bc04f824f365",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 12,
        "y": 6,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": null,
       "elementNUID": "b1a1a227-dcb9-4fa7-b763-912c31c4427d",
       "elementType": "command",
       "guid": "82950707-210e-4199-ba91-64dab2bfd45b",
       "options": {
        "autoScaleImg": false,
        "scale": 0,
        "showTitle": true,
        "title": "Count of Create Record",
        "titleAlign": "center"
       },
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 6,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": null,
       "elementNUID": "f935f302-285f-41be-88ef-51b64d66f63f",
       "elementType": "command",
       "guid": "9bf38a52-5f29-4127-aa76-8be4936b142d",
       "options": {
        "autoScaleImg": false,
        "scale": 0,
        "showTitle": true,
        "title": "Count of Upload New File",
        "titleAlign": "center"
       },
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 0,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": 0,
       "elementNUID": "52952625-7e0c-4f33-b220-92b7860e3f68",
       "elementType": "command",
       "guid": "c130a3ee-7f61-4655-a8bb-1bdd02819b5a",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 12,
        "y": 0,
        "z": null
       },
       "resultIndex": null
      }
     ],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "f5f057b9-6245-49c4-8ffa-5ebe0a95f232",
     "origId": 5424487702560080,
     "title": "Response Dashboard",
     "version": "DashboardViewV1",
     "width": 1600
    }
   ],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5424487702560292,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bails_resp_autoloader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
