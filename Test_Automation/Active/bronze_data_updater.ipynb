{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4f55781a-86c7-47ff-bba4-e4f343863097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Load Config and Setup Enviorment Variables\n",
    "state_under_test = \"paymentPending\"\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "config = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/configs/config.json\")\n",
    "env_name = config.first()[\"env\"].strip().lower()\n",
    "lz_key = config.first()[\"lz_key\"].strip().lower()\n",
    " \n",
    "# print(f\"env_code: {lz_key}\")  # This won't be redacted\n",
    "# print(f\"env_name: {env_name}\")  # This won't be redacted\n",
    " \n",
    "KeyVault_name = f\"ingest{lz_key}-meta002-{env_name}\"\n",
    "# print(f\"KeyVault_name: {KeyVault_name}\")\n",
    " \n",
    "# Service principal credentials\n",
    "client_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-ID\")\n",
    "client_secret = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-SECRET\")\n",
    "tenant_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-TENANT-ID\")\n",
    " \n",
    "# Storage account names\n",
    "curated_storage = f\"ingest{lz_key}curated{env_name}\"\n",
    "checkpoint_storage = f\"ingest{lz_key}xcutting{env_name}\"\n",
    "raw_storage = f\"ingest{lz_key}raw{env_name}\"\n",
    "landing_storage = f\"ingest{lz_key}landing{env_name}\"\n",
    "external_storage = f\"ingest{lz_key}external{env_name}\"\n",
    "  \n",
    "# Spark config for curated storage (Delta table)\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{curated_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{curated_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{curated_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{curated_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{curated_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{checkpoint_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{checkpoint_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{checkpoint_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{checkpoint_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{checkpoint_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{raw_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{raw_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{raw_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{raw_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{raw_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{landing_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{landing_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{landing_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{landing_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{landing_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{external_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{external_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{external_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{external_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{external_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "  \n",
    "# Setting variables for use in subsequent cells\n",
    "bronze_path = f\"abfss://bronze@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/\"\n",
    "silver_path = f\"abfss://silver@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/\"\n",
    "audit_path = f\"abfss://silver@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/AUDIT/{state_under_test}\"\n",
    "gold_path = f\"abfss://gold@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/{state_under_test}\"\n",
    " \n",
    " \n",
    "# Print all variables\n",
    "# variables = {\n",
    "#     # \"read_hive\": read_hive,\n",
    "    \n",
    "#     \"bronze_path\": bronze_path,\n",
    "#     \"silver_path\": silver_path,\n",
    "#     \"audit_path\": audit_path,\n",
    "#     \"gold_path\": gold_path,\n",
    "#     \"key_vault\": KeyVault_name,\n",
    "#     \"AppealState\": state_under_test\n",
    " \n",
    "# }\n",
    " \n",
    "# display(variables)\n",
    "\n",
    "import json\n",
    "\n",
    "#Get Latest Json Folder\n",
    "json_location = dbutils.fs.ls(f\"{gold_path}/\")[-1]\n",
    "latest_json_location = json_location.name\n",
    "dbutils.fs.ls(f\"{gold_path}/{latest_json_location}\")\n",
    "\n",
    "#Set Paths\n",
    "try: \n",
    "    json_path = f\"{gold_path}/{latest_json_location}/JSON/\"\n",
    "    # json_path = f\"{gold_path}/{latest_json_location}/INVALID_JSON/\"\n",
    "    M1_silver = f\"{silver_path}/silver_appealcase_detail\"\n",
    "    M1_bronze = f\"{bronze_path}/bronze_appealcase_crep_rep_floc_cspon_cfs\"\n",
    "    M2_silver = f\"{silver_path}/silver_caseapplicant_detail\"\n",
    "    M3_silver = f\"{silver_path}/silver_status_detail\"\n",
    "    C = f\"{silver_path}/silver_appealcategory_detail\"\n",
    "    bhc = f\"{bronze_path}/bronze_hearing_centres\"\n",
    "    bat = f\"{bronze_path}/bronze_appealtype\" \n",
    "    docsr = f\"{bronze_path}/bronze_documentsreceived\"   \n",
    "    apl_audit = f\"{audit_path}/apl_active_payment_pending_cr_audit_table/\"\n",
    "    sh =  f\"{silver_path}/silver_history_detail\"\n",
    "except:\n",
    "    print(f\"Error during fetch: {str(e)}\")\n",
    "\n",
    "#Create and Load Dataframes\n",
    "json_data = spark.read.format(\"json\").load(json_path)\n",
    "M1_silver = spark.read.format(\"delta\").load(M1_silver)\n",
    "M1_bronze = spark.read.format(\"delta\").load(M1_bronze)\n",
    "M2_silver = spark.read.format(\"delta\").load(M2_silver)\n",
    "M3_silver = spark.read.format(\"delta\").load(M3_silver)\n",
    "C = spark.read.format(\"delta\").load(C)\n",
    "bhc = spark.read.format(\"delta\").load(bhc)\n",
    "bat = spark.read.format(\"delta\").load(bat)\n",
    "docsr = spark.read.format(\"delta\").load(docsr)\n",
    "apl_audit = spark.read.format(\"delta\").load(apl_audit)\n",
    "sh_audit = spark.read.format(\"delta\").load(sh)\n",
    "\n",
    "#Can be removed later, added to allow developing of code in this notebook to begin with before moving to func files\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, lit, array, struct, collect_list, \n",
    "    max as spark_max, date_format, row_number, expr, \n",
    "    size, udf, coalesce, concat_ws, concat, trim, year, split, datediff,\n",
    "    collect_set, current_timestamp,transform, first, array_contains\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29a33535-9aae-430e-b410-f3ce10df1123",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"BirthDate\":316},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770218175778}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "UPDATE BRONZE DATA SCRIPT"
    }
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "#UPDATE BRONZE DATA SCRIPT FOR PAYMENT PENDING.\n",
    "#\n",
    "#NOTE: The below code will update bronze data that will not pass the DQ expecation checks due to\n",
    "#issues in the data that will be resolved before live but are needeed to get all the data through the checks\n",
    "#and sent to CCD in the mean time\n",
    "###############################\n",
    "from pyspark.sql.functions import *\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "######################\n",
    "#TO FIX PP DATA FROM FIRST STAGING DATA CUT (now superseeded due to new data cut)\n",
    "######################\n",
    "# BirthDate / appellantDateOfBirth\n",
    "\n",
    "# bronze_table = DeltaTable.forName(spark,\"ariadm_active_appeals.bronze_appealcase_caseappellant_appellant\")\n",
    "\n",
    "# display(bronze_table.toDF().filter(col(\"CaseNo\").isin(\"HU/00278/2025\", \"HU/00455/2025\", \"HU/00472/2025\" )).select(\"CaseNo\", \"BirthDate\"))\n",
    "\n",
    "# bronze_table.update(\n",
    "#     condition=col(\"CaseNo\").isin(\"HU/00278/2025\", \"HU/00455/2025\", \"HU/00472/2025\"),\n",
    "#     set={\"BirthDate\": lit(\"2000-02-01T00:00:00Z\")}\n",
    "# )\n",
    "\n",
    "# display(bronze_table.toDF().filter(col(\"CaseNo\").isin(\"HU/00278/2025\", \"HU/00455/2025\", \"HU/00472/2025\" )).select(\"CaseNo\", \"BirthDate\"))\n",
    "\n",
    "\n",
    "# #################\n",
    "# #valid_appellantNationalitiesDescription_not_null\n",
    "# #and\n",
    "# #valid_appellantNationalities_not_null\n",
    "# #NationalityId\n",
    "# #Where No mapping required for 201/203\n",
    "\n",
    "# bronze_table = DeltaTable.forName(spark, \"ariadm_active_appeals.bronze_appealcase_crep_rep_floc_cspon_cfs\")\n",
    "\n",
    "# display(bronze_table.toDF().filter(col(\"CaseNo\").isin(\"HU/00302/2025\", \"HU/00569/2025\", \"HU/00586/2025\",\"HU/00560/2025\" )).select(\"CaseNo\", \"NationalityId\"))\n",
    "\n",
    "# bronze_table.update(\n",
    "#     condition=col(\"CaseNo\").isin(\"HU/00302/2025\", \"HU/00569/2025\", \"HU/00586/2025\",\"HU/00560/2025\"),\n",
    "#     set={\"NationalityId\": lit(\"41\")}\n",
    "# )\n",
    "\n",
    "# display(bronze_table.toDF().filter(col(\"CaseNo\").isin(\"HU/00302/2025\", \"HU/00569/2025\", \"HU/00586/2025\",\"HU/00560/2025\" )).select(\"CaseNo\", \"NationalityId\"))\n",
    "\n",
    "# #################\n",
    "# #valid_oocAddressLine1 valid_oocAddressLine2\n",
    "# #changing null values to actual values\n",
    "\n",
    "# bronze_table = DeltaTable.forName(spark, \"ariadm_active_appeals.bronze_appealcase_crep_rep_floc_cspon_cfs\")\n",
    "# display(bronze_table.toDF().filter(col(\"CaseNo\").isin(\"HU/00185/2025\", \"HU/02151/2024\")).select(\"CaseNo\", \"CaseRep_Address1\", \"CaseRep_Address2\", \"CaseRep_Address3\", \"CaseRep_Address4\" , \"CaseRep_Address5\", \"CaseRep_Postcode\"))\n",
    "\n",
    "# bronze_table.update(\n",
    "#     condition=col(\"CaseNo\").isin(\"HU/00185/2025\", \"HU/02151/2024\"),\n",
    "#     set={\"CaseRep_Address1\": lit(\"925 Lisa Plains Apt. 642X\"),\n",
    "#          \"CaseRep_Address2\" : lit(\"Hill SquareX\"),\n",
    "#         \"CaseRep_Address3\" : lit(\"LynchhavenX\"),\n",
    "#         \"CaseRep_Address4\" : lit(\"AustraliaX\"),\n",
    "#         \"CaseRep_Address5\" : lit(\"NLX\"),\n",
    "#         # \"CaseRep_Postcode\" : lit(\"Hill SquareX\"),\n",
    "         \n",
    "#          }\n",
    "# )\n",
    "\n",
    "\n",
    "# display(bronze_table.toDF().filter(col(\"CaseNo\").isin(\"HU/00185/2025\", \"HU/02151/2024\")).select(\"CaseNo\", \"CaseRep_Address1\", \"CaseRep_Address2\", \"CaseRep_Address3\", \"CaseRep_Address4\" , \"CaseRep_Address5\", \"CaseRep_Postcode\"))\n",
    "\n",
    "\n",
    "# #################\n",
    "# #valid_oocAddressLine1 valid_oocAddressLine2\n",
    "# #change in ooc4 ReunionX to GuamX\n",
    "\n",
    "# bronze_table = DeltaTable.forName(spark, \"ariadm_active_appeals.bronze_appealcase_crep_rep_floc_cspon_cfs\")\n",
    "# display(bronze_table.toDF().filter(col(\"CaseNo\").isin(\"HU/02191/2024\", \"HU/01475/2024\")).select(\"CaseNo\", \"CaseRep_Address1\", \"CaseRep_Address2\", \"CaseRep_Address3\", \"CaseRep_Address4\" , \"CaseRep_Address5\", \"CaseRep_Postcode\"))\n",
    "\n",
    "# bronze_table.update(\n",
    "#     condition=col(\"CaseNo\").isin(\"HU/01475/2024\", \"HU/02191/2024\"),\n",
    "#     set={\n",
    "#         \"CaseRep_Address4\" : lit(\"AustraliaX\")                 \n",
    "#          }\n",
    "# )\n",
    "\n",
    "\n",
    "# display(bronze_table.toDF().filter(col(\"CaseNo\").isin(\"HU/02191/2024\", \"HU/01475/2024\")).select(\"CaseNo\", \"CaseRep_Address1\", \"CaseRep_Address2\", \"CaseRep_Address3\", \"CaseRep_Address4\" , \"CaseRep_Address5\", \"CaseRep_Postcode\"))\n",
    "\n",
    "\n",
    "\n",
    "# ################################\n",
    "\n",
    "# cases_to_update = ['EA/02806/2023',\n",
    "# 'HU/00575/2025',\n",
    "# 'HU/00581/2025',\n",
    "# 'HU/00447/2025',\n",
    "# 'HU/00574/2023',\n",
    "# 'HU/00591/2025',\n",
    "# 'EA/00588/2025',\n",
    "# 'EA/00551/2025',\n",
    "# 'HU/00304/2025',\n",
    "# 'EA/00495/2025',\n",
    "# 'EA/00560/2025',\n",
    "# 'EA/06826/2022',\n",
    "# 'EA/00490/2025',\n",
    "# 'EA/00554/2025',\n",
    "# 'EA/01778/2024',\n",
    "# 'HU/00562/2025',\n",
    "# 'EA/00552/2025',\n",
    "# 'HU/00511/2025',\n",
    "# 'EA/00483/2025',\n",
    "# 'EA/09676/2022',\n",
    "# 'EA/00493/2025',\n",
    "# 'HU/00224/2025',\n",
    "# 'EA/00496/2025',\n",
    "# 'EA/00538/2025',\n",
    "# 'EA/00558/2025',\n",
    "# 'EA/08372/2022',\n",
    "# 'HU/00822/2024',\n",
    "# 'HU/00574/2025',\n",
    "# 'EA/02065/2024',\n",
    "# 'HU/00442/2025',\n",
    "# 'EA/00586/2025',\n",
    "# 'HU/00590/2025',\n",
    "# 'HU/00569/2025',\n",
    "# 'EA/00557/2025',\n",
    "# 'HU/02346/2024',\n",
    "# 'EA/00562/2025',\n",
    "# 'HU/00573/2025',\n",
    "# 'HU/00571/2025',\n",
    "# 'EA/01893/2023',\n",
    "# 'EA/00584/2025',\n",
    "# 'HU/00579/2025',\n",
    "# 'HU/00555/2025',\n",
    "# 'HU/00583/2025',\n",
    "# 'EA/00591/2025',\n",
    "# 'EA/00556/2025',\n",
    "# 'EA/00497/2025',\n",
    "# 'HU/01972/2023',\n",
    "# 'EA/00437/2025',\n",
    "# 'HU/00577/2025',\n",
    "# 'EA/00585/2025',\n",
    "# 'HU/00252/2025',\n",
    "# 'HU/00557/2025',\n",
    "# 'EA/00485/2025',\n",
    "# 'HU/00563/2025',\n",
    "# 'HU/00278/2025',\n",
    "# 'EA/00559/2025',\n",
    "# 'EA/00553/2025',\n",
    "# 'HU/00578/2025',\n",
    "# 'HU/00445/2025',\n",
    "# 'HU/00325/2025',\n",
    "# 'EA/00555/2025',\n",
    "# 'HU/00572/2025',\n",
    "# 'HU/00582/2025',\n",
    "# 'EA/00587/2025',\n",
    "# 'HU/00638/2024',\n",
    "# 'HU/00453/2025',\n",
    "# \"EA/00289/2025\"\n",
    "# ]\n",
    "\n",
    "\n",
    "# bronze_table = DeltaTable.forName(spark, \"ariadm_active_appeals.bronze_appealcase_caseappellant_appellant\")\n",
    "\n",
    "# display(bronze_table.toDF().filter(col(\"CaseNo\").isin(cases_to_update)).select(\"CaseNo\", \"Appellant_Address4\"))\n",
    "\n",
    "# bronze_table.update(\n",
    "#     condition=col(\"CaseNo\").isin(cases_to_update),\n",
    "#     set={\n",
    "#         \"Appellant_Address4\" : lit(\"AustraliaX\")                 \n",
    "#          }\n",
    "# )\n",
    "\n",
    "\n",
    "# display(bronze_table.toDF().filter(col(\"CaseNo\").isin(cases_to_update)).select(\"CaseNo\", \"Appellant_Address4\"))\n",
    "\n",
    "##############################\n",
    "#PP Data After new data cut 13/2/2026\n",
    "##############################\n",
    "\n",
    "#GreeceX\n",
    "\n",
    "# \"EA/06826/2022\", \"EA/08372/2022\", \"EA/09676/2022\", \"HU/02151/2024\"\n",
    "\n",
    "# bronze_table = DeltaTable.forName(spark, \"ariadm_active_appeals.bronze_appealcase_caseappellant_appellant\")\n",
    "# display(bronze_table.toDF().filter(col(\"CaseNo\").isin(\"EA/06826/2022\", \"EA/08372/2022\", \"EA/09676/2022\", \"HU/02151/2024\")).select(\"CaseNo\", \"Appellant_Address4\"))\n",
    "\n",
    "# # bronze_table.update(\n",
    "# #     condition=col(\"CaseNo\").isin(\"EA/06826/2022\", \"EA/08372/2022\", \"EA/09676/2022\", \"HU/02151/2024\"),\n",
    "# #     set={\n",
    "# #         \"Appellant_Address4\" : lit(\"AustraliaX\")                 \n",
    "# #          }\n",
    "# # )\n",
    "\n",
    "\n",
    "# display(bronze_table.toDF().filter(col(\"CaseNo\").isin(\"EA/06826/2022\", \"EA/08372/2022\", \"EA/09676/2022\", \"HU/02151/2024\")).select(\"CaseNo\", \"Appellant_Address4\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbed2565-d60c-4c5b-8d14-74de5fe932c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# HearingCentre related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df512c3b-384c-41e7-8b5e-8e5bf1d3123f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "history table needs changing from silver to bronze: hive_metastore.ariadm_active_appeals.bronze_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4b111df-b0aa-42d9-9df7-5cb102f07ff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#################\n",
    "# hearingCentre\n",
    "\n",
    "history_table = DeltaTable.forName(spark,\"ariadm_active_appeals.bronze_history\").toDF()\n",
    "\n",
    "# history_table.display()\n",
    "\n",
    "appeal_case = DeltaTable.forName(spark,\"ariadm_active_appeals.bronze_appealcase_crep_rep_floc_cspon_cfs\").toDF()\n",
    "\n",
    "# appeal_case.display()\n",
    "\n",
    "hearingCentre_table = history_table.join(\n",
    "    appeal_case,\n",
    "    appeal_case.CaseNo == history_table.CaseNo,\n",
    "    \"inner\"\n",
    ").filter(\n",
    "    (appeal_case.CentreId).isin(517,406,522,13,296,55,101,77,476,13,79,37) &\n",
    "    (history_table.HistType == 6)\n",
    ")\n",
    "\n",
    "hearingCentre_table.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "543c3642-7ec6-42ca-8dd1-9efd76d92503",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Freeze the first result in memory\n",
    "hearingCentre_table.cache()\n",
    "first_count = hearingCentre_table.count()\n",
    "print(f\"First count: {first_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b5a40c0-b780-41aa-a658-5703ec9b3af0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Req 1\n",
    "(1) If there's 1 or fewer entires in the history table and CentreId IN (77,476,13,79,37) set the Comment to be the same as the Hearing Centre description\n",
    "\n",
    "77\tArnhem House, xxxxxxxxxx, xxxxxx\n",
    "\n",
    "476\tArnhem House (Exceptions)\n",
    "\n",
    "13\tLoughborough\n",
    "\n",
    "79\tNorth Shields (Kings Court)\n",
    "\n",
    "37\tNot known at this time\n",
    "\n",
    "(this means we will exercise the assignHearingCentre logic for these cases)\n",
    "\n",
    "_Avi interpret: where in the hearingCentre table we have CaseNos that only have one HistoryId and a matching CentreId of (77,476,13,79,37), map their Comment accordingly to the specified description_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb7ba71-8494-40ea-a9ce-780cb421f49b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"First count: {first_count}\")\n",
    "\n",
    "hearingCentre_table1 = hearingCentre_table.filter(\n",
    "    (appeal_case.CentreId).isin(77, 476, 13, 79, 37)\n",
    ").select(\n",
    "    history_table.CaseNo,\n",
    "    \"HistoryId\",\n",
    "    \"Comment\",\n",
    "    \"CentreId\",\n",
    ")\n",
    "\n",
    "second_count = hearingCentre_table1.count()\n",
    "print(f\"Second count: {second_count}\")\n",
    "\n",
    "hearingCentre_table1.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa76ad90-5b7c-428e-ba98-b4fe952f4f68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# group by CaseNo\n",
    "counts_df = hearingCentre_table1.groupBy(\"CaseNo\").count()\n",
    "\n",
    "# filter to keep only rows with one CaseNo\n",
    "single_entry_cases = counts_df.filter(\"count == 1\").select(\"CaseNo\")\n",
    "\n",
    "# apply this filter to the main table via a join\n",
    "filtered_table = hearingCentre_table1.join(\n",
    "    single_entry_cases, \n",
    "    \"CaseNo\", \n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "filtered_table.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53093159-3aaf-4d5b-91cb-5deee1e339c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "final_mapped_table = filtered_table.withColumn(\n",
    "    \"Comment\",\n",
    "    F.when(filtered_table.CentreId == 77,  \"Arnhem House, xxxxxxxxxx, xxxxxx\")\n",
    "     .when(filtered_table.CentreId == 476, \"Arnhem House (Exceptions), xxxxxxxxx, xxxxxx\")\n",
    "     .when(filtered_table.CentreId == 13,  \"Loughborough, xxxxxxxxx, xxxxxx\")\n",
    "     .when(filtered_table.CentreId == 79,  \"North Shields (Kings Court), xxxxxxxxx, xxxxxx\")\n",
    "     .when(filtered_table.CentreId == 37,  \"Not known at this time, xxxxxxxxx, xxxxxx\")\n",
    "     .otherwise(filtered_table.Comment) \n",
    ")\n",
    "\n",
    "display(final_mapped_table)\n",
    "\n",
    "cases_to_update = [row[0] for row in final_mapped_table.select(\"CaseNo\").distinct().collect()]\n",
    "print(f\"Ready to update {len(cases_to_update)} cases.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93251d2a-6d18-4ca1-a45d-ab4d0678bb69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The \"Delta Merge\" approach\n",
    "delta_history = DeltaTable.forName(spark, \"ariadm_active_appeals.bronze_history\")\n",
    "\n",
    "delta_history.alias(\"target\").merge(\n",
    "    final_mapped_table.alias(\"source\"),\n",
    "    \"target.HistoryId = source.HistoryId\" # Match records by their unique ID\n",
    ").whenMatchedUpdate(set = {\n",
    "    \"Comment\": \"source.Comment\" # Update the comment in the target with the one from our mapped table\n",
    "}).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dafe7e4-24e1-4cdd-a9ec-6d82f9188a50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check to see the IDs actually exist and get seeded in the history table\n",
    "\n",
    "delta_history.toDF().filter(\n",
    "  col(\"CaseNo\").isin(cases_to_update)\n",
    ").orderBy(\"HistType\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1adb0cf7-5c3e-4b9d-8e2c-aab8cc7fbed3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Req 2\n",
    "(2) IF CentreId IN (77,476,13,79,37) AND there are >1 rows:\n",
    "\n",
    "Set to any of:\n",
    "\n",
    "Alloa Sheriff Court\n",
    "\n",
    "Belfast\n",
    "\n",
    "Belfast - Laganside\n",
    "\n",
    "Birmingham IAC (Priory Courts)\n",
    "\n",
    "Birmingham Magistrates Court (VLC)\n",
    "\n",
    "Bradford\n",
    "\n",
    "Glasgow (Eagle Building)\n",
    "\n",
    "Glasgow (Tribunals Centre)\n",
    "\n",
    "Harmondsworth\n",
    "\n",
    "Hatton Cross\n",
    "\n",
    "Hatton Cross (Fast Track)\n",
    "\n",
    "Hendon Magistrates Court (HX)\n",
    "\n",
    "Nottingham Justice Centre\n",
    "\n",
    "Tameside Magistartes Court\n",
    "\n",
    "Taylor House\n",
    "\n",
    "Taylor House (Field House)\n",
    "\n",
    "Taylor House (HX)\n",
    "\n",
    "Yarl's Wood\n",
    "\n",
    "ZZ(DNU)Birmingham IAC Sheldon Court\n",
    "\n",
    "Arnhem House \n",
    "\n",
    "Arnhem House (Exceptions)\n",
    "\n",
    "Loughborough\n",
    "\n",
    "North Shields (Kings Court)\n",
    "\n",
    "_Avi interpret: where in the hearingCentre table we have CaseNos that have more than one HistoryId and a matching CentreId of (77,476,13,79,37), map their Comments to any of the specified fields_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c29f5691-6713-4b84-9f8f-4f1e769a99c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"First count: {first_count}\")\n",
    "\n",
    "hearingCentre_table2 = hearingCentre_table.filter(\n",
    "    (appeal_case.CentreId).isin(77, 476, 13, 79, 37)\n",
    ").select(\n",
    "    history_table.CaseNo,\n",
    "    \"HistoryId\",\n",
    "    \"Comment\",\n",
    "    \"CentreId\",\n",
    "    \"HistType\"\n",
    ")\n",
    "\n",
    "second_count = hearingCentre_table2.count()\n",
    "print(f\"Second count: {second_count}\")\n",
    "\n",
    "hearingCentre_table2.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45dd4546-ca10-48e4-b3a1-ec5fc86c9ca9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# group by CaseNo\n",
    "counts_df = hearingCentre_table2.groupBy(\"CaseNo\").count()\n",
    "\n",
    "# filter to keep only rows with one CaseNo\n",
    "single_entry_cases = counts_df.filter(\"count > 1\").select(\"CaseNo\")\n",
    "\n",
    "# apply this filter to the main table via a join\n",
    "filtered_table_2 = hearingCentre_table2.join(\n",
    "    single_entry_cases, \n",
    "    \"CaseNo\", \n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "filtered_table_2.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82876e63-6f2e-4549-8d2d-617fa4373592",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# define list of courts\n",
    "courts = [\n",
    "    \"Alloa Sheriff Court\", \"Belfast\", \"Belfast - Laganside\", \n",
    "    \"Birmingham IAC (Priory Courts)\", \"Birmingham Magistrates Court (VLC)\", \n",
    "    \"Bradford\", \"Glasgow (Eagle Building)\", \"Glasgow (Tribunals Centre)\", \n",
    "    \"Harmondsworth\", \"Hatton Cross\", \"Hatton Cross (Fast Track)\", \n",
    "    \"Hendon Magistrates Court (HX)\", \"Nottingham Justice Centre\", \n",
    "    \"Tameside Magistartes Court\", \"Taylor House\", \"Taylor House (Field House)\", \n",
    "    \"Taylor House (HX)\", \"Yarl's Wood\", \"ZZ(DNU)Birmingham IAC Sheldon Court\", \n",
    "    \"Arnhem House\", \"Arnhem House (Exceptions)\", \"Loughborough\", \n",
    "    \"North Shields (Kings Court)\"\n",
    "]\n",
    "\n",
    "# make an array\n",
    "court_array = F.array([F.lit(c) for c in courts])\n",
    "\n",
    "# 3. Apply the mapping\n",
    "final_mapped_table_2 = filtered_table_2.withColumn(\n",
    "    \"Comment\",\n",
    "    F.when(\n",
    "        F.col(\"CentreId\").isin(77, 476, 13, 79, 37),\n",
    "        court_array[(F.rand(seed=5) * len(courts)).cast(\"int\")]\n",
    "    ).otherwise(F.col(\"Comment\"))\n",
    ")\n",
    "\n",
    "final_mapped_table_2.display()\n",
    "\n",
    "cases_to_update_2 = [row[0] for row in final_mapped_table_2.select(\"CaseNo\").collect()]\n",
    "print(f\"Ready to update {len(cases_to_update_2)} cases.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36c25c79-c4e9-49e1-8c38-2e195e81993a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The \"Delta Merge\" approach\n",
    "delta_history = DeltaTable.forName(spark, \"ariadm_active_appeals.bronze_history\")\n",
    "\n",
    "delta_history.alias(\"target\").merge(\n",
    "    final_mapped_table_2.alias(\"source\"),\n",
    "    \"target.HistoryId = source.HistoryId\" # Match records by their unique ID\n",
    ").whenMatchedUpdate(set = {\n",
    "    \"Comment\": \"source.Comment\" # Update the comment in the target with the one from our mapped table\n",
    "}).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f33523e-90f8-45ed-b3f2-79b91582f4ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check to see the IDs actually exist and get seeded in the history table\n",
    "\n",
    "# delta_history.toDF().filter(\n",
    "#   col(\"CaseNo\").isin(cases_to_update_2)\n",
    "# ).orderBy(\"HistType\").display()\n",
    "\n",
    "delta_history.toDF().filter(\n",
    "  (col(\"HistType\") == 6) &\n",
    "  (col(\"CaseNo\") == \"AA/03752/2006\")\n",
    ").display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff488cbb-c828-4d63-a853-a655f1d8675b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Req 3\n",
    "(3) If CentreId IN (517,406,522,13,296,55,101):\n",
    "\n",
    "For these cases we want to set the Comment at MAX(HistoryId) as 'Castle Park Storage', 'Field House','Field House (TH)', 'UT (IAC) Cardiff CJC',\n",
    "\n",
    "'UT (IAC) Hearing in Field House',\n",
    "\n",
    "'UT (IAC) Hearing in Man CJC'\n",
    "\n",
    "(same as what the CentreId is)\n",
    "\n",
    "\n",
    "\n",
    "AND THEN for previous comments we set to any of:\n",
    "\n",
    "\n",
    "\n",
    "Alloa Sheriff Court, xxxxxxxxxx\n",
    "\n",
    "Belfast, xxxxxxxxxx\n",
    "\n",
    "Belfast - Laganside\n",
    "\n",
    "Birmingham IAC (Priory Courts)\n",
    "\n",
    "Birmingham Magistrates Court (VLC)\n",
    "\n",
    "Bradford\n",
    "\n",
    "Glasgow (Eagle Building)\n",
    "\n",
    "Glasgow (Tribunals Centre)\n",
    "\n",
    "Harmondsworth\n",
    "\n",
    "Hatton Cross\n",
    "\n",
    "Hatton Cross (Fast Track)\n",
    "\n",
    "Hendon Magistrates Court (HX)\n",
    "\n",
    "Nottingham Justice Centre\n",
    "\n",
    "Tameside Magistrates Court\n",
    "\n",
    "Taylor House\n",
    "\n",
    "Taylor House (Field House)\n",
    "\n",
    "Taylor House (HX)\n",
    "\n",
    "Yarl's Wood\n",
    "\n",
    "ZZ(DNU)Birmingham IAC Sheldon Court\n",
    "\n",
    "Arnhem House\n",
    "\n",
    "Arnhem House (Exceptions)\n",
    "\n",
    "Loughborough\n",
    "\n",
    "North Shields (Kings Court)\n",
    "\n",
    "_Avi interept: where CentreId is in (517,406,522,13,296,55,101), i must seed the comment of the row with the biggest HistoryId. the mappings for what values i need to seed into the comment are in the mapping doc ref data tab. for the remaining comments for the same CaseNo where the HistoryIds are lower than the max HistoryId, i choose randomly from the big list and seed it to any one of those_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8e2925c-acdc-4c10-a701-c1366b0d5f68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"First count: {first_count}\")\n",
    "\n",
    "hearingCentre_table3 = hearingCentre_table.filter(\n",
    "    (appeal_case.CentreId).isin(517, 406, 522, 13, 296, 55, 101)\n",
    ").select(\n",
    "    history_table.CaseNo,\n",
    "    \"HistoryId\",\n",
    "    \"Comment\",\n",
    "    \"CentreId\"\n",
    ")\n",
    "\n",
    "second_count = hearingCentre_table3.count()\n",
    "print(f\"Second count: {second_count}\")\n",
    "\n",
    "hearingCentre_table3.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32736873-ee66-4b9f-a138-282c605c90d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# identify the Max HistoryId for each Case\n",
    "max_history_df = hearingCentre_table3.groupBy(\"CaseNo\").agg(F.max(\"HistoryId\").alias(\"MaxHistoryId\"))\n",
    "\n",
    "# join back to the main table\n",
    "enriched_table = hearingCentre_table3.join(\n",
    "    max_history_df, \n",
    "    \"CaseNo\", \n",
    "    \"inner\")\n",
    "\n",
    "enriched_table.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f189cfb4-83bb-4649-9284-910e9f91b58e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define the random list (for non-max rows)\n",
    "courts = [\n",
    "    \"Alloa Sheriff Court, xxxxxxxxx, xxxxxx\", \n",
    "    \"Belfast, xxxxxxxxx, xxxxxx\", \n",
    "    \"Birmingham IAC (Priory Courts), xxxxxxxxx, xxxxxx\", \n",
    "    \"Bradford, xxxxxxxxx, xxxxxx\", \n",
    "    \"Glasgow (Eagle Building), xxxxxxxxx, xxxxxx\", \n",
    "    \"Harmondsworth, xxxxxxxxx, xxxxxx\", \n",
    "    \"Hatton Cross, xxxxxxxxx, xxxxxx\", \n",
    "    \"Nottingham Justice Centre, xxxxxxxxx, xxxxxx\", \n",
    "    \"Taylor House, xxxxxxxxx, xxxxxx\", \n",
    "    \"Yarl's Wood, xxxxxxxxx, xxxxxx\"\n",
    "]\n",
    "court_array = F.array([F.lit(c) for c in courts])\n",
    "\n",
    "# apply Logic\n",
    "final_mapped_table_3 = enriched_table.withColumn(\n",
    "    \"Comment\",\n",
    "    F.when(\n",
    "        F.col(\"HistoryId\") == F.col(\"MaxHistoryId\"),\n",
    "        F.when(F.col(\"CentreId\") == 517, \"UT (IAC) Hearing in Man CJC, xxxxxxxxx, xxxxxx\")\n",
    "         .when(F.col(\"CentreId\") == 406, \"UT (IAC) Hearing in Field House, xxxxxxxxx, xxxxxx\")\n",
    "         .when(F.col(\"CentreId\") == 522, \"UT (IAC) Cardiff CJC, xxxxxxxxx, xxxxxx\")\n",
    "         .when(F.col(\"CentreId\") == 13,  \"Loughborough, xxxxxxxxx, xxxxxx\")\n",
    "         .when(F.col(\"CentreId\") == 296, \"Field House (TH), xxxxxxxxx, xxxxxx\")\n",
    "         .when(F.col(\"CentreId\") == 55,  \"Field House, xxxxxxxxx, xxxxxx\")\n",
    "         .when(F.col(\"CentreId\") == 101, \"Castle Park Storage, xxxxxxxxx, xxxxxx\")\n",
    "    ).otherwise(\n",
    "        # random assignment for older historical records\n",
    "        court_array[(F.rand(seed=5) * len(courts)).cast(\"int\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "final_mapped_table_3 = final_mapped_table_3.drop(col(\"MaxHistoryId\"))\n",
    "\n",
    "display(final_mapped_table_3.orderBy(\"CaseNo\", \"HistoryId\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2344d16-119e-4418-a373-c85ed8f23292",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# change this to whichever ID you want to check (517, 406, 522, 13, 296, 55, 101)\n",
    "target_id = 101 \n",
    "\n",
    "sample_case = final_mapped_table_3.filter(F.col(\"CentreId\") == target_id) \\\n",
    "                         .select(\"CaseNo\") \\\n",
    "                         .distinct() \\\n",
    "                         .orderBy(F.rand()) \\\n",
    "                         .limit(1) \\\n",
    "                         .collect()[0][0]\n",
    "\n",
    "print(f\"Checking results for {target_id} - Random CaseNo: {sample_case}\")\n",
    "\n",
    "# 3. Display the history for that specific case\n",
    "final_mapped_table_3.filter(F.col(\"CaseNo\") == sample_case) \\\n",
    "           .select(\"CaseNo\", \"HistoryId\", \"CentreId\", \"Comment\") \\\n",
    "           .orderBy(\"HistoryId\") \\\n",
    "           .display()\n",
    "\n",
    "cases_to_update_3 = [row[0] for row in final_mapped_table_3.select(\"CaseNo\").collect()]\n",
    "print(f\"Ready to update {len(cases_to_update_2)} cases.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "251b1b82-64ac-4102-8e5d-1655dfab3c1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The \"Delta Merge\" approach\n",
    "delta_history = DeltaTable.forName(spark, \"ariadm_active_appeals.bronze_history\")\n",
    "\n",
    "delta_history.alias(\"target\").merge(\n",
    "    final_mapped_table_3.alias(\"source\"),\n",
    "    \"target.HistoryId = source.HistoryId\" # Match records by their unique ID\n",
    ").whenMatchedUpdate(set = {\n",
    "    \"Comment\": \"source.Comment\" # Update the comment in the target with the one from our mapped table\n",
    "}).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15af3d9d-0888-4a14-9b43-40a27a2b89ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check to see the IDs actually exist and get seeded in the history table\n",
    "\n",
    "# delta_history.toDF().orderBy(\"HistType\").display()\n",
    "\n",
    "delta_history.toDF().filter(\n",
    "  (col(\"HistType\") == 6) &\n",
    "  (col(\"CaseNo\") == \"EA/06652/2020\")\n",
    ").display() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9492eb1-ece2-4e01-ac8f-4d075738ade7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delta_silver_history = DeltaTable.forName(spark, \"ariadm_active_appeals.silver_history_detail\")\n",
    "\n",
    "delta_silver_history.toDF().filter(\n",
    "  (col(\"HistType\") == 6) &\n",
    "  (col(\"Comment\").contains(\",\"))\n",
    ").display() "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_data_updater",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
