{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8821ae9-478a-4045-ae52-afc7d315405f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "from pyspark.sql.functions import col, decode, split, element_at,udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, LongType\n",
    "from pyspark.sql.functions import date_format, col, when, count, lit, concat, round as spark_round\n",
    "import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba54019e-6f9e-4731-92f6-547abaeb311c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"DatabricksWorkflow\")\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "if not logger.hasHandlers():\n",
    "    logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b80491a-6e34-43e4-8db2-9113dbc5fcb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Final Code Starts Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6866638-de86-41f7-8534-3cd93cbc35fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/configs/config.json\")\n",
    "env = config.first()[\"env\"].strip().lower()\n",
    "lz_key = config.first()[\"lz_key\"].strip().lower()\n",
    "\n",
    "keyvault_name = f\"ingest{lz_key}-meta002-{env}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23c9a44f-1ec5-4e21-a0fe-18c223f6d2e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set up Service Principal credentials in Spark\n",
    "client_secret = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-SECRET')\n",
    "tenant_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-TENANT-ID')\n",
    "client_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-ID')\n",
    " \n",
    "curated_storage_account = f\"ingest{lz_key}curated{env}\"\n",
    "xcutting_storage_account = f\"ingest{lz_key}xcutting{env}\"\n",
    "\n",
    "storage_accounts = [curated_storage_account,xcutting_storage_account]\n",
    "\n",
    "for storage_account in storage_accounts:\n",
    "    configs = {\n",
    "            f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\": \"OAuth\",\n",
    "            f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\":\n",
    "                \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "            f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\": client_id,\n",
    "            f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\": client_secret,\n",
    "            f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\":\n",
    "                f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "        }\n",
    "    for key,val in configs.items():\n",
    "        spark.conf.set(key,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3b1dee5-37a1-45e9-8d52-81300c16fac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "expected_schema = StructType([\n",
    "    StructField(\"operation\", StringType(), True),\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"relation_id\", StringType(), True),\n",
    "    StructField(\"a360_record_id\", StringType(), True),\n",
    "    StructField(\"process_time\", TimestampType(), True),\n",
    "    StructField(\"status\", IntegerType(), True),\n",
    "    StructField(\"input\", StringType(), True),  # Contains nested JSON as a string\n",
    "    StructField(\"exception_description\", StringType(), True),\n",
    "    StructField(\"error_status\", StringType(), True),\n",
    "    StructField(\"a360_file_id\", StringType(), True),\n",
    "    StructField(\"file_size\", LongType(), True),\n",
    "    StructField(\"s_md5\", StringType(), True),\n",
    "    StructField(\"s_sha256\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),  # may be used as process_time\n",
    "    StructField(\"filename\", StringType(), True),\n",
    "    StructField(\"submission_folder\", StringType(), True),\n",
    "    StructField(\"file_hash\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af7f9ea1-093e-49d4-8da4-722f656b56f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Segment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aed11341-83ba-4178-b67d-f5c2a64e8c34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ARM_segment = \"BDEV\" if env == \"sbox\" else \"B\"\n",
    "ARIA_segment = \"BAILS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e0e8c05-c5b7-4cfb-a743-6cb05c512fe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### Set up Auto Loader job\n",
    "from pyspark.sql.functions import input_file_name,regexp_extract,col,expr\n",
    "\n",
    "\n",
    "sas_token = dbutils.secrets.get(scope=f\"ingest{lz_key}-meta002-{env}\", key=f\"ARIA{ARM_segment}-SAS-TOKEN\")\n",
    "storage_account_name = \"a360c2x2555dz\"\n",
    "container_name = \"dropzone\"\n",
    "sub_dir = f\"ARIA{ARM_segment}/response\"\n",
    "\n",
    "input_path = f\"wasbs://{container_name}@{storage_account_name}.blob.core.windows.net/{sub_dir}\"\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.sas.{container_name}.{storage_account_name}.blob.core.windows.net\",\n",
    "    sas_token\n",
    ")\n",
    "\n",
    "schema_location = f\"/mnt/autoLoaderSchema/ARM{ARIA_segment}/response/read_stream\"\n",
    "\n",
    "# Define a file regex that matches files ending with .rsp\n",
    "file_regex = \".*\\\\.rsp$\"\n",
    "\n",
    "output_container_name = \"silver\" \n",
    "\n",
    "output_storage_account_name = f\"ingest{lz_key}curated{env}\"\n",
    "\n",
    "output_subdir_amalgamated_responses = f\"ARIADM/ARM/response/{ARIA_segment}/amalgamated_responses\"\n",
    "amalgamated_responses_path = f\"abfss://{output_container_name}@{output_storage_account_name}.dfs.core.windows.net/{output_subdir_amalgamated_responses}\"\n",
    "\n",
    "output_subdir_input_upload = f\"ARIADM/ARM/response/{ARIA_segment}/input_upload\"\n",
    "input_upload_responses_path = f\"abfss://{output_container_name}@{output_storage_account_name}.dfs.core.windows.net/{output_subdir_input_upload}\"\n",
    "\n",
    "\n",
    "output_subdir_create_record_upload = f\"ARIADM/ARM/response/{ARIA_segment}/create_record\"\n",
    "create_record_responses_path = f\"abfss://{output_container_name}@{output_storage_account_name}.dfs.core.windows.net/{output_subdir_create_record_upload}\"\n",
    "\n",
    "\n",
    "output_subdir_upload_file_upload = f\"ARIADM/ARM/response/{ARIA_segment}/upload_file\"\n",
    "upload_file_responses_path = f\"abfss://{output_container_name}@{output_storage_account_name}.dfs.core.windows.net/{output_subdir_upload_file_upload}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ef6c21f-5ce3-4cdb-8cd8-361cd9c27c6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "check_point_path = f\"abfss://db-rsp-checkpoint@ingest{lz_key}xcutting{env}.dfs.core.windows.net/ARM{ARIA_segment}/RSP/\"\n",
    "schema_location = f\"abfss://db-rsp-checkpoint@ingest{lz_key}xcutting{env}.dfs.core.windows.net/ARM{ARIA_segment}/RSP/schema\"\n",
    "\n",
    "# output_sas = dbutils.secrets.get(scope=keyvault_name, key=f\"CURATED-{env}-SAS-TOKEN\")\n",
    "\n",
    "# spark.conf.set(\n",
    "#     f\"fs.azure.sas.{output_container_name}.{output_storage_account_name}.blob.core.windows.net\",\n",
    "#     output_sas\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f8dcbb1-d0fe-4f5b-8095-db0e6c9390e1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reconciliation count"
    }
   },
   "outputs": [],
   "source": [
    "## expected counts\n",
    "silver_path = f\"abfss://silver@ingest{lz_key}curated{env}.dfs.core.windows.net/ARIADM/ARM/AUDIT/BAILS/bl_ack_audit_table\"\n",
    "\n",
    "filtered_df = spark.read.format(\"delta\").load(silver_path)\\\n",
    "    .filter(col(\"http_response\")==201).select(col(\"filename\")).distinct()\n",
    "\n",
    "html_count = filtered_df.filter(col(\"filename\").contains(\"html\")).count()\n",
    "json_count = filtered_df.filter(col(\"filename\").contains(\"json\")).count()\n",
    "a360_count = filtered_df.filter(col(\"filename\").contains(\"a360\")).count()\n",
    "\n",
    "expected_created_records = html_count\n",
    "\n",
    "expected_input_upload = a360_count\n",
    "expected_upload_file = html_count + json_count\n",
    "\n",
    "logger.info(f\"\"\"\n",
    "expected_created_records = {expected_created_records}\n",
    "expected_input_upload = {expected_input_upload}\n",
    "expected_upload_file = {expected_upload_file}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99d4245b-cea5-461b-aef1-5cc2128e3f0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Stream responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd6a514d-a2d8-4d0b-a27e-3bd3bbf9b9f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Run autoloader read stream\n",
    "\n",
    "df = (spark.readStream.format(\"cloudFiles\")\n",
    "      .schema(expected_schema)\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", schema_location)\n",
    "    .option(\"multiline\", \"true\")\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"none\")\n",
    "    .option(\"checkpointLocation\", f\"{check_point_path}/rsp_readStream\")\n",
    "    .load(input_path)\n",
    "    .select( \"*\",col(\"_metadata.file_path\").alias(\"_file_path\"),\n",
    "             col(\"_metadata.file_modification_time\").alias(\"_file_modification_time\")\n",
    ")\n",
    "    # .withColumn(\"file_name\", regexp_extract(input_file_name(),\"response\\/(.*)\",1))\n",
    "    .withColumn(\"id\", expr(\"uuid()\")))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2c4f62d-6f56-4e91-aec1-2fdf58fd32c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_amalgamated = df\n",
    "df_am = df.filter(col(\"operation\").isNotNull())\n",
    "df_cr = df.filter(col(\"operation\")==\"create_record\")\n",
    "df_iu = df.filter(col(\"operation\")==\"input_upload\")\n",
    "df_uf = df.filter(col(\"operation\")==\"upload_new_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb07b7bd-c278-47c1-86d4-a42c516b98ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_am.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f36b6848-3802-4b1e-879f-f0f3fcb6a6d7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Amalgamated"
    }
   },
   "outputs": [],
   "source": [
    "df_complete = df_am.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", f\"{check_point_path}/amalgamated\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .start(amalgamated_responses_path)\n",
    "\n",
    "df_amalgamated_output = spark.read.format(\"delta\").load(amalgamated_responses_path)\n",
    "df_amalgamated_output.createOrReplaceTempView(\"bails_amalgamated_response_data\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "        SELECT \n",
    "        *\n",
    "\n",
    "FROM bails_amalgamated_response_data\n",
    "\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ed68c5b-9082-4c8f-8890-d9d5425356c8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Input Upload Stream"
    }
   },
   "outputs": [],
   "source": [
    "### Input upload table\n",
    "\n",
    "df_input_upload_query = df_iu.select(\"id\",\"operation\",\"timestamp\",\"status\",\"exception_description\",\"error_status\",\"filename\",\"submission_folder\",\"file_hash\",\"_file_path\",\"_file_modification_time\"\n",
    "                ).writeStream \\\n",
    "                .format(\"delta\") \\\n",
    "                .option(\"checkpointLocation\", f\"{check_point_path}/input_upload\") \\\n",
    "                .outputMode(\"append\") \\\n",
    "                .start(input_upload_responses_path)\n",
    "\n",
    "df_input_upload = spark.read.format(\"delta\").load(input_upload_responses_path)\n",
    "df_input_upload.createOrReplaceTempView(\"bails_input_upload\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "SELECT  DATE_FORMAT(timestamp, 'ddMMMyyyy') as date,\n",
    "        operation,\n",
    "        {expected_input_upload} as expected_input_upload,\n",
    "        COUNT(CASE WHEN status = 1 THEN 1 END) as count_successful_input_upload,\n",
    "        COUNT(CASE WHEN status != 1 THEN 1 END) as count_unsuccessful_input_upload,\n",
    "        concat(((count_successful_input_upload/expected_input_upload) * 100), \"%\") as `%_of_successful_input_upload`\n",
    "\n",
    "FROM bails_input_upload\n",
    "GROUP BY date, operation\n",
    "ORDER BY date DESC\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "823637f9-83bc-455c-8949-1fe33de55447",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Record Stream"
    }
   },
   "outputs": [],
   "source": [
    "## Create record table\n",
    "df_create_record_query = df_cr.select(\"id\",\"operation\",\"transaction_id\",\"relation_id\",\"a360_record_id\",\"process_time\",\"status\",\"input\",\"exception_description\",\"error_status\",\"_file_path\",\"_file_modification_time\"\n",
    "    ).writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", f\"{check_point_path}/create_record\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start(create_record_responses_path)\n",
    "\n",
    "df_create_record = spark.read.format(\"delta\").load(create_record_responses_path)\n",
    "df_create_record.createOrReplaceTempView(\"bails_create_record\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    DATE_FORMAT(process_time, 'ddMMMyyyy') as date,\n",
    "    operation,\n",
    "    {expected_created_records} as expected_created_records,\n",
    "    COUNT(CASE WHEN status = 1 THEN 1 END) as count_of_successful_created_records,\n",
    "    COUNT(CASE WHEN status != 1 THEN 1 END) as count_of_unsuccessful_created_records,\n",
    "    CONCAT(ROUND((COUNT(CASE WHEN status = 1 THEN 1 END) * 100.0 / {expected_created_records}), 2), '%') as `%_of_successful_created_records`\n",
    "FROM bails_create_record\n",
    "GROUP BY date, operation\n",
    "ORDER BY date DESC\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8afc094d-5614-4693-8f83-63c414fdadcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_upload_file = spark.read.format(\"delta\").load(upload_file_responses_path)\n",
    "df_upload_file.createOrReplaceTempView(\"bails_upload_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f18ba547-7d0b-4661-b5ba-2900b01e3b33",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Upload File Stream"
    }
   },
   "outputs": [],
   "source": [
    "### upload file table\n",
    "df_upload_file_query = df_uf.select(\"id\",\"operation\",\"transaction_id\",\"relation_id\",\"a360_record_id\",\"process_time\",\"status\",\"input\",\"exception_description\",\"error_status\",\"a360_file_id\",\"file_size\",\"s_md5\",\"s_sha256\",\"_file_path\",\"_file_modification_time\")\\\n",
    "                .writeStream \\\n",
    "                .format(\"delta\") \\\n",
    "                .option(\"checkpointLocation\", f\"{check_point_path}/response/upload_file\") \\\n",
    "                .outputMode(\"append\") \\\n",
    "                .start(upload_file_responses_path)\n",
    "\n",
    "df_upload_file = spark.read.format(\"delta\").load(upload_file_responses_path)\n",
    "df_upload_file.createOrReplaceTempView(\"bails_upload_file\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "SELECT  DATE_FORMAT(process_time, 'ddMMMyyyy') as date,\n",
    "        operation,\n",
    "        {expected_upload_file} as expected_upload_file,\n",
    "        COUNT(CASE WHEN status = 1 THEN 1 END) as count_successful_upload_file,\n",
    "        COUNT(CASE WHEN status != 1 THEN 1 END) as count_unsuccessful_upload_file,\n",
    "        concat(((count_successful_upload_file/expected_upload_file) * 100), \"%\") as `%_of_successful_upload_file`\n",
    "\n",
    "\n",
    "FROM bails_upload_file\n",
    "GROUP BY date, operation\n",
    "ORDER BY date DESC\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d03bc731-4b22-4151-b064-6a3c184b660a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b26c1cb6-1167-457d-aee8-4b9df2a17b6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_current_counts():\n",
    "    df_amalgamated_responses = spark.read.format(\"delta\").load(amalgamated_responses_path)\n",
    "    input_upload_count = df_amalgamated_responses.filter(col(\"operation\") == \"input_upload\").count()\n",
    "    create_record_count = df_amalgamated_responses.filter(col(\"operation\") == \"create_record\").count()\n",
    "    upload_file_count = df_amalgamated_responses.filter(col(\"operation\") == \"upload_file\").count()\n",
    "    return input_upload_count, create_record_count, upload_file_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e830055-ccec-4d16-9ee8-b2112abf6c45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# wait 60 seconds before the reconcilliaiton checks\n",
    "max_attempt = 5\n",
    "delay = 60 # seconds\n",
    "\n",
    "for attempt in range(1,max_attempt):\n",
    "    try:\n",
    "        input_upload_count, create_record_count, upload_file_count = get_current_counts()\n",
    "    except Exception as e:\n",
    "        if attempt < max_attempt:\n",
    "            print(f\"Attempt {attempt} failed: {e}. Retrying in {delay} seconds... \")\n",
    "            time.sleep(60)\n",
    "        else:\n",
    "            print(\"Failed to get processed counts after {max_attempt} attempts: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4101bbfe-b4a2-4f99-b005-159693c4737e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for q in spark.streams.active:\n",
    "    logger.info(q.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ef0e0cf-5660-4ee3-9e40-7bbaffefb031",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "while any(q.isActive for q in [df_input_upload_query,df_create_record_query,df_upload_file_query]):\n",
    "    input_upload_count, create_record_count, upload_file_count = get_current_counts()\n",
    "    logger.info(f\"\"\"Current counts:\\n\n",
    "                    input_upload_count={input_upload_count}/{expected_input_upload}\\n\n",
    "                    create_record_count={create_record_count}/{expected_created_records}\\n\n",
    "                    upload_file_count={upload_file_count}/{expected_upload_file}\"\"\")\n",
    "    if input_upload_count >= expected_input_upload:\n",
    "        df_input_upload_query.stop()\n",
    "        logger.info(f\"Stopping input_upload stream\")\n",
    "    if create_record_count is not None and create_record_count >= expected_created_records:\n",
    "        df_create_record_query.stop()\n",
    "        logger.info(f\"Stopping create_record stream\")\n",
    "    if upload_file_count >= expected_upload_file:\n",
    "        df_upload_file_query.stop()\n",
    "        logger.info(f\"Stopping upload_file stream\")\n",
    "    time.sleep(5)\n",
    "\n",
    "if any([q.isActive for q in spark.streams.active]):\n",
    "    df_am.stop()\n",
    "    logger.info(\"All streams have been stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d70f802-d3ae-4743-a3ba-7906c2fcea3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# while any(q.isActive for q in spark.streams.active):\n",
    "#     input_upload_count, create_record_count, upload_file_count = get_current_counts()\n",
    "#     print(f\"Current counts: input_upload_count={input_upload_count}/{expected_input_upload} ---- create_record_count={create_record_count}/{expected_created_records} ---- upload_file_count={upload_file_count}/{expected_upload_file}\")\n",
    "#     if input_upload_count >= expected_input_upload:\n",
    "#         df_input_upload_query.stop()\n",
    "#         print(f\"Stopping input_upload stream\")\n",
    "#     if create_record_count is not None and create_record_count >= expected_created_records:\n",
    "#         df_create_record_query.stop()\n",
    "#         print(f\"Stopping create_record stream\")\n",
    "#     if upload_file_count >= expected_upload_file:\n",
    "#         df_upload_file_query.stop()\n",
    "#         print(f\"Stopping upload_file stream\")\n",
    "    \n",
    "#     time.sleep(5)\n",
    "\n",
    "# if any([q.isActive for q in spark.streams.active]):\n",
    "#     df_amalgamated_query.stop()\n",
    "#     print(\"All streams have been stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "003cc183-78b3-457e-936e-35a2de890b83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wait 15 minutes for any stream to stop itself, or auto stop after 15mins.\n",
    "\n",
    "# try:\n",
    "#     spark.streams.awaitAnyTermination(900)\n",
    "# finally:\n",
    "#     # Stop all streams\n",
    "#     df_complete.stop()\n",
    "#     df_input_upload.stop()\n",
    "#     df_create_record.stop()\n",
    "#     df_upload_file.stop()\n",
    "#     print(\"All streams have been stopped\")\n",
    "\n",
    "\n",
    "df_input_upload = spark.read.format(\"delta\").load(input_upload_responses_path)\n",
    " \n",
    "# # Read the response data for create_record_upload\n",
    "df_create_record_upload = spark.read.format(\"delta\").load(create_record_responses_path)\n",
    " \n",
    "# # Read the response data for upload_file\n",
    "df_upload_file_upload = spark.read.format(\"delta\").load(upload_file_responses_path) \n",
    " \n",
    "# Read the response data for df_amalgamated_responses\n",
    "df_amalgamated_responses = spark.read.format(\"delta\").load(amalgamated_responses_path)\n",
    "\n",
    "logger.info(f\"Records in df_input_upload: {df_input_upload.count()}\")\n",
    "logger.info(f\"Records in df_create_record_upload: {df_create_record_upload.count()}\")\n",
    "logger.info(f\"Records in df_upload_file_upload: {df_upload_file_upload.count()}\")\n",
    "logger.info(f\"Records in df_amalgamated_responses: {df_amalgamated_responses.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34fe10c4-e91c-44af-9385-85748d340e3f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Amalgamated Dataframe"
    }
   },
   "outputs": [],
   "source": [
    "# df_amalgamated_output = spark.read.format(\"delta\").load(amalgamated_responses_path)\n",
    "# df_amalgamated_output.createOrReplaceTempView(\"bails_amalgamated_response_data\")\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "#         SELECT \n",
    "#         *\n",
    "\n",
    "# FROM bails_amalgamated_response_data\n",
    "\n",
    "# \"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cff2513-9645-40ba-b7a2-11e0f563fbdc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Input Upload Dataframe"
    }
   },
   "outputs": [],
   "source": [
    "# df_input_upload = spark.read.format(\"delta\").load(input_upload_responses_path)\n",
    "# df_input_upload.createOrReplaceTempView(\"bails_input_upload\")\n",
    "\n",
    "# spark.sql(f\"\"\"\n",
    "# SELECT  DATE_FORMAT(timestamp, 'ddMMMyyyy') as date,\n",
    "#         operation,\n",
    "#         {expected_input_upload} as expected_input_upload,\n",
    "#         COUNT(CASE WHEN status = 1 THEN 1 END) as count_successful_input_upload,\n",
    "#         COUNT(CASE WHEN status != 1 THEN 1 END) as count_unsuccessful_input_upload,\n",
    "#         concat(((count_successful_input_upload/expected_input_upload) * 100), \"%\") as `%_of_successful_input_upload`\n",
    "\n",
    "# FROM bails_input_upload\n",
    "# GROUP BY date, operation\n",
    "# ORDER BY date DESC\n",
    "# \"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7689272-f22e-4451-973c-81f34efff7b5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Record Dataframe"
    }
   },
   "outputs": [],
   "source": [
    "# df_create_record = spark.read.format(\"delta\").load(create_record_responses_path)\n",
    "# df_create_record.createOrReplaceTempView(\"bails_create_record\")\n",
    "\n",
    "# spark.sql(f\"\"\"\n",
    "# SELECT \n",
    "#     DATE_FORMAT(process_time, 'ddMMMyyyy') as date,\n",
    "#     operation,\n",
    "#     {expected_created_records} as expected_created_records,\n",
    "#     COUNT(CASE WHEN status = 1 THEN 1 END) as count_of_successful_created_records,\n",
    "#     COUNT(CASE WHEN status != 1 THEN 1 END) as count_of_unsuccessful_created_records,\n",
    "#     CONCAT(ROUND((COUNT(CASE WHEN status = 1 THEN 1 END) * 100.0 / {expected_created_records}), 2), '%') as `%_of_successful_created_records`\n",
    "# FROM bails_create_record\n",
    "# GROUP BY date, operation\n",
    "# ORDER BY date DESC\n",
    "# \"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df9ab0ea-4065-4199-a4ad-526de2d68059",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Upload File Dataframe"
    }
   },
   "outputs": [],
   "source": [
    "# df_upload_file = spark.read.format(\"delta\").load(upload_file_responses_path)\n",
    "# df_upload_file.createOrReplaceTempView(\"bails_input_upload\")\n",
    "\n",
    "# spark.sql(f\"\"\"\n",
    "# SELECT  DATE_FORMAT(process_time, 'ddMMMyyyy') as date,\n",
    "#         operation,\n",
    "#         {expected_upload_file} as expected_upload_file,\n",
    "#         COUNT(CASE WHEN status = 1 THEN 1 END) as count_successful_upload_file,\n",
    "#         COUNT(CASE WHEN status != 1 THEN 1 END) as count_unsuccessful_upload_file,\n",
    "#         concat(((count_successful_upload_file/expected_upload_file) * 100), \"%\") as `%_of_successful_input_upload`\n",
    "\n",
    "\n",
    "# FROM bails_input_upload\n",
    "# GROUP BY date, operation\n",
    "# ORDER BY date DESC\n",
    "# \"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b2ac9ca-6efe-4073-9e3d-cd8f9ed09ebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " dbutils.notebook.exit(\"Notebook completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37e52e25-6980-4e9e-9e2b-07c13e38a67f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5850068046645897,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bails_resp_autoloader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
