{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bdf9fd7-7829-4e41-9418-34ccb41a9f79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install confluent-kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2c52c0d-659a-4723-988d-312a410da72c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "import json\n",
    "from  itertools import islice\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import col, decode, split, element_at, udf, lit, reduce\n",
    "import logging\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark import SparkContext\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark import SparkContext\n",
    "import os\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cba8096c-d561-462a-bc36-6898e4b7e3c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Assign configs\n",
    "config = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/configs/config.json\")\n",
    "env = config.first()[\"env\"].strip().lower()\n",
    "lz_key = config.first()[\"lz_key\"].strip().lower()\n",
    "\n",
    "keyvault_name = f\"ingest{lz_key}-meta002-{env}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "535b9c83-f636-4d70-a9f7-c5ae8454b1f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Access the Service Principle secrets from keyvaults\n",
    "client_secret = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-SECRET')\n",
    "tenant_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-TENANT-ID')\n",
    "client_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2dc6c70-23e0-44aa-bf17-83b482008373",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Paramaterise containers\n",
    "curated_storage_account = f\"ingest{lz_key}curated{env}\"\n",
    "curated_container = \"gold\"\n",
    "silver_curated_container = \"silver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b249b972-f76f-4b58-af80-55c928d394ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##Assign OAuth to curated storage account\n",
    "storage_accounts = [curated_storage_account]\n",
    "\n",
    "for storage_account in storage_accounts:\n",
    "    configs = {\n",
    "            f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\": \"OAuth\",\n",
    "            f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\":\n",
    "                \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "            f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\": client_id,\n",
    "            f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\": client_secret,\n",
    "            f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\":\n",
    "                f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "        }\n",
    "    for key,val in configs.items():\n",
    "        spark.conf.set(key,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb21b630-eec9-4936-89fd-1cbc46bc8e93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print out the auth config for each storage account to confirm\n",
    "for storage_account in storage_accounts:\n",
    "    key = f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\"\n",
    "    print(f\"{key}: {spark.conf.get(key, 'MISSING')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63dbf6b9-7c6c-4950-bdf8-cd7a0d4d6152",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ***Read in HTML and JSON files***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5967fc07-a443-406a-a22a-801c55c73301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "######### are we going to be using one single EH for all processing?\n",
    "\n",
    "eh_kv_secret = dbutils.secrets.get(scope=keyvault_name, key=\"RootManageSharedAccessKey\")\n",
    "\n",
    "# Event Hub configurations\n",
    "eventhubs_hostname = f\"ingest{lz_key}-integration-eventHubNamespace001-{env}.servicebus.windows.net:9093\"\n",
    "conf = {\n",
    "    'bootstrap.servers': eventhubs_hostname,\n",
    "    'security.protocol': 'SASL_SSL',\n",
    "    'sasl.mechanism': 'PLAIN',\n",
    "    'sasl.username': '$ConnectionString',\n",
    "    'sasl.password': eh_kv_secret,\n",
    "    'retries': 5,                     # Increased retries\n",
    "    'enable.idempotence': True,        # Enable idempotent producer\n",
    "}\n",
    "broadcast_conf = sc.broadcast(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc9f21d3-c9d5-4ca3-8659-d0167b4cac0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## List the states\n",
    "states = [\n",
    "    \"paymentPending\", \n",
    "    \"appealSubmitted\", \n",
    "    \"awaitingRespondentEvidence(a)\", \n",
    "    \"awaitingRespondentEvidence(b)\", \n",
    "    \"caseUnderReview\", \n",
    "    \"reasonForAppealSubmitted\", \n",
    "    \"listing\",\n",
    "    \"PrepareForHearing\",\n",
    "    \"Decision\",\n",
    "    \"FTPA Submitted (a)\",\n",
    "    \"FTPA Submitted (b)\",\n",
    "    \"Decided (b)\",\n",
    "    \"Decided (a)\",\n",
    "    \"FTPA Decided\",\n",
    "    \"Ended\",\n",
    "    \"Remitted\"\n",
    "]\n",
    "\n",
    "valid_json_files = []\n",
    "invalid_json_files = []\n",
    "all_html_dfs = []\n",
    "\n",
    "## Loop through each state and assign file_path to reference JSON files from\n",
    "for state in states:\n",
    "    gold_files_base_path = f\"abfss://{curated_container}@{curated_storage_account}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/{state}/\"\n",
    "\n",
    "    try:\n",
    "        files = dbutils.fs.ls(gold_files_base_path)[-1] # Index on newest file\n",
    "        \n",
    "        valid_json = files.path + \"JSON/\" # Return only the path. Access only valid JSON files\n",
    "        valid_json_files.append(valid_json)\n",
    "\n",
    "        ## Remove once valid_json has outputs from fresh DLT run\n",
    "        invalid_json = files.path + \"INVALID_JSON/\"\n",
    "        try:\n",
    "            dbutils.fs.ls(invalid_json)\n",
    "        except Exception:\n",
    "            print(f\"No INVALID_JSON directory found for state: {state}\")\n",
    "            continue\n",
    "    \n",
    "        # Load binary data\n",
    "        binary_df = (\n",
    "            spark.read.format('binaryFile')\n",
    "            .option('pathGlobFilter', '*.{html,json}')\n",
    "            .option('recursiveFileLookup', 'true')\n",
    "            .load(invalid_json)\n",
    "        )\n",
    "        \n",
    "        # Process data\n",
    "        html_df = (\n",
    "            binary_df\n",
    "            .withColumn(\"content_str\", decode(col('content'), 'utf-8'))\n",
    "            .withColumn(\"file_path\", element_at(split(col('path'), '/'), -1))\n",
    "            .withColumn(\"state\", lit(state))\n",
    "            .select('content_str', 'file_path', 'state')\n",
    "        )\n",
    "        \n",
    "        # Check if we have data to process\n",
    "        record_count = html_df.count()\n",
    "        if record_count == 0:\n",
    "            print(f\"No data to process for state: {state}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Found {record_count} records for state: {state}\")\n",
    "        all_html_dfs.append(html_df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing state {state}: {e}\")\n",
    "\n",
    "display([{'path': v} for v in valid_json_files])\n",
    "\n",
    "if all_html_dfs:\n",
    "    ## Union all DataFrames produced together by columns\n",
    "    combined_html_df = reduce(DataFrame.unionByName, all_html_dfs)\n",
    "    display(combined_html_df)\n",
    "\n",
    "# Repartition for parallelism\n",
    "# num_spark_partitions =  16\n",
    "# optimized_html_df = combined_html_df.repartition(num_spark_partitions, col(\"state\"))\n",
    "\n",
    "num_spark_partitions =  1\n",
    "optimized_html_df = combined_html_df.repartition(num_spark_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ce1db05-2a80-4341-8eaa-b8ca88b93192",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755102509839}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ##List the states\n",
    "# states = [\n",
    "#     \"paymentPending\", \n",
    "#     \"appealSubmitted\", \n",
    "#     \"awaitingRespondentEvidence(a)\", \n",
    "#     \"awaitingRespondentEvidence(b)\", \n",
    "#     \"caseUnderReview\", \n",
    "#     \"reasonForAppealSubmitted\", \n",
    "#     \"listing\",\n",
    "#     \"PrepareForHearing\",\n",
    "#     \"Decision\",\n",
    "#     \"FTPA Submitted (a)\",\n",
    "#     \"FTPA Submitted (b)\",\n",
    "#     \"Decided (b)\",\n",
    "#     \"Decided (a)\",\n",
    "#     \"FTPA Decided\",\n",
    "#     \"Ended\",\n",
    "#     \"Remitted\"\n",
    "# ]\n",
    "\n",
    "# valid_json_files = []\n",
    "# invalid_json_files = []\n",
    "# all_html_dfs = []\n",
    "\n",
    "# ##Loop through each state and assign file_path to reference JSON files from\n",
    "# for state in states:\n",
    "#     gold_files_base_path = f\"abfss://{curated_container}@{curated_storage_account}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/{state}/\"\n",
    "\n",
    "#     # Add the if state to avoid errors until other states are populated\n",
    "#     if state == \"paymentPending\":\n",
    "#             files = dbutils.fs.ls(gold_files_base_path)[-1] #Index on newest file\n",
    "            \n",
    "#             valid_json = files.path + \"JSON/\" #Return only the path. Access only valid JSON files\n",
    "#             valid_json_files.append(valid_json)\n",
    "\n",
    "#             ## Remove once valid_json has outputs from fresh DLT run\n",
    "#             invalid_json = files.path + \"INVALID_JSON/\"\n",
    "#             invalid_json_files.append(invalid_json)\n",
    "\n",
    "#             binary_df = (\n",
    "#             spark.read.format('binaryFile')\n",
    "#             .option('pathGlobFilter', '*.{html,json}')\n",
    "#             .option('recursiveFileLookup', 'true')\n",
    "#             .load(invalid_json)\n",
    "#             )\n",
    "\n",
    "#             html_df = (\n",
    "#             binary_df\n",
    "#             .withColumn(\"content_str\", decode(col('content'), 'utf-8'))\n",
    "#             .withColumn(\"file_path\", element_at(split(col('path'), '/'), -1))\n",
    "#             .withColumn(\"state\", lit(state)) \n",
    "#             .select('content_str', 'file_path', 'state')\n",
    "#             )\n",
    "\n",
    "#             all_html_dfs.append(html_df)\n",
    "\n",
    "# display([{'path': v} for v in valid_json_files])\n",
    "\n",
    "# if all_html_dfs:\n",
    "#     ##Union all DataFrames produced together by columns\n",
    "#     combined_html_df = reduce(DataFrame.unionByName, all_html_dfs)\n",
    "#     display(combined_html_df)\n",
    "\n",
    "# # Repartition for parallelism\n",
    "# num_spark_partitions =  16\n",
    "# optimized_html_df = combined_html_df.repartition(num_spark_partitions, col(\"state\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03acacf5-ce51-434f-917c-28abc0845724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_partition(partition):\n",
    "    import logging\n",
    "    from confluent_kafka import Producer\n",
    "    from datetime import datetime\n",
    "\n",
    "    # Initialize logger\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger('KafkaProducer')\n",
    "    \n",
    "    failure_list = []\n",
    "    success_list = []\n",
    "    results = []\n",
    "\n",
    "    # Initialize producer\n",
    "    producer = Producer(**broadcast_conf.value)\n",
    "\n",
    "    for row in partition:\n",
    "        if row.file_path is None or row.content_str is None:\n",
    "            logger.warning(f\"Skipping row with missing file_path/content_str: {row}\")\n",
    "            continue\n",
    "\n",
    "        ## Use current row for callback\n",
    "        current_state = row.state\n",
    "        current_file_path = row.file_path\n",
    "\n",
    "        def delivery_report(err, msg):\n",
    "            key_str = msg.key().decode('utf-8') if msg.key() is not None else \"Unknown\"\n",
    "            timestamp = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            \n",
    "            if err is not None:\n",
    "                err_msg = str(err)\n",
    "                logger.error(f\"Message delivery failed for key {key_str}: {err}\")\n",
    "                failure_list.append((key_str, current_state, \"failure\", err_msg, timestamp))\n",
    "            else:\n",
    "                success_list.append((key_str, current_state, \"success\", \"\", timestamp))\n",
    "\n",
    "        try:\n",
    "            # Handle different content_str types\n",
    "            if isinstance(row.content_str, str):\n",
    "                value = row.content_str.encode('utf-8')\n",
    "            elif isinstance(row.content_str, bytearray):\n",
    "                value = bytes(row.content_str)\n",
    "            elif isinstance(row.content_str, bytes):\n",
    "                value = row.content_str\n",
    "            else:\n",
    "                logger.error(f\"Unsupported type for content_str: {type(row.content_str)}\")\n",
    "                failure_list.append((current_file_path, current_state, \"failure\", \"Unsupported content type\", datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "                continue\n",
    "\n",
    "            # Produce message to Kafka\n",
    "            producer.produce(\n",
    "                topic=f'evh-active-pub-{env}-{lz_key}-uks-dlrm-01',\n",
    "                key=current_file_path.encode('utf-8'),\n",
    "                value=value,\n",
    "                callback=delivery_report\n",
    "            )\n",
    "\n",
    "        except BufferError:\n",
    "            logger.error(\"Producer buffer full. Polling for events.\")\n",
    "            producer.poll(1)\n",
    "            # Retry the message production\n",
    "            try:\n",
    "                producer.produce(\n",
    "                    topic=f'evh-active-pub-{env}-{lz_key}-uks-dlrm-01',\n",
    "                    key=current_file_path.encode('utf-8'),\n",
    "                    value=value,\n",
    "                    callback=delivery_report\n",
    "                )\n",
    "            except Exception as retry_e:\n",
    "                logger.error(f\"Failed to produce message after buffer retry: {retry_e}\")\n",
    "                failure_list.append((current_file_path, current_state, \"failure\", f\"Buffer error retry failed: {str(retry_e)}\", datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error during production: {e}\")\n",
    "            failure_list.append((current_file_path, current_state, \"failure\", str(e), datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "\n",
    "    # Flush producer and handle any remaining messages\n",
    "    try:\n",
    "        producer.flush()\n",
    "        logger.info(\"Producer flushed successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during flush: {e}\")\n",
    "\n",
    "    # Combine all results\n",
    "    results.extend(success_list)\n",
    "    results.extend(failure_list)\n",
    "\n",
    "    return results\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"file_name\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"error_message\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for state_df in all_html_dfs:\n",
    "    ## Pick up state value (name)\n",
    "    current_state = state_df.select(\"state\").first()[\"state\"]\n",
    "    print(f\"Starting processing for state: {current_state}\")\n",
    "\n",
    "    result_rdd = state_df.rdd.mapPartitions(process_partition)\n",
    "    result_df = spark.createDataFrame(result_rdd, schema)\n",
    "\n",
    "    # Trigger execution & force completion for this state\n",
    "    count = result_df.count()\n",
    "    print(f\"Completed processing {count} records for state: {current_state}\")\n",
    "\n",
    "    display(result_df)\n",
    "    all_results.append(result_df)\n",
    "\n",
    "# Combine final results\n",
    "if all_results:\n",
    "    final_results_df = reduce(DataFrame.unionByName, all_results)\n",
    "    display(final_results_df)\n",
    "else:\n",
    "    print(\"No results generated for any state.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38d3987b-a4c5-42bf-9c7e-0b2621883e51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Display failed files\n",
    "\n",
    "failed_files = final_results_df.filter(col(\"status\") == \"failure\")\n",
    "\n",
    "display(failed_files)\n",
    "failed_files.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a5d0d77-653f-4306-a493-f8c9c53b7a8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Filter over the relevant state as all data is merged together and append each state seperately\n",
    "\n",
    "final_results_df = final_results_df.coalesce(4)\n",
    "for state in states:\n",
    "    silver_base_path = f\"abfss://{silver_curated_container}@{curated_storage_account}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/{state}/publish_audit_db_eh\"\n",
    "    \n",
    "    (final_results_df\n",
    "        .filter(col(\"state\") == state)\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .save(f\"{silver_base_path}\")\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Active_Publish_EH_HTML_JSON",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
