{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7dbb9e65-b271-41a9-acc1-4e95fee023cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "####################\n",
    "#CONFIG & ENV SETUP\n",
    "####################\n",
    "AppealState = \"paymentPending\"\n",
    "\n",
    "config = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/configs/config.json\")\n",
    "env_name = config.first()[\"env\"].strip().lower()\n",
    "lz_key = config.first()[\"lz_key\"].strip().lower()\n",
    " \n",
    "print(f\"env_code: {lz_key}\")  # This won't be redacted\n",
    "print(f\"env_name: {env_name}\")  # This won't be redacted\n",
    " \n",
    "KeyVault_name = f\"ingest{lz_key}-meta002-{env_name}\"\n",
    "print(f\"KeyVault_name: {KeyVault_name}\")\n",
    " \n",
    "# Service principal credentials\n",
    "client_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-ID\")\n",
    "client_secret = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-SECRET\")\n",
    "tenant_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-TENANT-ID\")\n",
    " \n",
    "# Storage account names\n",
    "curated_storage = f\"ingest{lz_key}curated{env_name}\"\n",
    "checkpoint_storage = f\"ingest{lz_key}xcutting{env_name}\"\n",
    "raw_storage = f\"ingest{lz_key}raw{env_name}\"\n",
    "landing_storage = f\"ingest{lz_key}landing{env_name}\"\n",
    "external_storage = f\"ingest{lz_key}external{env_name}\"\n",
    " \n",
    " \n",
    "# Spark config for curated storage (Delta table)\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{curated_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{curated_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{curated_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{curated_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{curated_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{checkpoint_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{checkpoint_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{checkpoint_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{checkpoint_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{checkpoint_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{raw_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{raw_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{raw_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{raw_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{raw_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{landing_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{landing_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{landing_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{landing_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{landing_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{external_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{external_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{external_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{external_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{external_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    "\n",
    "# Setting variables for use in subsequent cells\n",
    "bronze_path = f\"abfss://bronze@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/\"\n",
    "silver_path = f\"abfss://silver@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/\"\n",
    "audit_path = f\"abfss://silver@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/AUDIT/{AppealState}\"\n",
    "gold_path = f\"abfss://gold@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/{AppealState}\"\n",
    " \n",
    " \n",
    " \n",
    "# Print all variables\n",
    "variables = {\n",
    "    # \"read_hive\": read_hive,\n",
    "    \n",
    "    \"bronze_path\": bronze_path,\n",
    "    \"silver_path\": silver_path,\n",
    "    \"audit_path\": audit_path,\n",
    "    \"gold_path\": gold_path,\n",
    "    \"key_vault\": KeyVault_name,\n",
    "    \"AppealState\": AppealState\n",
    " \n",
    "}\n",
    " \n",
    "display(variables)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "from functools import reduce\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, to_date, coalesce, greatest, lit, explode, date_add, current_date, count, monotonically_increasing_id, array_contains, count, explode, year, month, dayofmonth, when, first, countDistinct, collect_list, concat_ws, array, size, lower\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    DateType,\n",
    "    ArrayType,\n",
    ")\n",
    "import json\n",
    "# from docx import Document\n",
    "# from docx.shared import Inches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a89f6954-d298-4902-9b9a-46e6c8bf69c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Setup Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4ffbd56-1f5f-4670-883c-aac9e5595b3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "#Used during development to Quickly reload libraries as they dont auto refresh when edited\n",
    "#####################\n",
    "import importlib\n",
    "import functions.parquet_helper as pqhelper\n",
    "import functions.sql_helper as sqlhelper\n",
    "import functions.compare_data_helper as datahelper\n",
    "import reporting.sql_to_parquet_html_report as html\n",
    "import reporting.csv_report as csv\n",
    "\n",
    "importlib.reload(pqhelper)\n",
    "importlib.reload(sqlhelper)\n",
    "importlib.reload(datahelper)\n",
    "importlib.reload(html)\n",
    "importlib.reload(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46207015-3119-4f07-a695-53b8a37ed455",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768229375787}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_6caf7489\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_ace2d1b5\",\"enabled\":true,\"columnId\":\"status\",\"dataType\":\"string\",\"filterType\":\"oneof\"}],\"local\":false,\"updatedAt\":1770198948489}],\"syncTimestamp\":1770198948490}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "#TEST 1 - MAIN TEST - Source (SQL) to Target(parquet)\n",
    "# - Tests -\n",
    "#- Check Schema\n",
    "#- Check Row Counts\n",
    "#- Check Row Hash (all data matches exactly)\n",
    "#######################\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "from models.test_result import TestResult\n",
    "import functions.parquet_helper as pqhelper\n",
    "import functions.sql_helper as sqlhelper\n",
    "import functions.compare_data_helper as datahelper\n",
    "import reporting.sql_to_parquet_html_report as html\n",
    "import reporting.csv_report as csv\n",
    "\n",
    "\n",
    "###################\n",
    "#SETUP / CONFIG\n",
    "###################\n",
    "\n",
    "PARQUET_BASE_PATH = \"abfss://landing@ingest00landingstg.dfs.core.windows.net/SQLServer/Sales/IRIS/dbo/\"\n",
    "SQL_SCHEMA = \"ARIA_STG.dbo\"\n",
    "results = []\n",
    "\n",
    "#Setup SQL Database Connection\n",
    "jdbc_hostname = \"ingest00-legacy\"\n",
    "jdbc_port = 1433\n",
    "database_name = \"ARIA_STG\"\n",
    "username = \"ARIA_databricks\"\n",
    "password = \"\"\n",
    "SQL_SCHEMA = \"ARIA_STG.dbo\"\n",
    "SQL_JDBC_URL = f\"jdbc:sqlserver://{jdbc_hostname}:{jdbc_port};databaseName={database_name};user={username};password={password};encrypt=false\"\n",
    "\n",
    "#####################\n",
    "#Get SQL Table List\n",
    "#####################\n",
    "sql_tables = sqlhelper.get_sql_tables(SQL_JDBC_URL)\n",
    "# print(str(len(sql_tables)))\n",
    "\n",
    "######################\n",
    "#Known Empty tables (Checked in orig sql database and confirmed with no folder/data in parquet file location)\n",
    "######################\n",
    "known_empty_tables = [\"Spot_Work_LogShippingInfo\", \"Centre\", \"UserCopyEntity\", \"Spot_Log\", \"Spot_RefreshAutoClose\",\"Spot_Repl_Agents\", \"Spot_Repl_PubSub\" , \"Spot_TableFragDetails\" , \"Spot_TraceConsumer_Global\", \"Spot_TraceData\", \"Spot_TraceGlobalSummary\", \"Spot_Work_BufferCacheContents\", \"Spot_Work_Locks\", \"Spot_Work_OldestTranInfo\", \"Spot_Work1\", \"Spot_Work3\", \"AssociatedRep\", \"Spot_WorkFileDetails\", \"BundleRequest\" , \"BundleAccess\", \"InterfaceRestartQueue\"]\n",
    "\n",
    "########################\n",
    "#Begin Main Test - Looping each table and check each one - building a report of pass/fail\n",
    "########################\n",
    "#Loop all tables and check, schema, counts and row hash's\n",
    "for table in sql_tables:\n",
    "    #Check if Empty Table (as they were not copied to parquet)\n",
    "    if table in known_empty_tables:\n",
    "        results.append(TestResult(table, \"TABLE_CHECK\", \"PASS\", f\"Ignored Checking Empty table :{table}\"))\n",
    "        continue\n",
    "\n",
    "\n",
    "    #############\n",
    "    #Setup parquet location for table\n",
    "    #############    \n",
    "    table_parquet_path = PARQUET_BASE_PATH + table + \"/\"    \n",
    "    try:\n",
    "        parquet_folders = pqhelper.get_parquet_root_paths(table_parquet_path,dbutils)\n",
    "    except:\n",
    "        results.append(TestResult(table, \"SEARCH_PARQUET\", \"FAIL\", f\"Failed to find Parquet Folder for table :{table}\"))\n",
    "        continue\n",
    "\n",
    "    #Found one Parquet as Expected\n",
    "    if len(parquet_folders) == 1:                        \n",
    "        parquet_path = parquet_folders[0]\n",
    "        \n",
    "        #############\n",
    "        #Load SQL Tables into Dataframes\n",
    "        #############\n",
    "        try:\n",
    "            #if Transaction table, put in squarebracks else raises error for reserved word\n",
    "            if table == \"Transaction\":\n",
    "                table = f\"[{table}]\"                        \n",
    "            sql_df = sqlhelper.read_sql_table(table, SQL_JDBC_URL, SQL_SCHEMA)\n",
    "            # display(sql_df)\n",
    "        except:\n",
    "            results.append(TestResult(table, \"READ_SQL\", \"FAIL\", f\"Failed to read SQL for table :{table}\"))\n",
    "            continue\n",
    "        \n",
    "        #############\n",
    "        #Load Parquet files as Dataframes        \n",
    "        #############\n",
    "        try:\n",
    "            pq_df = spark.read.parquet(parquet_path)        \n",
    "            # display(pq_df)\n",
    "        except:\n",
    "            results.append(TestResult(table, \"READ_PQ\", \"FAIL\", f\"Failed to read Parquet for table :{table}\"))\n",
    "            continue\n",
    "\n",
    "\n",
    "        #############\n",
    "        #Compare SCHEMA\n",
    "        #############\n",
    "        try:\n",
    "            results.append(datahelper.check_schema(table, sql_df, pq_df))\n",
    "        except:\n",
    "            results.append(TestResult(table, \"CHECK_SCHEMA\", \"FAIL\", f\"Failed to check Schema for table :{table}\"))\n",
    "            continue\n",
    "\n",
    "        #############\n",
    "        #Check table Row Counts \n",
    "        #############\n",
    "        try:\n",
    "            results.append(datahelper.check_row_counts(table, sql_df, pq_df))\n",
    "        except:\n",
    "            results.append(TestResult(table, \"CHECK_ROW_COUNT\", \"FAIL\", f\"Failed to check row Count for table :{table}\"))\n",
    "            continue\n",
    "\n",
    "        #############\n",
    "        #Compare Row Data (converting row data to string and hash for source and target then comparing row hash values)\n",
    "        #############        \n",
    "        try:            \n",
    "            result, sql_h,pq_h = datahelper.check_row_data(table, sql_df, pq_df, \"\")\n",
    "            results.append(result)\n",
    "            # display(sql_h)\n",
    "            # display(pq_h)   \n",
    "        except:\n",
    "            results.append(TestResult(table, \"CHECK_ROW_DATA\", \"FAIL\", f\"Failed to check data for table :{table}\"))\n",
    "            continue   \n",
    "\n",
    "        ############# \n",
    "        #Report All Tests Run for Table\n",
    "        ############# \n",
    "        results.append(TestResult(table, f\"TABLE_CHECK\", \"PASS\", f\"All Tests Have Run for : {table} (Schema/Record Count/Row Hash Check)\")) \n",
    "\n",
    "    #Failed to find or more than one parquet folder\n",
    "    else:        \n",
    "        results.append(TestResult(table, \"SEARCH_PARQUET\", \"FAIL\", f\"Failed to find One (found : {str(len(parquet_folders))}) Parquet folder for table :{table}\"))\n",
    "\n",
    "\n",
    "####################\n",
    "#Print Results as DataFrame and Display\n",
    "####################\n",
    "ordered_cols = [\n",
    "    \"table_name\",\n",
    "    \"test_type\",\n",
    "    \"status\",\n",
    "    \"message\",    \n",
    "]\n",
    "results_df = spark.createDataFrame(results)\n",
    "results_ordered_df = results_df.select(*ordered_cols)\n",
    "display(results_ordered_df)\n",
    "\n",
    "########################\n",
    "#Create Results Folder with CSV AND HTML REPORT\n",
    "########################\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f\"results/{timestamp}/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate HTML report\n",
    "html_report_path = html.generate_html_report(results,output_dir,timestamp)\n",
    "\n",
    "#Generate CSV Report\n",
    "csv_report_path = csv.generate_csv_report(results,output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65d7fa24-3cb6-4732-8ae5-b714e8bbea65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "#TEST 2 - #Compare SQL Table List with Parquet Folder list\n",
    "#######################\n",
    "\n",
    "##RESULTS 21 Folder difference - due to empty tables in sql that are not copied over.\n",
    "known_empty_tables = [\"Spot_Work_LogShippingInfo\", \"Centre\", \"UserCopyEntity\", \"Spot_Log\", \"Spot_RefreshAutoClose\",\"Spot_Repl_Agents\", \"Spot_Repl_PubSub\" , \"Spot_TableFragDetails\" , \"Spot_TraceConsumer_Global\", \"Spot_TraceData\", \"Spot_TraceGlobalSummary\", \"Spot_Work_BufferCacheContents\", \"Spot_Work_Locks\", \"Spot_Work_OldestTranInfo\", \"Spot_Work1\", \"Spot_Work3\", \"AssociatedRep\", \"Spot_WorkFileDetails\", \"BundleRequest\" , \"BundleAccess\", \"InterfaceRestartQueue\"]\n",
    "\n",
    "#TODO - Add in using list above in results\n",
    "#TODO add on reporting to file / table solution\n",
    "\n",
    "###########\n",
    "#Get SQL list of Tables\n",
    "###########\n",
    "sql_tables = {\n",
    "    t.lower()\n",
    "    for t in sqlhelper.get_sql_tables(SQL_JDBC_URL)\n",
    "}\n",
    "\n",
    "###########\n",
    "#Get list of Parquet Folders\n",
    "###########\n",
    "folders = {\n",
    "    f.name.rstrip(\"/\").lower()\n",
    "    for f in dbutils.fs.ls(PARQUET_BASE_PATH)\n",
    "    if f.isDir()\n",
    "}\n",
    "# Normalise\n",
    "folders = {f.lower() for f in folders}\n",
    "#print(str(len(folders)))\n",
    "\n",
    "###########\n",
    "#Compare SQl vs Parquet lists of tables/folders\n",
    "###########\n",
    "matching = sql_tables & folders\n",
    "missing_folders = sql_tables - folders\n",
    "extra_folders = folders - sql_tables\n",
    "\n",
    "###########\n",
    "#Report Results\n",
    "###########\n",
    "\n",
    "print(f\"Tables in DB     : {len(sql_tables)}\")\n",
    "print(f\"Folders in DBFS  : {len(folders)}\")\n",
    "print(f\"Matching         : {len(matching)}\")\n",
    "print()\n",
    "\n",
    "print(f\"❌ Missing folders : {str(len(missing_folders))} - (table exists, folder missing):\")\n",
    "for t in sorted(missing_folders):\n",
    "    print(\"  \", t)\n",
    "\n",
    "print(f\"\\n⚠️ Extra folders : {str(len(missing_folders))} - (folder exists, no table):\")\n",
    "for f in sorted(extra_folders):\n",
    "    print(\"  \", f)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sql_to_parquet_testing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
