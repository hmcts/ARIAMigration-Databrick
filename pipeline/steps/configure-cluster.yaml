parameters:
  pythonVersion: ''
  env: ''
  mainRepositoryName: ''
  databricksInstance: '' # E.g: https://<databricks-instance>.cloud.databricks.com
  databricksClusterId: '' # Target Cluster ID
  wheelName: 'ARIAFUNCITONS-0.0.1-py3-none-any.whl' # (e.g., ARIA_FUNCTIONS-0.1.0-py3-none-any.whl)

##
# Install Databricks CLI
##
steps:
- task: AzureCLI@2
  displayName: 'Install Databricks CLI'
  condition: succeeded()
  inputs:
    azureSubscription: 'DTS-DATAINGEST-${{ upper(parameters.env) }}'
    scriptType: bash
    scriptLocation: inlineScript
    inlineScript: |
      set -x
      source working_dir/bin/activate
      export PYTHONPATH="${PYTHONPATH}:/./"
      pip install databricks-cli
      databricks configure --token --host ${{ parameters.databricksInstance }}
      databricks fs cp $(Pipeline.Workspace)/wheel-cache/${{ parameters.wheelName }} dbfs:/FileStore/shared_wheels/${{ parameters.wheelName }}
      curl -X POST -H "Authorization: Bearer $(DATABRICKS_TOKEN)" \
           -H "Content-Type: application/json" \
           -d '{
                 "cluster_id": "${{ parameters.databricksClusterId }}",
                 "libraries": [
                   {
                     "whl": "dbfs:/FileStore/shared_wheels/${{ parameters.wheelName }}"
                   }
                 ]
               }' \
           ${{ parameters.databricksInstance }}/api/2.0/libraries/install
  env:
    DATABRICKS_TOKEN: $(AUTH-AccessToken-sbox)
  workingDirectory: '$(System.DefaultWorkingDirectory)'
