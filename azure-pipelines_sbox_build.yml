trigger:
  branches:
    include:
    - main

pool:
  vmImage: 'ubuntu-latest' 

parameters:  
  - name: environment_components
    type: object
    default:
      - deployment: sbox_aria
        environment: sbox
        service_connection: DTS-DATAINGEST-SBOX
        landing_zones: 
          - landing_zone: '00'
            databricks_host: 'https://adb-3635282203417052.12.azuredatabricks.net'
            databricks_token: $(DATABRICKS_TOKEN_WORKSPACE_MAIN00)"
            Client_ID: $(SBOX_CLIENT_ID)
            Subscription_ID: $(SBOX_SUBSCRIPTION_ID)
            HTML_StorageAccount: 'ingest00landingsbox'
            Resource_Group: 'ingest00-main-sbox'
          - landing_zone: '01'
            databricks_host: 'https://adb-376876256300083.3.azuredatabricks.net'
            databricks_token: $(DATABRICKS_TOKEN_WORKSPACE_MAIN01)"
            Client_ID: $(SBOX_CLIENT_ID)
            Subscription_ID: $(SBOX_SUBSCRIPTION_ID)
            HTML_StorageAccount: 'ingest01landingsbox'
            Resource_Group: 'ingest01-main-sbox'
          - landing_zone: '02'
            databricks_host: 'https://adb-1879076228317698.18.azuredatabricks.net'
            databricks_token: $(DATABRICKS_TOKEN_WORKSPACE_MAIN02)"
            Client_ID: $(SBOX_CLIENT_ID)
            Subscription_ID: $(SBOX_SUBSCRIPTION_ID)
            HTML_StorageAccount: 'ingest02landingsbox'
            Resource_Group: 'ingest02-main-sbox'

      - deployment: 'stg_aria'
        environment: 'stg'
        service_connection: 'DTS-DATAINGEST-STG'
        dependsOn: 'sbox_aria'
        lzironments:
          - landing_zone: '00'
            databricks_host:
            databricks_token:
            Client_ID:
            Subscription_ID:
            HTML_StorageAccount: 'ingest00landingstg'
          - landing_zone: '01'
            databricks_host:
            databricks_token:
            Client_ID:
            Subscription_ID:
            HTML_StorageAccount: 'ingest01landingstg'
      - deployment: 'prod_aria'
        environment: 'prod'
        service_connection: 'DTS-DATAINGEST-PROD'
        dependsOn: 'stg_aria'
        lzironments: 
          - landing_zone: '00'
            databricks_host:
            databricks_token:
            Client_ID:
            Subscription_ID:
            HTML_StorageAccount: 'ingest00landingprod'
          - landing_zone: '01'
            databricks_host:
            databricks_token:
            Client_ID:
            Subscription_ID:
            HTML_StorageAccount: 'ingest01landingprod'


stages:
- stage: 'Build'
  displayName: 'Build Stage'
  jobs:
  - job: BuildJob
    displayName: 'Run Build and deploy'
    steps:
    - task: UsePythonVersion@0
      displayName: 'Use Python 3.x'
      inputs:
        versionSpec: '3.x'

    - script: |
        python --version
        pip install --upgrade pip
        pip install build
        pip install databricks-cli
        python -m build --wheel --outdir $(Build.SourcesDirectory)/Databricks/SharedFunctionsLib/dist \
          $(Build.SourcesDirectory)/Databricks/SharedFunctionsLib

        ls -la $(Build.SourcesDirectory)/Databricks/SharedFunctionsLib/dist
      displayName: 'Install Python dependencies & build Python wheel'
    
    - script: |
       pip install --upgrade pip
       pip install -r AzureFunctions/requirements.txt --target="./AzureFunctions/.python_packages/lib/site-packages"
      displayName: 'Install JOH AzureFunctions dependencies into package'

    # Configure and deploy to Workspace MAIN00
    - script: |
        echo "[DEFAULT]" > $(HOME)/.databrickscfg
        echo "host = $(DATABRICKS_HOST_WORKSPACE_MAIN00)" >> $(HOME)/.databrickscfg
        echo "token = $(DATABRICKS_TOKEN_WORKSPACE_MAIN00)" >> $(HOME)/.databrickscfg
      displayName: 'Configure Databricks CLI for Workspace MAIN00'

    - script: |
        databricks workspace delete /live --recursive 
        databricks workspace mkdirs /live
        databricks workspace import_dir Databricks /live --overwrite
      displayName: 'Publish Databricks folder to /live in Databricks MAIN00 Workspace'

    # Configure and deploy to Workspace MAIN01
    - script: |
        echo "[DEFAULT]" > $(HOME)/.databrickscfg
        echo "host = $(DATABRICKS_HOST_WORKSPACE_MAIN01)" >> $(HOME)/.databrickscfg
        echo "token = $(DATABRICKS_TOKEN_WORKSPACE_MAIN01)" >> $(HOME)/.databrickscfg
      displayName: 'Configure Databricks CLI for Workspace MAIN01'

    - script: |
        databricks workspace delete /live --recursive
        databricks workspace mkdirs /live
        databricks workspace import_dir Databricks /live --overwrite
      displayName: 'Publish Databricks folder to /live in Databricks MAIN01 Workspace'

    # Configure and deploy to Workspace MAIN02
    - script: |
        echo "[DEFAULT]" > $(HOME)/.databrickscfg
        echo "host = $(DATABRICKS_HOST_WORKSPACE_MAIN02)" >> $(HOME)/.databrickscfg
        echo "token = $(DATABRICKS_TOKEN_WORKSPACE_MAIN02)" >> $(HOME)/.databrickscfg
      displayName: 'Configure Databricks CLI for Workspace MAIN02'

    - script: |
        databricks workspace delete /live --recursive
        databricks workspace mkdirs /live
        databricks workspace import_dir Databricks /live --overwrite
      displayName: 'Publish Databricks folder to /live in Databricks MAIN02 Workspace'
    
    - task: PublishBuildArtifacts@1
      inputs:
        PathtoPublish: '$(Build.SourcesDirectory)/Databricks/SharedFunctionsLib/dist'
        ArtifactName: 'ariafunction'
        publishLocation: 'Container'
      displayName: 'Publish Wheel Artifact'
    
    - task: PublishBuildArtifacts@1
      inputs:
        PathtoPublish: '$(Build.SourcesDirectory)/AzureFunctions'
        ArtifactName: 'functionapp'
        publishLocation: 'Container'
      displayName: 'Publish JOH Azure Function'
    
- stage: 'Deploy'
  displayName: 'Deploy Stage'
  jobs:
  - job: DeployDatabricksResources
    displayName: 'Run Build and deploy'
    steps:

    - task: DownloadBuildArtifacts@1
      inputs:
        buildType: 'current'
        downloadType: 'single'
        artifactName: 'ariafunction'
        downloadPath: '$(Pipeline.Workspace)'
      displayName: 'Download wheel Artifact'
    
    - task: DownloadBuildArtifacts@1
      inputs:
        buildType: 'current'
        downloadType: 'single'
        artifactName: 'functionapp'
        downloadPath: '$(Pipeline.Workspace)'
      displayName: 'Download Azure Function Artifact'
    
    # Loop through each environment
    - "${{ each deployment in parameters.environment_components }}":
      - stage: ${{ deployment.deployment }}
        dependsOn: ${{ deployment.dependsOn }}
        jobs:
          - job: waitForValidation
            pool: server
            displayName: Wait for external validation
            steps:
              - "${{ if or(eq(deployment.environment, 'stg'), eq(deployment.environment, 'prod')) }}":
                  - task: ManualValidation@1
                    timeoutInMinutes: 23160 # task times out in 2 weeks
                    inputs:
                      approvers: andrew.mcdevitt@hmcts.net, ara.islam1@hmcts.net
                      instructions: Please validate the build configuration and resume

      # Loop through each landing zone per environment 
      - "${{ each lz in deployment.landing_zones }}":
          - job: Deploy HTML to ADLS - ${{ deployment.environment }} - LZ${{ lz.landing_zone }}
            displayName: 'Deploy HTML to ADLS'
            pool: ${{ eq(deployment.environment, 'sbox') && 'hmcts-sds-ptlsbox' || 'hmcts-sds-ptl' }}
            # Use hmcts-sds-pltsbox pool for SBOX
            # Use hmcts-sds-ptl pool for STG/PROD
            steps:     
              - task: AzureCLI@2
                inputs:
                  azureSubscription: ${{ if eq(deployment.environment, 'sbox') && 'DTS-DATAINGEST-SBOX' || 
                                            eq(deployment.environment, 'stg') && 'DTS-DATAINGEST-STG' || 
                                            eq(deployment.environment, 'prod') && 'DTS-DATAINGEST-PROD' }}
                  scriptType: 'bash'
                  scriptLocation: 'inlineScript'
                  inlineScript: |
                    export AZURE_STORAGE_YES_PROMPT=true

                    echo "Assigning Service Principal Blob Contributor Access"
                    az role assignment create \
                    --assignee $(lz.Client_ID) \
                    --role "Storage Blob Data Contributor" \
                    --scope /subscriptions/$(lz.Subscription_ID)/resourceGroups/$(lz.Resource_Group)/providers/Microsoft.Storage/storageAccounts/$(HTML_StorageAccount)
                    echo "Success! Service Principal has been assigned Storage Blob Data Contributor for the following scope: /subscriptions/$(Subscription_ID)/resourceGroups/$(Resource_Group)/providers/Microsoft.Storage/storageAccounts/$(HTML_StorageAccount)"
                    
                    echo "Output  $(lz.HTML_StorageAccount) storage account key"
                    storage_account_key=$(az storage account keys list \
                    --resource-group $(lz.Resource_Group) \
                    --account-name $(lz.HTML_StorageAccount) \
                    --query '[0].value' -o tsv)
                    echo "Success! $(lz.HTML_StorageAccount) is $storage_account_key"

                    echo "Retrieve  $(lz.HTML_StorageAccount) blob endpoint"
                    blob_endpoint=$(az storage account show \
                    --resource-group $(lz.Resource_Group) \
                    --name $(lz.HTML_StorageAccount) \
                    --query "primaryEndpoints.blob" -o tsv)
                    echo "Success! $(lz.HTML_StorageAccount) is $blob_endpoint"
                
                    echo "Create storage container html-template if it does not exist"
                    az storage container create \
                    --account-name $(lz.HTML_StorageAccount) \
                    --name html-template \
                    --account-key $storage_account_key \
                    --blob-endpoint $blob_endpoint 
                    echo "Success! html-template container created!"

                    echo "Uploading HTML templates to blob storage $(HTML_StorageAccount)/html-template"
                    az storage blob upload-batch \
                    --account-name $(lz.HTML_StorageAccount) \
                    --account-key $storage_account_key \
                    --destination html-template \
                    --source '$(Build.SourcesDirectory)/HTML_Templates/' \
                    --pattern '*.html' \
                    --overwrite true
                    echo "Success! HTML templates imported to $(lz.HTML_StorageAccount)/html-template"

                displayName: 'Storage account and storage container configuration'
    
    - task: AzureFunctionApp@2
      inputs: 
        connectedServiceNameARM: 'DTS-DATAINGEST-SBOX'
        appType: 'functionAppLinux'
        appName: 'af-joh-sbox02-uks-dlrm-01'
        package: '$(Pipeline.Workspace)/functionapp'
        deploymentMethod: 'auto'
      displayName: 'Deploy Azure Function to af-joh-sbox02-uks-dlrm-01'

    - task: UsePythonVersion@0
      displayName: 'Use Python 3.x'
      inputs:
        versionSpec: '3.x'

    - script: |
          /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
          eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"
          
          brew install databricks/tap/databricks
          databricks --version
      displayName: 'Install Databricks CLI'

      #Ensure databrcks CLI is available in current session (loading homebrew shell)
    - script: |
        eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"  
        databricks --version  
        echo "[DEFAULT]" > $(HOME)/.databrickscfg
        echo "host = $(DATABRICKS_HOST_WORKSPACE_MAIN02)" >> $(HOME)/.databrickscfg
        echo "token = $(DATABRICKS_TOKEN_WORKSPACE_MAIN02)" >> $(HOME)/.databrickscfg
        cat ~/.databrickscfg  # Print the contents for debugging
      displayName: 'Configure Databricks CLI for Workspace MAIN02'

    - script: |
        eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"  

        set -e 

        WHEEL_FILE=$(find '$(Pipeline.Workspace)' -name "*.whl" | tail -n 1)

        if [ -n "$WHEEL_FILE" ]; then
          echo "Found wheel file: $WHEEL_FILE"

          FILENAME=$(basename "$WHEEL_FILE")
          echo "Extracted filename: $FILENAME"

          DEST_PATH="dbfs:/FileStore/shared_wheels/$FILENAME"
          echo "Destination path: $DEST_PATH"

          echo "creating directory dbfs:/FileStore/shared_wheels"
          databricks fs mkdirs dbfs:/FileStore/shared_wheels    

          echo "copying wheel file to DBFS..."
          echo "copying $WHEEL_FILE to $DEST_PATH"
          databricks fs cp "$WHEEL_FILE" "$DEST_PATH" --overwrite --debug

          echo "Displaying content of dbfs:/FileStore/shared_wheels"
          databricks fs ls dbfs:/FileStore/shared_wheels

        else
          echo "No wheel file found"
          exit 1
        fi
      displayName: 'Upload Wheel to Databricks DBFS'
        
    - script: |
        eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)" 
        KEYVAULT_NAME="ingest02-meta002-sbox"

        echo "Query databricks secrets inside KeyVault backed scope (ingest00-meta002-sbox)"
        databricks secrets list-secrets $KEYVAULT_NAME
        databricks secrets list-secrets ingest00-meta002-sbox
      displayName: 'View KeyVault backed secret keys in Databricks'
    
    - script: |
        echo "Current dir: $(pwd)"
        ls -al /home/vsts/work/1/s
        eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)" 

        cd ARIA_DABs

        echo "Current dir: $(pwd)"
        ls -al  

        databricks bundle destroy -t sandbox 
        databricks bundle validate -t sandbox --debug
        databricks bundle deploy -t sandbox --debug

      #  databricks bundle run -t sandbox db_wf_joh_recieve_job
      displayName: "Deploy Databricks resources to ingest02 workspace"