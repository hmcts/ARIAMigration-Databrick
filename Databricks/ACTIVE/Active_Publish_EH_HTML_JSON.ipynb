{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "446e282f-e690-438d-ba84-041d67ad0e39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install confluent-kafka #required by job cluster until we deploy via DABs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6d3662f-e541-49ef-927f-1e7c6ae27334",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame, Row\n",
    "from pyspark.sql.functions import col, decode, split, element_at, lit, from_json, regexp_replace, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "from confluent_kafka import Producer\n",
    "\n",
    "import json\n",
    "from itertools import islice\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0772806f-f2ca-4fc6-bf4c-f8784d5dabc4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initialise logging"
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"DatabricksWorkflow\")\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "if not logger.hasHandlers():\n",
    "    logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84acefcc-8861-4465-b331-f740d160460e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set-up configs"
    }
   },
   "outputs": [],
   "source": [
    "# --- Load configuration JSON ---\n",
    "config_path = \"dbfs:/configs/config.json\"\n",
    "try:\n",
    "    config = spark.read.option(\"multiline\", \"true\").json(config_path)\n",
    "    logger.info(f\"Successfully read config file from {config_path}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not read config file at {config_path}: {e}\", exc_info=True)\n",
    "    raise FileNotFoundError(f\"Could not read config file at {config_path}: {e}\")\n",
    "\n",
    "# --- Extract environment and lz_key ---\n",
    "try:\n",
    "    first_row = config.first()\n",
    "    env = first_row[\"env\"].strip().lower()\n",
    "    lz_key = first_row[\"lz_key\"].strip().lower()\n",
    "    logger.info(f\"Extracted configs: env={env}, lz_key={lz_key}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Missing expected keys 'env' or 'lz_key' in config file: {e}\", exc_info=True)\n",
    "    raise KeyError(f\"Missing expected keys 'env' or 'lz_key' in config file: {e}\")\n",
    "\n",
    "# --- Construct keyvault name ---\n",
    "try:\n",
    "    keyvault_name = f\"ingest{lz_key}-meta002-{env}\"\n",
    "    logger.info(f\"Constructed keyvault name: {keyvault_name}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error constructing keyvault name: {e}\", exc_info=True)\n",
    "    raise ValueError(f\"Error constructing keyvault name: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "688eadb3-029f-4141-8725-2fb843088687",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set-up OAuth"
    }
   },
   "outputs": [],
   "source": [
    "# --- Access the Service Principal secrets from Key Vault ---\n",
    "try:\n",
    "    client_secret = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-SECRET')\n",
    "    logger.info(\"Successfully retrieved SERVICE-PRINCIPLE-CLIENT-SECRET from Key Vault\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not retrieve 'SERVICE-PRINCIPLE-CLIENT-SECRET' from Key Vault '{keyvault_name}': {e}\", exc_info=True)\n",
    "    raise KeyError(f\"Could not retrieve 'SERVICE-PRINCIPLE-CLIENT-SECRET' from Key Vault '{keyvault_name}': {e}\")\n",
    "\n",
    "try:\n",
    "    tenant_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-TENANT-ID')\n",
    "    logger.info(\"Successfully retrieved SERVICE-PRINCIPLE-TENANT-ID from Key Vault\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not retrieve 'SERVICE-PRINCIPLE-TENANT-ID' from Key Vault '{keyvault_name}': {e}\", exc_info=True)\n",
    "    raise KeyError(f\"Could not retrieve 'SERVICE-PRINCIPLE-TENANT-ID' from Key Vault '{keyvault_name}': {e}\")\n",
    "\n",
    "try:\n",
    "    client_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-ID')\n",
    "    logger.info(\"Successfully retrieved SERVICE-PRINCIPLE-CLIENT-ID from Key Vault\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not retrieve 'SERVICE-PRINCIPLE-CLIENT-ID' from Key Vault '{keyvault_name}': {e}\", exc_info=True)\n",
    "    raise KeyError(f\"Could not retrieve 'SERVICE-PRINCIPLE-CLIENT-ID' from Key Vault '{keyvault_name}': {e}\")\n",
    "\n",
    "logger.info(\"✅ Successfully retrieved all Service Principal secrets from Key Vault\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f7d048b-1249-4316-b286-3f5e61aab6f6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Assign OAuth"
    }
   },
   "outputs": [],
   "source": [
    "# --- Parameterise containers ---\n",
    "curated_storage_account = f\"ingest{lz_key}curated{env}\"\n",
    "curated_container = \"gold\"\n",
    "silver_curated_container = \"silver\"\n",
    "checkpoint_storage_account = f\"ingest{lz_key}xcutting{env}\"\n",
    "\n",
    "# --- Assign OAuth to storage accounts ---\n",
    "storage_accounts = [curated_storage_account, checkpoint_storage_account]\n",
    "\n",
    "for storage_account in storage_accounts:\n",
    "    try:\n",
    "        configs = {\n",
    "            f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\": \"OAuth\",\n",
    "            f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\":\n",
    "                \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "            f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\": client_id,\n",
    "            f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\": client_secret,\n",
    "            f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\":\n",
    "                f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "        }\n",
    "\n",
    "        for key, val in configs.items():\n",
    "            try:\n",
    "                spark.conf.set(key, val)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to set Spark config '{key}' for storage account '{storage_account}': {e}\", exc_info=True)\n",
    "                raise RuntimeError(f\"Failed to set Spark config '{key}' for storage account '{storage_account}': {e}\")\n",
    "\n",
    "        logger.info(f\"✅ Successfully configured OAuth for storage account: {storage_account}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error configuring OAuth for storage account '{storage_account}': {e}\", exc_info=True)\n",
    "        raise RuntimeError(f\"Error configuring OAuth for storage account '{storage_account}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e29f790a-cbd2-496f-ab33-a6945d7dd415",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Kafka configuration"
    }
   },
   "outputs": [],
   "source": [
    "eh_kv_secret = dbutils.secrets.get(scope=keyvault_name, key=\"RootManageSharedAccessKey\")\n",
    "\n",
    "# Event Hub configurations\n",
    "eventhubs_hostname = f\"ingest{lz_key}-integration-eventHubNamespace001-{env}.servicebus.windows.net:9093\"\n",
    "conf = {\n",
    "    'bootstrap.servers': eventhubs_hostname,\n",
    "    'security.protocol': 'SASL_SSL',\n",
    "    'sasl.mechanism': 'PLAIN',\n",
    "    'sasl.username': '$ConnectionString',\n",
    "    'sasl.password': eh_kv_secret,\n",
    "    'retries': 5,                     # Increased retries\n",
    "    'enable.idempotence': True,        # Enable idempotent producer #confirm use with ara\n",
    "}\n",
    "broadcast_conf = sc.broadcast(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b0f638a-1d76-42e9-8d06-23ec8c53d7a7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "New Kafka"
    }
   },
   "outputs": [],
   "source": [
    "# --- Define schema for result DataFrame ---\n",
    "result_schema = StructType([\n",
    "    StructField(\"RunID\", StringType(), True),\n",
    "    StructField(\"CaseNo\", StringType(), True),\n",
    "    StructField(\"Filename\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"PublishingDateTime\", StringType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"Error\", StringType(), True)\n",
    "])\n",
    "\n",
    "# --- Widgets and state ---\n",
    "dbutils.widgets.text(\"state\", \"default\", \"State to Process\")\n",
    "state = dbutils.widgets.get(\"state\")\n",
    "logger.info(f\"🔄 Processing state: {state}\")\n",
    "\n",
    "# --- Define paths ---\n",
    "curated_storage_account = f\"ingest{lz_key}curated{env}\"\n",
    "curated_container = \"gold\"\n",
    "silver_curated_container = \"silver\"\n",
    "\n",
    "gold_files_base_path = f\"abfss://{curated_container}@{curated_storage_account}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/{state}/\"\n",
    "silver_base_path = f\"abfss://{silver_curated_container}@{curated_storage_account}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/{state}/publish_audit_db_eh\"\n",
    "\n",
    "# --- Load files ---\n",
    "try:\n",
    "    files = dbutils.fs.ls(gold_files_base_path)[-1]  # newest file\n",
    "    valid_json = files.path + \"JSON/\"\n",
    "    logger.info(f\"📂 Valid JSON path: {valid_json}\")\n",
    "\n",
    "    try:\n",
    "        dbutils.fs.ls(valid_json)\n",
    "    except Exception:\n",
    "        logger.warning(f\"ℹ️ No VALID_JSON directory found for state: {state}\")\n",
    "    else:\n",
    "        # Load binary files\n",
    "        binary_df = (\n",
    "            spark.read.format('binaryFile')\n",
    "            .option('pathGlobFilter', '*.{html,json}')\n",
    "            .option('recursiveFileLookup', 'true')\n",
    "            .load(valid_json)\n",
    "        )\n",
    "\n",
    "        # Generate unique RunID per batch\n",
    "        try:\n",
    "            logger.info(\"Attempting to get Databricks context...\")\n",
    "\n",
    "            # Get the context JSON (string)\n",
    "            context_str = dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson()\n",
    "            logger.debug(f\"Raw context JSON: {context_str}\")\n",
    "\n",
    "            # Parse JSON into a dict\n",
    "            context = json.loads(context_str)\n",
    "            tags = context.get(\"tags\", {})\n",
    "\n",
    "            # Pull jobRunId directly\n",
    "            run_id = tags.get(\"jobRunId\")\n",
    "            if run_id:\n",
    "                logger.info(f\"Using jobRunId from tags: {run_id}\")\n",
    "            else:\n",
    "                logger.warning(\"jobRunId not found in tags!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Exception retrieving Databricks jobRunId: {e}\")\n",
    "            run_id = None        \n",
    "        \n",
    "        # Transform dataframe\n",
    "        html_df = (\n",
    "            binary_df\n",
    "            .withColumn(\"content_str\", decode(col('content'), 'utf-8'))\n",
    "            .withColumn(\"file_path\", element_at(split(col('path'), '/'), -1))\n",
    "            .withColumn(\"State\", lit(state))\n",
    "            .withColumn(\"CaseNo\", regexp_replace(col(\"file_path\"), r\"\\.json$\", \"\"))\n",
    "            .withColumn(\"RunID\", lit(run_id))\n",
    "            .withColumn(\"PublishingDateTime\", current_timestamp())\n",
    "            .select('RunID', 'CaseNo', 'content_str', 'file_path', 'State')\n",
    "        )\n",
    "\n",
    "        display(html_df)\n",
    "\n",
    "        record_count = html_df.count()\n",
    "        if record_count == 0:\n",
    "            print(f\"ℹ️ No data to process for state: {state}\")\n",
    "        else:\n",
    "            print(f\"📊 Found {record_count} records for state: {state}\")\n",
    "            optimized_html_df = html_df.repartition(1)\n",
    "\n",
    "            # --- Partition processing ---\n",
    "            def process_partition(partition):\n",
    "                import logging\n",
    "                from confluent_kafka import Producer\n",
    "                from datetime import datetime\n",
    "\n",
    "                success_list = []\n",
    "                failure_list = []\n",
    "\n",
    "                producer = Producer(**broadcast_conf.value)\n",
    "\n",
    "                for row in partition:\n",
    "                    if row.file_path is None or row.content_str is None:\n",
    "                        logger.warning(f\"Skipping row with missing file_path/content_str: {row}\")\n",
    "                        continue\n",
    "\n",
    "                    current_CaseNo = row.CaseNo\n",
    "                    current_state_row = row.State\n",
    "                    current_RunID = row.RunID\n",
    "                    current_file_path = row.file_path\n",
    "\n",
    "                    # --- Closure for callback to capture row-specific variables ---\n",
    "                    def make_delivery_report(case_no, state, run_id):\n",
    "                        def delivery_report(err, msg):\n",
    "                            key_str = msg.key().decode('utf-8') if msg.key() else \"Unknown\"\n",
    "                            timestamp = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "                            if err:\n",
    "                                failure_list.append((run_id, case_no, key_str, state, timestamp, \"ERROR\", str(err)))\n",
    "                                logger.error(f\"Message delivery failed for {case_no}: {err}\")\n",
    "                            else:\n",
    "                                success_list.append((run_id, case_no, key_str, state, timestamp, \"SUCCESS\", \"\"))\n",
    "                                logger.info(f\"Message delivered successfully for {case_no}\")\n",
    "                        return delivery_report\n",
    "\n",
    "                    delivery_callback = make_delivery_report(current_CaseNo, current_state_row, current_RunID)\n",
    "\n",
    "                    # --- Produce to Kafka ---\n",
    "                    try:\n",
    "                        if isinstance(row.content_str, str):\n",
    "                            value = json.dumps({     #value=row.content_str\n",
    "                                \"RunID\": current_RunID,\n",
    "                                \"CaseNo\": current_CaseNo,\n",
    "                                \"State\": current_state_row,\n",
    "                                \"Filename\": current_file_path,\n",
    "                                \"Content\": row.content_str\n",
    "                            }).encode('utf-8')\n",
    "                            \n",
    "                        elif isinstance(row.content_str, (bytes, bytearray)):\n",
    "                            value = json.dumps({\n",
    "                                        \"RunID\": current_RunID,\n",
    "                                        \"CaseNo\": current_CaseNo,\n",
    "                                        \"State\": current_state_row,\n",
    "                                        \"Filename\": current_file_path,\n",
    "                                        \"Content\": row.content_str.decode('utf-8', errors='ignore')\n",
    "                                    }).encode('utf-8')\n",
    "                        else:\n",
    "                            failure_list.append((current_RunID, current_CaseNo, \"Unknown\", current_state_row, datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"), \"ERROR\", f\"Unsupported type {type(row.content_str)}\"))\n",
    "                            continue\n",
    "\n",
    "                        try: \n",
    "                            producer.produce(\n",
    "                            topic=f'evh-active-pub-{lz_key}-uks-dlrm-01',\n",
    "                            key=current_file_path.encode('utf-8'),\n",
    "                            value=value,\n",
    "                            callback=delivery_callback\n",
    "                            )\n",
    "                        \n",
    "                        except KafkaException as e:\n",
    "                            logger.error(f\"Kafka produce failed (check connectivity!): {e}\")\n",
    "\n",
    "                    except BufferError:\n",
    "                        logger.error(\"Producer buffer full.\")\n",
    "\n",
    "                # Flush producer at the end of partition\n",
    "                try:\n",
    "                    producer.flush()\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Flush error: {e}\")\n",
    "\n",
    "                # Merge results\n",
    "                results = success_list + failure_list\n",
    "                return results\n",
    "\n",
    "            # --- Map partitions and collect results ---\n",
    "            result_rdd = optimized_html_df.rdd.mapPartitions(process_partition)\n",
    "            result_df = spark.createDataFrame(result_rdd, result_schema)\n",
    "\n",
    "            # --- Write results incrementally to Delta ---\n",
    "            result_df.write.format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .option(\"mergeSchema\", \"true\") \\\n",
    "                .save(silver_base_path)\n",
    "\n",
    "            # --- Display results ---\n",
    "            display(result_df.select(\"RunID\", \"CaseNo\", \"State\", \"PublishingDateTime\", \"Status\", \"Error\"))\n",
    "\n",
    "            # Highlight failures\n",
    "            failed_df = result_df.filter(col(\"Status\") == \"ERROR\")\n",
    "            failed_count = failed_df.count()\n",
    "            if failed_count > 0:\n",
    "                logger.error(f\"⚠️ Found {failed_count} failed records for state: {state}\")\n",
    "                display(failed_df.select(\"RunID\", \"CaseNo\", \"State\", \"PublishingDateTime\", \"Status\", \"Error\"))\n",
    "            else:\n",
    "                logger.info(f\"✅ No failed records for state: {state}\")\n",
    "\n",
    "            kafka_result_count = result_df.count()\n",
    "            logger.info(f\"📊 Kafka processing completed: {kafka_result_count} records for state: {state}\")\n",
    "            logger.info(f\"✅ Successfully sent {record_count} records to Kafka for state: {state}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"❌ Error processing state {state}: {e}\")\n",
    "\n",
    "logger.info(f\"🎉 Completed processing for state: {state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65e95e6e-4b55-4307-9ca5-e928303fcbdc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Original Kafka"
    }
   },
   "outputs": [],
   "source": [
    "# --- Define schema for result DataFrame ---\n",
    "result_schema = StructType([\n",
    "    StructField(\"RunID\", StringType(), True),\n",
    "    StructField(\"CaseNo\", StringType(), True),\n",
    "    StructField(\"Filename\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"PublishingDateTime\", StringType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"Error\", StringType(), True)\n",
    "])\n",
    "\n",
    "# --- Widgets and state ---\n",
    "dbutils.widgets.text(\"state\", \"default\", \"State to Process\")\n",
    "state = dbutils.widgets.get(\"state\")\n",
    "logger.info(f\"🔄 Processing state: {state}\")\n",
    "\n",
    "# --- Define paths ---\n",
    "curated_storage_account = f\"ingest{lz_key}curated{env}\"\n",
    "curated_container = \"gold\"\n",
    "silver_curated_container = \"silver\"\n",
    "\n",
    "gold_files_base_path = f\"abfss://{curated_container}@{curated_storage_account}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/{state}/\"\n",
    "silver_base_path = f\"abfss://{silver_curated_container}@{curated_storage_account}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/{state}/publish_audit_db_eh\"\n",
    "\n",
    "# --- Load files ---\n",
    "try:\n",
    "    files = dbutils.fs.ls(gold_files_base_path)[-1]  # newest file\n",
    "    valid_json = files.path + \"JSON/\"\n",
    "    logger.info(f\"📂 Valid JSON path: {valid_json}\")\n",
    "\n",
    "    try:\n",
    "        dbutils.fs.ls(valid_json)\n",
    "    except Exception:\n",
    "        logger.warning(f\"ℹ️ No VALID_JSON directory found for state: {state}\")\n",
    "    else:\n",
    "        # Load binary files\n",
    "        binary_df = (\n",
    "            spark.read.format('binaryFile')\n",
    "            .option('pathGlobFilter', '*.{html,json}')\n",
    "            .option('recursiveFileLookup', 'true')\n",
    "            .load(valid_json)\n",
    "        )\n",
    "\n",
    "        # Generate unique RunID per batch\n",
    "        try:\n",
    "            logger.info(\"Attempting to get Databricks context...\")\n",
    "\n",
    "            # Get the context JSON (string)\n",
    "            context_str = dbutils.notebook.entry_point.getDbutils().notebook().getContext().toJson()\n",
    "            logger.debug(f\"Raw context JSON: {context_str}\")\n",
    "\n",
    "            # Parse JSON into a dict\n",
    "            context = json.loads(context_str)\n",
    "            tags = context.get(\"tags\", {})\n",
    "\n",
    "            # Pull jobRunId directly\n",
    "            run_id = tags.get(\"jobRunId\")\n",
    "            if run_id:\n",
    "                logger.info(f\"Using jobRunId from tags: {run_id}\")\n",
    "            else:\n",
    "                logger.warning(\"jobRunId not found in tags!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Exception retrieving Databricks jobRunId: {e}\")\n",
    "            run_id = None        \n",
    "        \n",
    "        # Transform dataframe\n",
    "        html_df = (\n",
    "            binary_df\n",
    "            .withColumn(\"content_str\", decode(col('content'), 'utf-8'))\n",
    "            .withColumn(\"file_path\", element_at(split(col('path'), '/'), -1))\n",
    "            .withColumn(\"State\", lit(state))\n",
    "            .withColumn(\"CaseNo\", regexp_replace(col(\"file_path\"), r\"\\.json$\", \"\"))\n",
    "            .withColumn(\"RunID\", lit(run_id))\n",
    "            .withColumn(\"PublishingDateTime\", current_timestamp())\n",
    "            .select('RunID', 'CaseNo', 'content_str', 'file_path', 'State')\n",
    "        )\n",
    "\n",
    "        record_count = html_df.count()\n",
    "        if record_count == 0:\n",
    "            print(f\"ℹ️ No data to process for state: {state}\")\n",
    "        else:\n",
    "            print(f\"📊 Found {record_count} records for state: {state}\")\n",
    "            optimized_html_df = html_df.repartition(1)\n",
    "\n",
    "            # --- Partition processing ---\n",
    "            def process_partition(partition):\n",
    "                import logging\n",
    "                from confluent_kafka import Producer\n",
    "                from datetime import datetime\n",
    "\n",
    "                success_list = []\n",
    "                failure_list = []\n",
    "\n",
    "                producer = Producer(**broadcast_conf.value)\n",
    "\n",
    "                for row in partition:\n",
    "                    if row.file_path is None or row.content_str is None:\n",
    "                        logger.warning(f\"Skipping row with missing file_path/content_str: {row}\")\n",
    "                        continue\n",
    "\n",
    "                    current_CaseNo = row.CaseNo\n",
    "                    current_state_row = row.State\n",
    "                    current_RunID = row.RunID\n",
    "                    current_file_path = row.file_path\n",
    "\n",
    "                    # --- Closure for callback to capture row-specific variables ---\n",
    "                    def make_delivery_report(case_no, state, run_id):\n",
    "                        def delivery_report(err, msg):\n",
    "                            key_str = msg.key().decode('utf-8') if msg.key() else \"Unknown\"\n",
    "                            timestamp = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "                            if err:\n",
    "                                failure_list.append((run_id, case_no, key_str, state, timestamp, \"ERROR\", str(err)))\n",
    "                                logger.error(f\"Message delivery failed for {case_no}: {err}\")\n",
    "                            else:\n",
    "                                success_list.append((run_id, case_no, key_str, state, timestamp, \"SUCCESS\", \"\"))\n",
    "                                logger.info(f\"Message delivered successfully for {case_no}\")\n",
    "                        return delivery_report\n",
    "\n",
    "                    delivery_callback = make_delivery_report(current_CaseNo, current_state_row, current_RunID)\n",
    "\n",
    "                    # --- Produce to Kafka ---\n",
    "                    try:\n",
    "                        if isinstance(row.content_str, str):\n",
    "                            value = row.content_str.encode('utf-8')\n",
    "                        elif isinstance(row.content_str, (bytes, bytearray)):\n",
    "                            value = bytes(row.content_str)\n",
    "                        else:\n",
    "                            failure_list.append((current_RunID, current_CaseNo, \"Unknown\", current_state_row, datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S.%f\"), \"ERROR\", f\"Unsupported type {type(row.content_str)}\"))\n",
    "                            continue\n",
    "\n",
    "                        try: \n",
    "                            producer.produce(\n",
    "                            topic=f'evh-active-pub-{lz_key}-uks-dlrm-01',\n",
    "                            key=current_file_path.encode('utf-8'),\n",
    "                            value=value,\n",
    "                            callback=delivery_callback\n",
    "                            )\n",
    "                        \n",
    "                        except KafkaException as e:\n",
    "                            logger.error(f\"Kafka produce failed (check connectivity!): {e}\")\n",
    "\n",
    "                    except BufferError:\n",
    "                        logger.error(\"Producer buffer full.\")\n",
    "\n",
    "                # Flush producer at the end of partition\n",
    "                try:\n",
    "                    producer.flush()\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Flush error: {e}\")\n",
    "\n",
    "                # Merge results\n",
    "                results = success_list + failure_list\n",
    "                return results\n",
    "\n",
    "            # Map partitions and collect results \n",
    "            result_rdd = optimized_html_df.rdd.mapPartitions(process_partition)\n",
    "            result_df = spark.createDataFrame(result_rdd, result_schema)\n",
    "\n",
    "            # Write results incrementally to Delta\n",
    "            result_df.write.format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .option(\"mergeSchema\", \"true\") \\\n",
    "                .save(silver_base_path)\n",
    "\n",
    "            display(result_df.select(\"RunID\", \"CaseNo\", \"State\", \"PublishingDateTime\", \"Status\", \"Error\"))\n",
    "\n",
    "            # Highlight failures\n",
    "            failed_df = result_df.filter(col(\"Status\") == \"ERROR\")\n",
    "            failed_count = failed_df.count()\n",
    "            if failed_count > 0:\n",
    "                logger.error(f\"⚠️ Found {failed_count} failed records for state: {state}\")\n",
    "                display(failed_df.select(\"RunID\", \"CaseNo\", \"State\", \"PublishingDateTime\", \"Status\", \"Error\"))\n",
    "            else:\n",
    "                logger.info(f\"✅ No failed records for state: {state}\")\n",
    "\n",
    "            kafka_result_count = result_df.count()\n",
    "            logger.info(f\"📊 Kafka processing completed: {kafka_result_count} records for state: {state}\")\n",
    "            logger.info(f\"✅ Successfully sent {record_count} records to Kafka for state: {state}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"❌ Error processing state {state}: {e}\")\n",
    "\n",
    "logger.info(f\"🎉 Completed processing for state: {state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "753f6003-b42f-4af3-a5a1-f870a8993e1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "publishPayloadResult = spark.read.format(\"delta\"\n",
    "                ).load(f\"abfss://{silver_curated_container}@{curated_storage_account}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/{state}/publish_audit_db_eh\")\n",
    "\n",
    "publishPayloadResult.write.format(\"delta\"\n",
    "                ).mode(\"append\"\n",
    "                ).save(f\"abfss://{silver_curated_container}@{curated_storage_account}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/all_states_combined/publish_audit_db_eh\")\n",
    "\n",
    "publishPayloadResult.createOrReplaceTempView(\"publishPayloadResult\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "767ff8ab-2c5d-4c8f-bdaf-d1dfd1e247e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM publishPayloadResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5614f6fe-1de8-4af6-9359-c7d4e584fad4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(f\"{state} notebook completed successfully\")\n",
    "logger.info(f\"🎉 Completed processing for state: {state}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7323413295938318,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Active_Publish_EH_HTML_JSON",
   "widgets": {
    "state": {
     "currentValue": "paymentPending",
     "nuid": "165e7609-00d9-4c0d-b090-f52f842d50af",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default",
      "label": "State to Process",
      "name": "state",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "default",
      "label": "State to Process",
      "name": "state",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
