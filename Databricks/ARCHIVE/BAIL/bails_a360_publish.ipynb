{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "971d01f4-9654-4671-91ff-b2b6e49d88bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "from itertools import islice\n",
    "from confluent_kafka import Producer\n",
    "import logging\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import col, decode, split, element_at, udf, when, col, element_at, lit, concat, upper\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d4d8f92-b90f-4d9c-a0b6-178e49c23c3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Set up globals [configs](url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95d3f69b-14cd-43f1-9bf7-4fd6665454a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/configs/config.json\")\n",
    "env = config.first()[\"env\"].strip().lower()\n",
    "lz_key = config.first()[\"lz_key\"].strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0434066-6b1f-4b5b-939d-5224305fae53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "keyvault_name = f\"ingest{lz_key}-meta002-{env}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f898702-2640-4f01-9b46-3b3844c8fa4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Access the Service Principle secrets from keyvaults\n",
    "client_secret = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-SECRET')\n",
    "tenant_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-TENANT-ID')\n",
    "client_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13ea4498-156c-41ce-94b9-41b3a296b165",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "curated_storage_account = f\"ingest{lz_key}curated{env}\"\n",
    "gold_curated_container = \"gold\"\n",
    "silver_curated_container = \"silver\"\n",
    "segment = \"BAILS\"\n",
    "segment_short = \"bl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "793430e7-a115-45c9-84d5-132c6727c596",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_accounts = [curated_storage_account]\n",
    "\n",
    "for storage_account in storage_accounts:\n",
    "    configs = {\n",
    "            f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\": \"OAuth\",\n",
    "            f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\":\n",
    "                \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "            f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\": client_id,\n",
    "            f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\": client_secret,\n",
    "            f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\":\n",
    "                f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "        }\n",
    "    for key,val in configs.items():\n",
    "        spark.conf.set(key,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99b868a1-afbe-44fa-aefa-b0f043b536d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print out the auth config for each storage account to confirm\n",
    "for storage_account in storage_accounts:\n",
    "    key = f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\"\n",
    "    print(f\"{key}: {spark.conf.get(key, 'MISSING')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c582a0c0-4950-4928-b3cb-635b709a137b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_files_base_path = f\"abfss://{gold_curated_container}@{curated_storage_account}.dfs.core.windows.net/ARIADM/ARM/{segment}/\"\n",
    "\n",
    "silver_base_path = f\"abfss://{silver_curated_container}@{curated_storage_account}.dfs.core.windows.net\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca5d0732-52ee-47db-8fc2-e2cd9efaac84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read in HTML and JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "625208fc-4823-45f7-8e56-5338370ef88a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eh_kv_secret = dbutils.secrets.get(scope=keyvault_name, key=\"RootManageSharedAccessKey\")\n",
    "\n",
    "# Event Hub configurations\n",
    "eventhubs_hostname = f\"ingest{lz_key}-integration-eventHubNamespace001-{env}.servicebus.windows.net:9093\"\n",
    "conf = {\n",
    "    'bootstrap.servers': eventhubs_hostname,\n",
    "    'security.protocol': 'SASL_SSL',\n",
    "    'sasl.mechanism': 'PLAIN',\n",
    "    'sasl.username': '$ConnectionString',\n",
    "    'sasl.password': eh_kv_secret,\n",
    "    'retries': 5,                     # Increased retries\n",
    "    'enable.idempotence': True,        # Enable idempotent producer\n",
    "}\n",
    "broadcast_conf = sc.broadcast(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24c2aef4-dec0-43e7-8d12-c118e0c7a812",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_9a17bd9\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_721aac06\",\"enabled\":true,\"columnId\":\"file_path\",\"dataType\":\"string\",\"filterType\":\"oneof\",\"filterValues\":[]}],\"local\":false,\"updatedAt\":1741697475693}],\"syncTimestamp\":1741697475693}",
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "binary_df = spark.read.format('binaryFile') \\\n",
    "                     .option('pathGlobFilter', '*.a360') \\\n",
    "                     .option('recursiveFileLookup', 'true') \\\n",
    "                     .load(gold_files_base_path)\n",
    "\n",
    "html_df = binary_df.withColumn(\"content_str\", decode(col('content'), 'utf-8')) \\\n",
    "                   .withColumn('file_path', element_at(split(col('path'), '/'), -1))\n",
    "html_df = html_df.select('content_str','file_path')\n",
    "\n",
    "html_df = html_df.withColumn(\n",
    "    \"suffix\",\n",
    "    when(col(\"file_path\").endswith(\"html\"), lit(\"HTML\"))\n",
    "    .when(col(\"file_path\").endswith(\"json\"), lit(\"JSON\"))\n",
    "    .when(col(\"file_path\").endswith(\"a360\"), lit(\"A360\"))\n",
    ").withColumn(\n",
    "    \"blob_url\",\n",
    "    concat(\n",
    "        lit(\"https://ingest\"),\n",
    "        lit(lz_key),\n",
    "        lit(\"curated\"),\n",
    "        lit(env),\n",
    "        lit(f\".blob.core.windows.net/gold/ARIADM/ARM/{segment}/\"),\n",
    "        upper(col(\"suffix\")), lit(\"/\"),\n",
    "        col(\"file_path\")\n",
    "    )\n",
    ").drop(\"content_str\")\n",
    "\n",
    "html_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d99f4b22-2d4c-4550-9959-219d94d18973",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Send to EventHubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e6df5e0-b6ae-481f-a221-a7bcd2a64141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Repartition based on cluster resources\n",
    "num_spark_partitions =  8\n",
    "optimized_html_df = html_df.repartition(num_spark_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cacfef9-1522-43f8-ad26-82864f1c6bce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_partition(partition):\n",
    "    import logging\n",
    "    from confluent_kafka import Producer\n",
    "    from datetime import datetime\n",
    "\n",
    "    # Initialize logger\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger('KafkaProducer')\n",
    "    \n",
    "    failure_list = []\n",
    "    success_list = []\n",
    "    results = []\n",
    "\n",
    "    # Initialize producer\n",
    "    producer = Producer(**broadcast_conf.value)\n",
    "\n",
    "    def delivery_report(err, msg):\n",
    "        key_str = msg.key().decode('utf-8') if msg.key() is not None else \"Unknown\"\n",
    "        if err is not None:\n",
    "            err_msg = str(err)\n",
    "            logger.error(f\"Message delivery failed for key {key_str}: {err}\")\n",
    "            failure_list.append((key_str, \"failure\", err_msg, datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "        else:\n",
    "            success_list.append((key_str, \"success\", \"\", datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "\n",
    "    for row in partition:\n",
    "        if row.file_path is None or row.blob_url is None:\n",
    "            logger.warning(f\"Skipping row with missing file_path/blob_url: {row}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            value = row.blob_url.encode('utf-8')\n",
    "            producer.produce(\n",
    "                topic=f'evh-{segment_short}-pub-{lz_key}-uks-dlrm-01',\n",
    "                key=row.file_path.encode('utf-8'),\n",
    "                value=value,\n",
    "                callback=delivery_report\n",
    "            )\n",
    "\n",
    "        except BufferError:\n",
    "            logger.error(\"Producer buffer full. Polling for events.\")\n",
    "            producer.poll(1)  \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error during production: {e}\")\n",
    "            failure_list.append((row.file_path, \"failure\", str(e), datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "\n",
    "    try:\n",
    "        producer.flush()\n",
    "        logger.info(\"Producer flushed successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during flush: {e}\")\n",
    "\n",
    "    # Append results to list instead of using yield\n",
    "    results.extend(success_list)\n",
    "    results.extend(failure_list)\n",
    "\n",
    "    return results  # Return list instead of using yield\n",
    "\n",
    "# Schema for result DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"file_name\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"error_message\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Apply the optimized processing\n",
    "result_rdd = optimized_html_df.rdd.mapPartitions(process_partition).collect() \n",
    "\n",
    "# Create DataFrame and show results\n",
    "result_df = spark.createDataFrame(result_rdd, schema)\n",
    "display(result_df)  # Debugging step to verify output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ea0014f-d416-470f-9d74-15337a6d4b32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Display failed files\n",
    "failed_files = result_df.filter(col(\"status\") == \"failure\")\n",
    "\n",
    "display(failed_files)\n",
    "failed_files.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b180a57-f911-4089-b2cc-86f6f3e3e520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df.write.format(\"delta\").mode(\"append\").save(f\"{silver_base_path}/ARIADM/ARM/AUDIT/{segment}/{segment_short}_pub_audit_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00ef0abd-cd75-4332-956a-9afbeb4923ba",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Close the notebook"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Notebook completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bails_a360_publish",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
