trigger:
  branches:
    include:
      - main

pool:
  vmImage: 'ubuntu-latest'

parameters:
  - name: environment_components
    type: object
    default:
      - deployment: sbox_aria
        environment: sbox
        service_connection: DTS-DATAINGEST-SBOX
        landing_zones: 
          - landing_zone: '00'
            databricks_host: 'https://adb-3635282203417052.12.azuredatabricks.net'
            databricks_token: "$(DATABRICKS_TOKEN_WORKSPACE_MAIN00)"
            Client_ID: $(SBOX_CLIENT_ID)
            Subscription_ID: $(SBOX_SUBSCRIPTION_ID)
            HTML_StorageAccount: 'ingest00landingsbox'
            Resource_Group: 'ingest00-main-sbox'
          - landing_zone: '01'
            databricks_host: 'https://adb-376876256300083.3.azuredatabricks.net'
            databricks_token: "$(DATABRICKS_TOKEN_WORKSPACE_MAIN01)"
            Client_ID: $(SBOX_CLIENT_ID)
            Subscription_ID: $(SBOX_SUBSCRIPTION_ID)
            HTML_StorageAccount: 'ingest01landingsbox'
            Resource_Group: 'ingest01-main-sbox'
          - landing_zone: '02'
            databricks_host: 'https://adb-1879076228317698.18.azuredatabricks.net'
            databricks_token: "$(DATABRICKS_TOKEN_WORKSPACE_MAIN02)"
            Client_ID: $(SBOX_CLIENT_ID)
            Subscription_ID: $(SBOX_SUBSCRIPTION_ID)
            HTML_StorageAccount: 'ingest02landingsbox'
            Resource_Group: 'ingest02-main-sbox'

      - deployment: stg_aria
        environment: stg
        service_connection: DTS-DATAINGEST-STG
        dependsOn: sbox_aria
        landing_zones:
          - landing_zone: '00'
            HTML_StorageAccount: 'ingest00landingstg'
            Resource_Group: 'ingest00-main-stg'
            Client_ID: $(STG_CLIENT_ID)
            Subscription_ID: $(STG_SUBSCRIPTION_ID)
          - landing_zone: '01'
            HTML_StorageAccount: 'ingest01landingstg'
            Resource_Group: 'ingest01-main-stg'
            Client_ID: $(STG_CLIENT_ID)
            Subscription_ID: $(STG_SUBSCRIPTION_ID)

      - deployment: prod_aria
        environment: prod
        service_connection: DTS-DATAINGEST-PROD
        dependsOn: stg_aria
        landing_zones: 
          - landing_zone: '00'
            HTML_StorageAccount: 'ingest00landingprod'
            Resource_Group: 'ingest00-main-prod'
            Client_ID: $(PROD_CLIENT_ID)
            Subscription_ID: $(PROD_SUBSCRIPTION_ID)
          - landing_zone: '01'
            HTML_StorageAccount: 'ingest01landingprod'
            Resource_Group: 'ingest01-main-prod'
            Client_ID: $(PROD_CLIENT_ID)
            Subscription_ID: $(PROD_SUBSCRIPTION_ID)

stages:
  - stage: DownloadArtifacts
    displayName: 'Download Build Artifacts'
    jobs:
      - job: DownloadJob
        displayName: 'Download wheel and Azure Function artifacts'
        steps:
          - task: DownloadBuildArtifacts@1
            inputs:
              buildType: 'current'
              downloadType: 'single'
              artifactName: 'ariafunction'
              downloadPath: '$(Pipeline.Workspace)'
            displayName: 'Download Wheel Artifact'

          - task: DownloadBuildArtifacts@1
            inputs:
              buildType: 'current'
              downloadType: 'single'
              artifactName: 'functionapp'
              downloadPath: '$(Pipeline.Workspace)'
            displayName: 'Download Azure Function Artifact'

# Loop over environment deployments
  - ${{ each deployment in parameters.environment_components }}:
    - stage: ${{ deployment.deployment }}
      displayName: 'Deploy to ${{ deployment.environment }}'
      ${{ if ne(deployment.deployment, 'sbox_aria') }}:
        dependsOn: ${{ deployment.dependsOn }}
      jobs:
        - ${{ if or(eq(deployment.environment, 'stg'), eq(deployment.environment, 'prod')) }}:
          - job: waitForValidation
            displayName: Wait for Manual Validation
            pool: server
            steps:
              - task: ManualValidation@1
                timeoutInMinutes: 23160
                inputs:
                  approvers: andrew.mcdevitt@hmcts.net, ara.islam1@hmcts.net
                  instructions: Please validate the build configuration and resume

        # Loop through landing zones in each environment
        - ${{ each lz in deployment.landing_zones }}:
          - job: Deploy_HTML_LZ${{ lz.landing_zone }}
            displayName: Deploy HTML to ADLS - ${{ deployment.environment }} - LZ${{ lz.landing_zone }}
            pool:
              name: ${{ if eq(deployment.environment, 'sbox') }} && hmcts-sds-ptlsbox || 'hmcts-sds-ptl' }}
            steps:
              - task: AzureCLI@2
                displayName: Configure Storage and Upload HTML Templates
                inputs:
                  azureSubscription: ${{ deployment.service_connection }}
                  scriptType: 'bash'
                  scriptLocation: 'inlineScript'
                  inlineScript: |
                    echo "Assigning SP Blob Contributor Access"
                    az role assignment create \
                      --assignee "${{ lz.Client_ID }}" \
                      --role "Storage Blob Data Contributor" \
                      --scope "/subscriptions/${{ lz.Subscription_ID }}/resourceGroups/${{ lz.Resource_Group }}/providers/Microsoft.Storage/storageAccounts/${{ lz.HTML_StorageAccount }}"

                    echo "Retrieving Storage Account Key"
                    storage_account_key=$(az storage account keys list \
                      --resource-group ${{ lz.Resource_Group }} \
                      --account-name ${{ lz.HTML_StorageAccount }} \
                      --query '[0].value' -o tsv)

                    echo "Retrieving Blob Endpoint"
                    blob_endpoint=$(az storage account show \
                      --resource-group ${{ lz.Resource_Group }} \
                      --name ${{ lz.HTML_StorageAccount }} \
                      --query "primaryEndpoints.blob" -o tsv)

                    echo "Creating html-template container if not exists"
                    az storage container create \
                      --account-name ${{ lz.HTML_StorageAccount }} \
                      --name html-template \
                      --account-key $storage_account_key \
                      --blob-endpoint $blob_endpoint

                    echo "Uploading HTML Templates"
                    az storage blob upload-batch \
                      --account-name ${{ lz.HTML_StorageAccount }} \
                      --account-key $storage_account_key \
                      --destination html-template \
                      --source '$(Pipeline.Workspace)/HTML_Templates/' \
                      --pattern '*.html' \
                      --overwrite true
    
    - task: AzureFunctionApp@2
      inputs: 
        connectedServiceNameARM: 'DTS-DATAINGEST-SBOX'
        appType: 'functionAppLinux'
        appName: 'af-joh-sbox02-uks-dlrm-01'
        package: '$(Pipeline.Workspace)/functionapp'
        deploymentMethod: 'auto'
      displayName: 'Deploy Azure Function to af-joh-sbox02-uks-dlrm-01'

    - task: UsePythonVersion@0
      displayName: 'Use Python 3.x'
      inputs:
        versionSpec: '3.x'

    - script: |
          /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
          eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"
          
          brew install databricks/tap/databricks
          databricks --version
      displayName: 'Install Databricks CLI'

      #Ensure databrcks CLI is available in current session (loading homebrew shell)
    - script: |
        eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"  
        databricks --version  
        echo "[DEFAULT]" > $(HOME)/.databrickscfg
        echo "host = $(DATABRICKS_HOST_WORKSPACE_MAIN02)" >> $(HOME)/.databrickscfg
        echo "token = $(DATABRICKS_TOKEN_WORKSPACE_MAIN02)" >> $(HOME)/.databrickscfg
        cat ~/.databrickscfg  # Print the contents for debugging
      displayName: 'Configure Databricks CLI for Workspace MAIN02'

    - script: |
        eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"  

        set -e 

        WHEEL_FILE=$(find '$(Pipeline.Workspace)' -name "*.whl" | tail -n 1)

        if [ -n "$WHEEL_FILE" ]; then
          echo "Found wheel file: $WHEEL_FILE"

          FILENAME=$(basename "$WHEEL_FILE")
          echo "Extracted filename: $FILENAME"

          DEST_PATH="dbfs:/FileStore/shared_wheels/$FILENAME"
          echo "Destination path: $DEST_PATH"

          echo "creating directory dbfs:/FileStore/shared_wheels"
          databricks fs mkdirs dbfs:/FileStore/shared_wheels    

          echo "copying wheel file to DBFS..."
          echo "copying $WHEEL_FILE to $DEST_PATH"
          databricks fs cp "$WHEEL_FILE" "$DEST_PATH" --overwrite --debug

          echo "Displaying content of dbfs:/FileStore/shared_wheels"
          databricks fs ls dbfs:/FileStore/shared_wheels

        else
          echo "No wheel file found"
          exit 1
        fi
      displayName: 'Upload Wheel to Databricks DBFS'
        
    - script: |
        eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)" 
        KEYVAULT_NAME="ingest02-meta002-sbox"

        echo "Query databricks secrets inside KeyVault backed scope (ingest00-meta002-sbox)"
        databricks secrets list-secrets $KEYVAULT_NAME
        databricks secrets list-secrets ingest00-meta002-sbox
      displayName: 'View KeyVault backed secret keys in Databricks'
    
    - script: |
        echo "Current dir: $(pwd)"
        ls -al /home/vsts/work/1/s
        eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)" 

        cd ARIA_DABs

        echo "Current dir: $(pwd)"
        ls -al  

        databricks bundle destroy -t sandbox 
        databricks bundle validate -t sandbox --debug
        databricks bundle deploy -t sandbox --debug

      #  databricks bundle run -t sandbox db_wf_joh_recieve_job
      displayName: "Deploy Databricks resources to ingest02 workspace"