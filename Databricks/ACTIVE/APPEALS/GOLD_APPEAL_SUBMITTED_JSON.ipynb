{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55703e49-2501-4dff-9e01-82528ccc926c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Python Wheel"
    }
   },
   "outputs": [],
   "source": [
    "%pip install /dbfs/FileStore/packages/shared_functions-0.6.4-py3-none-any.whl\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shared_functions.paymentPending as PP\n",
    "import shared_functions.appealSubmitted as APS\n",
    "from shared_functions.DQRules import base_DQRules, build_rule_expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7831545c-1e19-4244-ab14-25869bbd78f7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import packages"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import *\n",
    "import uk_postcodes_parsing\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "from pyspark.sql.types import StringType, StructType, ArrayType, MapType\n",
    "from pyspark.sql.functions import col, lit, to_json, struct, concat, regexp_replace\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e251edd4-ffe0-47d2-9c90-2add21a1af59",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Assign configs"
    }
   },
   "outputs": [],
   "source": [
    "config = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/configs/config.json\")\n",
    "env_name = config.first()[\"env\"].strip().lower()\n",
    "lz_key = config.first()[\"lz_key\"].strip().lower()\n",
    "\n",
    "print(f\"env_code: {lz_key}\")  # This won't be redacted\n",
    "print(f\"env_name: {env_name}\")  # This won't be redacted\n",
    "\n",
    "KeyVault_name = f\"ingest{lz_key}-meta002-{env_name}\"\n",
    "print(f\"KeyVault_name: {KeyVault_name}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36f0c0bd-2026-4b2c-8800-f4732e142615",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Assign OAuth"
    }
   },
   "outputs": [],
   "source": [
    "# Service principal credentials\n",
    "client_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-ID\")\n",
    "client_secret = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-SECRET\")\n",
    "tenant_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-TENANT-ID\")\n",
    "\n",
    "# Storage account names\n",
    "curated_storage = f\"ingest{lz_key}curated{env_name}\"\n",
    "checkpoint_storage = f\"ingest{lz_key}xcutting{env_name}\"\n",
    "raw_storage = f\"ingest{lz_key}raw{env_name}\"\n",
    "landing_storage = f\"ingest{lz_key}landing{env_name}\"\n",
    "external_storage = f\"ingest{lz_key}external{env_name}\"\n",
    "\n",
    "\n",
    "# Spark config for curated storage (Delta table)\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{curated_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{curated_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{curated_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{curated_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{curated_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{checkpoint_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{checkpoint_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{checkpoint_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{checkpoint_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{checkpoint_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{raw_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{raw_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{raw_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{raw_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{raw_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{landing_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{landing_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{landing_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{landing_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{landing_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{external_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{external_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{external_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{external_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{external_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e169fc23-7079-4102-8017-e009060ec361",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Assign Paths"
    }
   },
   "outputs": [],
   "source": [
    "AppealState = \"appealSubmitted\"\n",
    "output_name = 'appeal_submitted'\n",
    "\n",
    "# Setting variables for use in subsequent cells\n",
    "bronze_path = f\"abfss://bronze@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/\"\n",
    "silver_path = f\"abfss://silver@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/\"\n",
    "audit_path = f\"abfss://silver@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/AUDIT/{AppealState}\"\n",
    "gold_outputs = f\"ARIADM/ACTIVE/CCD/APPEALS/{AppealState}\"\n",
    "\n",
    "# Print all variables\n",
    "variables = {\n",
    "    # \"read_hive\": read_hive,\n",
    "    \n",
    "    \"bronze_path\": bronze_path,\n",
    "    \"silver_path\": silver_path,\n",
    "    \"audit_path\": audit_path,\n",
    "    \"gold_outputs\": gold_outputs,\n",
    "    \"key_vault\": KeyVault_name,\n",
    "    \"AppealState\": AppealState\n",
    "\n",
    "}\n",
    "\n",
    "display(variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e818dd62-dc2f-4b3b-bac0-f9216f747357",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This is Temp and needs further review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d30465e-ae52-4a05-9eab-bba6e0964456",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read in tables"
    }
   },
   "outputs": [],
   "source": [
    "silver_m1 = spark.table(\"ariadm_active_appeals.silver_appealcase_detail\").filter(col(\"dv_targetState\") == lit(AppealState)).distinct()\n",
    "silver_m2 = spark.table(\"ariadm_active_appeals.silver_caseapplicant_detail\") \n",
    "silver_m3 = spark.table(\"ariadm_active_appeals.silver_status_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "silver_m4 = spark.table(\"ariadm_active_appeals.silver_transaction_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "silver_m5 = spark.table(\"ariadm_active_appeals.silver_link_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "silver_c = spark.table(\"ariadm_active_appeals.silver_appealcategory_detail\")\n",
    "bronze_countryFromAddress = spark.table(\"ariadm_active_appeals.bronze_countries_countryFromAddress\").withColumn(\"lu_countryGovUkOocAdminJ\",col(\"countryGovUkOocAdminJ\"))\n",
    "bronze_HORef_cleansing = spark.table(\"ariadm_active_appeals.bronze_HORef_cleansing\")\n",
    "bronze_remission_lookup_df = spark.table(\"ariadm_active_appeals.bronze_remissions\").distinct()\n",
    "silver_h = spark.table(\"ariadm_active_appeals.silver_history_detail\")\n",
    "bronze_hearing_centres = spark.table(\"ariadm_active_appeals.bronze_hearing_centres\")\n",
    "bronze_derive_hearing_centres = spark.table(\"ariadm_active_appeals.bronze_derive_hearing_centres\")\n",
    "bronze_HORef_cleansing = spark.table(\"ariadm_active_appeals.bronze_HORef_cleansing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "725183e4-68d6-4f35-b27f-07d7eee61392",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: appealType"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = PP.appealType(silver_m1)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d9d0d26-a1c2-4f16-83f5-a80b03efadfe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: CaseData"
    }
   },
   "outputs": [],
   "source": [
    "df,df_audit = PP.caseData(silver_m1, silver_m2, silver_m3, silver_h, bronze_hearing_centres, bronze_derive_hearing_centres)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09990ce8-21fa-4219-b5d0-13535ffb304d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: flagsLabels"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = PP.flagsLabels(silver_m1, silver_m2, silver_c)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f34c43f4-2863-49aa-a26b-59ce05ee4ae6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: legalRepDetails"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = PP.legalRepDetails(silver_m1)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6be8eabb-810a-4cff-85b1-e0c20a5f6cb4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: appellantDetails"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = PP.appellantDetails(silver_m1, silver_m2, silver_c, bronze_countryFromAddress,bronze_HORef_cleansing)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c07ba4a2-0028-4133-b7af-c80ca1359d98",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: homeOfficeDetails"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = PP.homeOfficeDetails(silver_m1, silver_m2, silver_c, bronze_HORef_cleansing)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c102932-86cd-491c-ba77-39fc6a8d92e9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: paymentType"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = APS.paymentType(silver_m1, silver_m4)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c06653d4-f0de-4256-bc42-620efe620d81",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: PartyIDs"
    }
   },
   "outputs": [],
   "source": [
    "df_final,df_audit = PP.partyID(silver_m1, silver_m3, silver_c)\n",
    "# display(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfacea01-d7a6-481d-a3b9-40393a1048a0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: remissionTypes"
    }
   },
   "outputs": [],
   "source": [
    "df_final, df_audit = APS.remissionTypes(silver_m1, bronze_remission_lookup_df, silver_m4)\n",
    "# display(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3202d868-a71f-49e2-9571-5a28b812ed34",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: sponsorDetails"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = PP.sponsorDetails(silver_m1, silver_c)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78e872c0-0b0b-4259-a3b8-233ba36a9d31",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: GeneralFunctions"
    }
   },
   "outputs": [],
   "source": [
    "df, df_audit = PP.general(silver_m1, silver_m2, silver_m3, silver_h, bronze_hearing_centres, bronze_derive_hearing_centres)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c85fb77-8935-4f9a-a237-7778c4dd59a8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: GeneralDefault - Default Values"
    }
   },
   "outputs": [],
   "source": [
    "df = PP.generalDefault(silver_m1)\n",
    "# display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79de0936-f579-432a-990c-dad2f6f3f74a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: Documents"
    }
   },
   "outputs": [],
   "source": [
    "documents_content, documents_audit = PP.documents(silver_m1)\n",
    "# display(documents_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d102545f-6a84-4e04-83e2-daa92eee6c51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_appealSubmitted, df_audit_appealSubmitted = PP.caseState(silver_m1, \"appealSubmitted\")\n",
    "# display(df_appealSubmitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5552c83b-7c45-4994-a403-1b94dc87827f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: mainappealSubmitted"
    }
   },
   "outputs": [],
   "source": [
    "def mainappealSubmitted(silver_segmentation,silver_m1, silver_m2, silver_m3,silver_m4, silver_c,silver_h, bronze_remissions, bronze_countryFromAddress, bronze_HORef_cleansing):\n",
    "\n",
    "    AppealState = \"appealSubmitted\"\n",
    "\n",
    "    # Aggregate details\n",
    "    AppealType_df, AppealType_df_audit = PP.appealType(silver_m1)\n",
    "    caseData_df, caseData_df_audit = PP.caseData(silver_m1, silver_m2, silver_m3, silver_h, bronze_hearing_centres, bronze_derive_hearing_centres)\n",
    "    flagsLabels_df, flagsLabels_df_audit = PP.flagsLabels(silver_m1, silver_m2, silver_c)\n",
    "    appellantDetails_df, appellantDetails_df_audit = PP.appellantDetails(silver_m1, silver_m2, silver_c, bronze_countryFromAddress,bronze_HORef_cleansing)\n",
    "    legalRepDetails_df, legalRepDetails_df_audit = PP.legalRepDetails(silver_m1)\n",
    "    partyID_df, partyID_df_audit = PP.partyID(silver_m1, silver_m3, silver_c)\n",
    "    payment_df, payment_df_audit = APS.paymentType(silver_m1, silver_m4)\n",
    "    homeOfficeDetails_df, homeOfficeDetails_df_audit = PP.homeOfficeDetails(silver_m1, silver_m2, silver_c, bronze_HORef_cleansing)\n",
    "    remissionTypes_df, remissionTypes_df_audit = APS.remissionTypes(silver_m1, bronze_remission_lookup_df, silver_m4)\n",
    "    sponsorDetails_df, sponsorDetails_df_audit = PP.sponsorDetails(silver_m1, silver_c)\n",
    "    general_df, general_df_audit = PP.general(silver_m1, silver_m2, silver_m3, silver_h, bronze_hearing_centres, bronze_derive_hearing_centres)\n",
    "    generalDefault_df = PP.generalDefault(silver_m1)\n",
    "    documents_df, documents_df_audit = PP.documents(silver_m1)\n",
    "    caseState_df, caseState_df_audit = PP.caseState(silver_m1,\"appealSubmitted\")\n",
    "    silver_segmentation_df = silver_segmentation\n",
    "\n",
    "    # Join all aggregated data with Appeal Case Details\n",
    "    df_combined = (\n",
    "        silver_segmentation_df.join(AppealType_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(caseData_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(legalRepDetails_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(appellantDetails_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(flagsLabels_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(partyID_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(homeOfficeDetails_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(remissionTypes_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(sponsorDetails_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(payment_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(general_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(generalDefault_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(documents_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(caseState_df, on = \"CaseNo\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    # Join all aggregated data with Appeal Case Details\n",
    "    df_combined_audit = (\n",
    "        silver_segmentation_df.join(AppealType_df, on=\"CaseNo\", how=\"left\")\n",
    "        .join(caseData_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(legalRepDetails_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(appellantDetails_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(flagsLabels_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(partyID_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(homeOfficeDetails_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(remissionTypes_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(sponsorDetails_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(payment_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(general_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "        .join(documents_df_audit, on=\"CaseNo\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    Datetime_name = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "    # Create JSON and filename and omit columns that are with null values\n",
    "    df_final = df_combined.withColumn(\n",
    "        \"JSON_Content\", to_json(struct(*df_combined.drop(col(\"CaseNo\")).columns))\n",
    "    ).withColumn(\n",
    "        \"JSON_File_name\", concat(lit(f\"{gold_outputs}/{Datetime_name}/JSON/APPEALS_\"), regexp_replace(col(\"CaseNo\"), \"/\", \"_\"), lit(\".json\"))\n",
    "    )\n",
    "    \n",
    "    return df_final, df_combined_audit\n",
    "\n",
    "# silver_m1 = spark.table(\"ariadm_active_appeals.silver_appealcase_detail\").filter(col(\"dv_targetState\") == lit(AppealState)).distinct()\n",
    "# silver_m2 =  spark.table(\"ariadm_active_appeals.silver_caseapplicant_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "# silver_m3 = spark.table(\"ariadm_active_appeals.silver_status_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "# silver_c = spark.table(\"ariadm_active_appeals.silver_appealcategory_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "# silver_h = spark.table(\"ariadm_active_appeals.silver_history_detail\")\n",
    "# silver_segmentation = spark.table(\"ariadm_active_appeals.stg_segmentation_states\").filter(col(\"TargetState\") == lit(AppealState))\n",
    "# bronze_remissions = spark.table(\"ariadm_active_appeals.bronze_remissions\").distinct()\n",
    "\n",
    "# bronze_countryFromAddress = spark.table(\"ariadm_active_appeals.bronze_countries_countryFromAddress\").withColumn(\"lu_countryGovUkOocAdminJ\",col(\"countryGovUkOocAdminJ\"))\n",
    "\n",
    "# bronze_hearing_centres = spark.table(\"ariadm_active_appeals.bronze_hearing_centres\")\n",
    "# bronze_derive_hearing_centres = spark.table(\"ariadm_active_appeals.bronze_derive_hearing_centres\")\n",
    "\n",
    "# bronze_HORef_cleansing = spark.table(\"ariadm_active_appeals.bronze_HORef_cleansing\")\n",
    "\n",
    "# df_final, df_audit = mainappealSubmitted(silver_segmentation,silver_m1, silver_m2, silver_m3,silver_m4, silver_c,silver_h, bronze_remissions, bronze_countryFromAddress, bronze_HORef_cleansing)\n",
    "\n",
    "# display(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1e06424-6966-4125-9563-3d73261b37c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Function: Upload  and Blob Client Connection Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bd38396-bc28-4971-84ae-805bfd07e775",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Secret Retrieval for Database Connection"
    }
   },
   "outputs": [],
   "source": [
    "secret = dbutils.secrets.get(KeyVault_name, f\"CURATED-{env_name}-SAS-TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5169b7b1-135d-4de4-a228-a25b02b66f97",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Azure Blob Storage Connection Setup in Python"
    }
   },
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "import os\n",
    "\n",
    "# Set up the BlobServiceClient with your connection string\n",
    "connection_string = secret\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Specify the container name\n",
    "container_name = \"gold\"\n",
    "container_client = blob_service_client.get_container_client(container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd8881aa-9b58-41e1-ad05-93d0230af4c5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Function: Format Dates for UploadBlob Storage"
    }
   },
   "outputs": [],
   "source": [
    "# Upload HTML to Azure Blob Storage\n",
    "def upload_to_blob(file_name, file_content):\n",
    "    try:\n",
    "        # blob_client = container_client.get_blob_client(f\"{gold_outputs}/HTML/{file_name}\")\n",
    "        blob_client = container_client.get_blob_client(f\"{file_name}\")\n",
    "        blob_client.upload_blob(file_content, overwrite=True)\n",
    "        return \"success\"\n",
    "    except Exception as e:\n",
    "        return f\"error: {str(e)}\"\n",
    "\n",
    "# Register the upload function as a UDF\n",
    "upload_udf = udf(upload_to_blob)\n",
    "\n",
    "# df_with_upload_status = df_final.withColumn(\n",
    "#     \"Status\", upload_udf(col(\"JSON_File_name\"), col(\"JSON_Content\"))\n",
    "# )\n",
    "\n",
    "# display(df_with_upload_status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db97feb8-762a-40b6-8cb9-5afa8f134fe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Gold Outputs and Tracking DLT Table Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "870438cd-a53f-413b-b3c9-2fb6107ee728",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Combine Validation Checks into String"
    }
   },
   "outputs": [],
   "source": [
    "checks = {}\n",
    "checks = base_DQRules()\n",
    "\n",
    "checks[\"valid_paymentStatus\"] = ( \n",
    "  \"(dv_CCDAppealType IN ('EA','EU','HU','PA') AND (paymentStatus IS NOT NULL)) OR (dv_CCDAppealType NOT IN ('EA','EU','HU','PA') AND (paymentStatus IS NULL))\"\n",
    ")\n",
    "\n",
    "checks[\"valid_paAppealTypePaymentOption\"] = (\n",
    "  \"((dv_representation = 'LR' AND dv_CCDAppealType IN ('PA') AND paAppealTypePaymentOption IN ('payNow', 'payLater') AND paAppealTypePaymentOption IS NOT NULL) OR (paAppealTypePaymentOption IS NULL))\"\n",
    ")\n",
    " \n",
    "checks[\"valid_paAppealTypeAipPaymentOption\"] = (\n",
    "  \"((dv_representation = 'AIP' AND dv_CCDAppealType IN ('PA') AND paAppealTypeAipPaymentOption IN ('payNow', 'payLater') AND paAppealTypeAipPaymentOption IS NOT NULL) OR (paAppealTypeAipPaymentOption IS NULL))\"\n",
    ")\n",
    "\n",
    "checks[\"valid_paidAmount\"] = (\n",
    "  \"((dv_CCDAppealType IN ('EA', 'EU', 'HU', 'PA') AND paidAmount IS NOT NULL AND TRY_CAST(paidAmount AS INT) IS NOT NULL) OR (dv_CCDAppealType NOT IN ('EA', 'EU', 'HU', 'PA') AND paidAmount IS NULL))\"\n",
    ")\n",
    " \n",
    "checks[\"valid_additionalPaymentInfo\"] = (\n",
    "  \"((dv_CCDAppealType IN ('EA', 'EU', 'HU', 'PA') AND additionalPaymentInfo IS NOT NULL) OR (dv_CCDAppealType NOT IN ('EA', 'EU', 'HU', 'PA') AND additionalPaymentInfo IS NULL))\"\n",
    ")\n",
    " \n",
    "checks[\"valid_paymentDescription\"] = (\n",
    "  \"((dv_CCDAppealType IN ('EA', 'EU', 'HU', 'PA') AND paymentDescription IS NOT NULL) OR (dv_CCDAppealType NOT IN ('EA', 'EU', 'HU', 'PA') AND paymentDescription IS NULL))\"\n",
    ")\n",
    "\n",
    "checks[\"valid_remissionDecision\"] = (\n",
    "  \"((dv_CCDAppealType IN ('EA', 'EU', 'HU', 'PA') AND remissionDecision IN ('approved', 'partiallyApproved', 'rejected') AND remissionDecision IS NOT NULL) OR (remissionDecision IS NULL))\"\n",
    ")\n",
    " \n",
    "checks[\"valid_remissionDecisionReason\"] = (\n",
    "  \"((dv_CCDAppealType IN ('EA', 'EU', 'HU', 'PA') AND remissionDecisionReason IS NOT NULL) OR (dv_CCDAppealType NOT IN ('EA', 'EU', 'HU', 'PA') AND remissionDecisionReason IS NULL))\"\n",
    ")\n",
    " \n",
    "checks[\"valid_amountRemitted\"] = (\n",
    "  \"((dv_CCDAppealType IN ('EA', 'EU', 'HU', 'PA') AND amountRemitted IS NOT NULL AND TRY_CAST(amountRemitted AS INT) IS NOT NULL) OR (dv_CCDAppealType NOT IN ('EA', 'EU', 'HU', 'PA') AND amountRemitted IS NULL))\"\n",
    ")\n",
    "\n",
    "checks[\"valid_caseNotes\"] = (\n",
    "  \"(caseNotes IS NOT NULL)\"\n",
    ")\n",
    "\n",
    "checks[\"valid_amountLeftToPay\"] = (\n",
    "  \"((dv_CCDAppealType IN ('EA', 'EU', 'HU', 'PA') AND amountLeftToPay IS NOT NULL AND TRY_CAST(amountLeftToPay AS INT) IS NOT NULL) OR (dv_CCDAppealType NOT IN ('EA', 'EU', 'HU', 'PA') AND amountLeftToPay IS NULL))\"\n",
    ")\n",
    "\n",
    "dq_rules = build_rule_expression(checks)\n",
    "# dq_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63c18d17-ba6f-4431-a1d6-fcedfe927157",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transformation: stg_main_appealSubmitted_validation"
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, lit, expr\n",
    "\n",
    "@dlt.table(\n",
    "    name=f\"stg_main_{output_name}_validation\",\n",
    "    comment=\"DLT table running mainPaymentPending to generate a JSON_Content column for CCD validation. Applies DLT expectations on CCD, adding is_valid to flag validation results.\",\n",
    "    path=f\"{audit_path}/stg_main_{output_name}_validation\"\n",
    ")\n",
    "@dlt.expect_all(checks)\n",
    "def stg_main_appealSubmitted_validation():\n",
    "    try:\n",
    "        silver_m1 = dlt.read(\"silver_appealcase_detail\").filter(col(\"dv_targetState\") == lit(AppealState)).distinct()\n",
    "        bronze_appealtype_lookup_df = dlt.read(\"bronze_appealtype\").distinct()\n",
    "        bronze_hearing_centres_lookup_df = dlt.read(\"bronze_hearing_centres\").distinct()\n",
    "        # stg_representation = dlt.read(\"stg_representation\").select(col(\"Representation\").alias(\"valid_representation\"))\n",
    "        silver_m2 = dlt.read(\"silver_caseapplicant_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_m3 = dlt.read(\"silver_status_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_m4 = dlt.read(\"silver_transaction_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_m5 = dlt.read(\"silver_link_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m6 = dlt.read(\"silver_adjudicator_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_c = dlt.read(\"silver_appealcategory_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m8 = dlt.read(\"silver_documentsreceived_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_h = dlt.read(\"silver_history_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        bronze_countries_postal_lookup_df = dlt.read(\"bronze_countries_postal\").distinct()\n",
    "        bronze_remissions_lookup_df = dlt.read(\"bronze_remissions\").distinct()\n",
    "        bronze_countryFromAddress = dlt.read(\"bronze_countries_countryFromAddress\")\n",
    "        bronze_HORef_cleansing = dlt.read(\"bronze_HORef_cleansing\")\n",
    "        bronze_hearing_centres = dlt.read(\"bronze_hearing_centres\")\n",
    "        bronze_derive_hearing_centres = dlt.read(\"bronze_derive_hearing_centres\")\n",
    "        bronze_remission_lookup_df =  dlt.read(\"bronze_remissions\").distinct()\n",
    "        silver_segmentation = spark.table(\"ariadm_active_appeals.stg_segmentation_states\").filter(col(\"TargetState\") == lit(AppealState))\n",
    "    except:\n",
    "        silver_m1 = spark.table(\"ariadm_active_appeals.silver_appealcase_detail\").filter(col(\"dv_targetState\") == lit(AppealState)).distinct()\n",
    "        bronze_appealtype_lookup_df = spark.table(\"ariadm_active_appeals.bronze_appealtype\").distinct()\n",
    "        bronze_hearing_centres_lookup_df = spark.table(\"ariadm_active_appeals.bronze_hearing_centres\").distinct()\n",
    "        # stg_representation = spark.table(\"ariadm_active_appeals.stg_representation\").select(col(\"Representation\").alias(\"valid_representation\"))\n",
    "        silver_m2 = spark.table(\"ariadm_active_appeals.silver_caseapplicant_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_m3 = spark.table(\"ariadm_active_appeals.silver_status_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_m4 = spark.table(\"ariadm_active_appeals.silver_transaction_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_m5 = spark.table(\"ariadm_active_appeals.silver_link_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m6 = spark.table(\"ariadm_active_appeals.silver_adjudicator_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_c = spark.table(\"ariadm_active_appeals.silver_appealcategory_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m8 = spark.table(\"ariadm_active_appeals.silver_documentsreceived_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_h = spark.table(\"ariadm_active_appeals.silver_history_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        bronze_countries_postal_lookup_df = spark.table(\"ariadm_active_appeals.bronze_countries_postal\").distinct() \n",
    "        bronze_remissions_lookup_df = spark.table(\"ariadm_active_appeals.bronze_remissions\").distinct()\n",
    "        bronze_countryFromAddress = spark.table(\"ariadm_active_appeals.bronze_countries_countryFromAddress\")\n",
    "        bronze_HORef_cleansing = spark.table(\"ariadm_active_appeals.bronze_HORef_cleansing\")\n",
    "        bronze_hearing_centres = spark.table(\"ariadm_active_appeals.bronze_hearing_centres\")\n",
    "        bronze_derive_hearing_centres = spark.table(\"ariadm_active_appeals.bronze_derive_hearing_centres\")\n",
    "        bronze_remission_lookup_df = spark.table(\"ariadm_active_appeals.bronze_remissions\").distinct()\n",
    "        silver_segmentation = spark.table(\"ariadm_active_appeals.stg_segmentation_states\").filter(col(\"TargetState\") == lit(AppealState))\n",
    "\n",
    " \n",
    "    df_final,df_audit = mainappealSubmitted(silver_segmentation,silver_m1, silver_m2, silver_m3, silver_m4, silver_c, silver_h, bronze_remission_lookup_df, bronze_countryFromAddress, bronze_HORef_cleansing)\n",
    " \n",
    "    valid_representation = silver_m1.select(col(\"CaseNo\"), col(\"dv_representation\"),col(\"dv_CCDAppealType\"),col(\"CaseRep_Address5\"), col(\"CaseRep_Postcode\"),col(\"MainRespondentId\"), col(\"lu_appealType\"), col(\"HORef\"),col(\"Sponsor_Authorisation\"),col(\"Sponsor_Name\")) \n",
    "    valid_appealant_address = silver_m2.select(col(\"CaseNo\"), col(\"Appellant_Address1\"), col(\"Appellant_Address2\"),col(\"Appellant_Address3\"),(\"Appellant_Address4\"), col(\"Appellant_Address5\"), col(\"Appellant_Postcode\"),col(\"Appellant_Email\"),col(\"Appellant_Telephone\"), col(\"FCONumber\")).filter(col(\"Relationship\").isNull())\n",
    "    valid_country_list = bronze_countries_postal_lookup_df.select(col(\"countryGovUkOocAdminJ\").alias(\"valid_countryGovUkOocAdminJ\")).distinct()\n",
    "    valid_catagoryid_list = silver_c.groupBy(\"CaseNo\").agg(F.collect_list(\"CategoryId\").alias(\"valid_categoryIdList\"))\n",
    "    valid_HORef_cleansing = bronze_HORef_cleansing.select( col(\"CaseNo\"),coalesce(col(\"HORef\"), col(\"FCONumber\")).alias(\"lu_HORef\"))\n",
    "    valid_reasonDescription = silver_m1.alias(\"m1\").join(bronze_remission_lookup_df, on=[\"PaymentRemissionReason\",\"PaymentRemissionRequested\"], how=\"left\").select(\"CaseNo\", \"ReasonDescription\",col(\"remissionClaim\").alias(\"lu_remissionClaim\"),col(\"feeRemissionType\").alias(\"lu_feeRemissionType\"))\n",
    " \n",
    "    df_final = df_final.join(valid_representation, on=\"CaseNo\", how=\"left\"\n",
    "                            ).join(valid_country_list, on=col(\"CaseRep_Address5\") == col(\"valid_countryGovUkOocAdminJ\"), how=\"left\"\n",
    "                            ).join(valid_catagoryid_list, on=\"CaseNo\", how=\"left\"\n",
    "                            ).join(valid_appealant_address, on=\"CaseNo\", how=\"left\"\n",
    "                            ).join(valid_HORef_cleansing, on=\"CaseNo\", how=\"left\"\n",
    "                            ).join(valid_reasonDescription, on=\"CaseNo\", how=\"left\")\n",
    "\n",
    " \n",
    "    df_final = df_final.withColumn(\"is_valid\", expr(dq_rules))\n",
    "\n",
    "    # df_final = df_final.drop(col(\"dv_representation\"), col(\"CaseRepAddress5\"), col(\"CaseRepPostcode\"), col(\"valid_countryGovUkOocAdminJ\"))\n",
    "\n",
    "    # columns_to_drop = [\"dv_representation\", \"CaseRepAddress5\", \"CaseRepPostcode\"]\n",
    "\n",
    "    # if all(col in df_final.columns for col in columns_to_drop): #If the columns exist - remove \n",
    "    #     df_final = df_final.drop(*columns_to_drop)\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3362b255-72e6-4a4f-8b12-170c233128ad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transformation: stg_valid_appealSubmitted_records"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=f\"stg_valid_{output_name}_records\",\n",
    "    comment=\"Delta Live Gold Table with JSON content.\",\n",
    "    path=f\"{audit_path}/stg_valid_{output_name}_records\"\n",
    ")\n",
    "def stg_valid_appealSubmitted_records():\n",
    "    \"\"\"\n",
    "    Delta Live Table for creating and uploading JSON content for Appeals.\n",
    "    \"\"\"\n",
    "    # Load source data\n",
    "    df = dlt.read(f\"stg_main_{output_name}_validation\")\n",
    "\n",
    "    df_filtered = df.filter(\n",
    "        (col(\"is_valid\") == True)\n",
    "    )\n",
    "\n",
    "    # Repartition to optimize parallelism\n",
    "    repartitioned_df = df_filtered.repartition(64)\n",
    "\n",
    "    df_with_upload_status = repartitioned_df.filter(~col(\"JSON_content\").like(\"Error%\")).withColumn(\n",
    "            \"Status\", upload_udf(col(\"JSON_File_Name\"), col(\"JSON_content\"))\n",
    "        )\n",
    "\n",
    "    # Return the DataFrame for DLT table creation\n",
    "    return df_with_upload_status.select(\"CaseNo\", \"JSON_content\",col(\"JSON_File_Name\").alias(\"File_Name\"),\"Status\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60bfdf72-3653-4133-ae58-42f66d648a28",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transformation: stg_invalid_appealSubmitted_quarantine_records"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=f\"stg_invalid_{output_name}_quarantine_records\",\n",
    "    comment=\"Quarantined records that failed data quality checks or JSON generation.\",\n",
    "    path=f\"{audit_path}/stg_invalid_{output_name}_quarantine_records\"\n",
    ")\n",
    "def stg_invalid_appealSubmitted_quarantine_records():\n",
    "\n",
    "    df = dlt.read(f\"stg_main_{output_name}_validation\")\n",
    "\n",
    "    df_filtered = df.filter(\n",
    "        (col(\"is_valid\") != True)\n",
    "    ).withColumn(\"JSON_File_Name\", regexp_replace(col(\"JSON_File_Name\"), \"/JSON/\", \"/INVALID_JSON/\"))\n",
    "\n",
    "    # Repartition to optimize parallelism\n",
    "    repartitioned_df = df_filtered.repartition(64)\n",
    "\n",
    "    df_with_upload_status = repartitioned_df.filter(~col(\"JSON_content\").like(\"Error%\")).withColumn(\n",
    "            \"Status\", upload_udf(col(\"JSON_File_Name\"), col(\"JSON_content\"))\n",
    "        )\n",
    "\n",
    "    return df_with_upload_status.select(\"CaseNo\", \"JSON_content\",col(\"JSON_File_Name\").alias(\"File_Name\"),\"Status\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a81bf947-c500-4014-b228-71862de0a3fc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transformation: apl_active_appealSubmitted_cr_audit_table"
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, lit, expr\n",
    "\n",
    "@dlt.table(\n",
    "    name=f\"apl_active_{output_name}_cr_audit_table\",\n",
    "    comment=\"DLT table Covers 4.2 Silver layer LLD requirements: Audits CCD attributes, input field values, derived values, and all columns for validation and traceability.\",\n",
    "    path=f\"{audit_path}/apl_active_{output_name}_cr_audit_table\"\n",
    ")\n",
    "def apl_active_appealSubmitted_cr_audit_table():\n",
    "    try:\n",
    "        silver_m1 = dlt.read(\"silver_appealcase_detail\").filter(col(\"dv_targetState\") == lit(AppealState)).distinct()\n",
    "        bronze_appealtype_lookup_df = dlt.read(\"bronze_appealtype\").distinct()\n",
    "        bronze_hearing_centres_lookup_df = dlt.read(\"bronze_hearing_centres\").distinct()\n",
    "        # stg_representation = dlt.read(\"stg_representation\").select(col(\"Representation\").alias(\"valid_representation\"))\n",
    "        silver_m2 = dlt.read(\"silver_caseapplicant_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_m3 = dlt.read(\"silver_status_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m4 = dlt.read(\"silver_transaction_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_m5 = dlt.read(\"silver_link_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m6 = dlt.read(\"silver_adjudicator_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_c = dlt.read(\"silver_appealcategory_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_h = dlt.read(\"silver_history_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m8 = dlt.read(\"silver_documentsreceived_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m9 = dlt.read(\"silver_history_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        bronze_countries_postal_lookup_df = dlt.read(\"bronze_countries_postal\").distinct()\n",
    "        bronze_remissions_lookup_df = dlt.read(\"bronze_remissions\").distinct()\n",
    "        bronze_countryFromAddress = dlt.read(\"bronze_countries_countryFromAddress\")\n",
    "        bronze_HORef_cleansing = dlt.read(\"bronze_HORef_cleansing\")\n",
    "        silver_segmentation = dlt.read(\"stg_segmentation_states\").filter(col(\"TargetState\") == lit(AppealState))\n",
    "    except:\n",
    "        silver_m1 = spark.table(\"ariadm_active_appeals.silver_appealcase_detail\").filter(col(\"dv_targetState\") == lit(AppealState)).distinct()\n",
    "        bronze_appealtype_lookup_df = spark.table(\"ariadm_active_appeals.bronze_appealtype\").distinct()\n",
    "        bronze_hearing_centres_lookup_df = spark.table(\"ariadm_active_appeals.bronze_hearing_centres\").distinct()\n",
    "        # stg_representation = spark.table(\"ariadm_active_appeals.stg_representation\").select(col(\"Representation\").alias(\"valid_representation\"))\n",
    "        silver_m2 = spark.table(\"ariadm_active_appeals.silver_caseapplicant_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_m3 = spark.table(\"ariadm_active_appeals.silver_status_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m4 = spark.table(\"ariadm_active_appeals.silver_transaction_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_m5 = spark.table(\"ariadm_active_appeals.silver_link_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m6 = spark.table(\"ariadm_active_appeals.silver_adjudicator_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_c = spark.table(\"ariadm_active_appeals.silver_appealcategory_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        silver_h = spark.table(\"ariadm_active_appeals.silver_history_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m8 = spark.table(\"ariadm_active_appeals.silver_documentsreceived_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        # silver_m9 = spark.table(\"ariadm_active_appeals.silver_history_detail\").filter(col(\"dv_targetState\") == lit(AppealState))\n",
    "        bronze_countries_postal_lookup_df = spark.table(\"ariadm_active_appeals.bronze_countries_postal\").distinct() \n",
    "        bronze_remissions_lookup_df = spark.table(\"ariadm_active_appeals.bronze_remissions\").distinct()\n",
    "        bronze_countryFromAddress = spark.table(\"ariadm_active_appeals.bronze_countries_countryFromAddress\")\n",
    "        bronze_HORef_cleansing = spark.table(\"ariadm_active_appeals.bronze_HORef_cleansing\")\n",
    "        silver_segmentation = spark.table(\"ariadm_active_appeals.stg_segmentation_states\").filter(col(\"TargetState\") == lit(AppealState))\n",
    " \n",
    "    df_final,df_audit = mainappealSubmitted(silver_segmentation,silver_m1, silver_m2, silver_m3, silver_m4, silver_c, silver_h, bronze_remission_lookup_df, bronze_countryFromAddress, bronze_HORef_cleansing)\n",
    "\n",
    "    return df_audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c065211-f4d5-47d6-9f7f-757fedeaa149",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Exit Notebook"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Notebook completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "GOLD_APPEAL_SUBMITTED_JSON",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
