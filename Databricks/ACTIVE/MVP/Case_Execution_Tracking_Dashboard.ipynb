{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "282a1963-cd5d-4076-a466-330fd4eed639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install confluent_kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4609d39-8e6c-41d5-9abf-392a9ee56455",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Libraries"
    }
   },
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "import json\n",
    "from  itertools import islice\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import col, decode, split, element_at, udf, lit, reduce, from_json, regexp_replace, concat, when\n",
    "import logging\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DateType\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark import SparkContext\n",
    "import os\n",
    "from functools import reduce\n",
    "import time\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64cf17e7-1a72-4e5d-b50f-835f5f169017",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initialise logging"
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"DatabricksWorkflow\")\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "if not logger.hasHandlers():\n",
    "    logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3010f725-1597-452d-89a1-d3636edff2ad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set-up cnfigs"
    }
   },
   "outputs": [],
   "source": [
    "config_path = \"dbfs:/configs/config.json\"\n",
    "try:\n",
    "    config = spark.read.option(\"multiline\", \"true\").json(config_path)\n",
    "    logger.info(f\"Successfully read config file from {config_path}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not read config file at {config_path}: {e}\", exc_info=True)\n",
    "    raise FileNotFoundError(f\"Could not read config file at {config_path}: {e}\")\n",
    "\n",
    "try:\n",
    "    first_row = config.first()\n",
    "    env = first_row[\"env\"].strip().lower()\n",
    "    lz_key = first_row[\"lz_key\"].strip().lower()\n",
    "    logger.info(f\"Extracted configs: env={env}, lz_key={lz_key}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Missing expected keys 'env' or 'lz_key' in config file: {e}\", exc_info=True)\n",
    "    raise KeyError(f\"Missing expected keys 'env' or 'lz_key' in config file: {e}\")\n",
    "\n",
    "try:\n",
    "    keyvault_name = f\"ingest{lz_key}-meta002-{env}\"\n",
    "    logger.info(f\"Constructed keyvault name: {keyvault_name}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error constructing keyvault name: {e}\", exc_info=True)\n",
    "    raise ValueError(f\"Error constructing keyvault name: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daf71926-d55f-4e3c-be2a-8ae9808b5390",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set-up OAuth"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    client_secret = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-SECRET')\n",
    "    logger.info(\"Successfully retrieved SERVICE-PRINCIPLE-CLIENT-SECRET from Key Vault\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not retrieve 'SERVICE-PRINCIPLE-CLIENT-SECRET' from Key Vault '{keyvault_name}': {e}\", exc_info=True)\n",
    "    raise KeyError(f\"Could not retrieve 'SERVICE-PRINCIPLE-CLIENT-SECRET' from Key Vault '{keyvault_name}': {e}\")\n",
    "\n",
    "try:\n",
    "    tenant_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-TENANT-ID')\n",
    "    logger.info(\"Successfully retrieved SERVICE-PRINCIPLE-TENANT-ID from Key Vault\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not retrieve 'SERVICE-PRINCIPLE-TENANT-ID' from Key Vault '{keyvault_name}': {e}\", exc_info=True)\n",
    "    raise KeyError(f\"Could not retrieve 'SERVICE-PRINCIPLE-TENANT-ID' from Key Vault '{keyvault_name}': {e}\")\n",
    "\n",
    "try:\n",
    "    client_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-ID')\n",
    "    logger.info(\"Successfully retrieved SERVICE-PRINCIPLE-CLIENT-ID from Key Vault\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not retrieve 'SERVICE-PRINCIPLE-CLIENT-ID' from Key Vault '{keyvault_name}': {e}\", exc_info=True)\n",
    "    raise KeyError(f\"Could not retrieve 'SERVICE-PRINCIPLE-CLIENT-ID' from Key Vault '{keyvault_name}': {e}\")\n",
    "\n",
    "logger.info(\"✅ Successfully retrieved all Service Principal secrets from Key Vault\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "881b1297-abd6-47b9-83f3-0111b4a47cd8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Assign OAuth"
    }
   },
   "outputs": [],
   "source": [
    "# --- Parameterise containers ---\n",
    "curated_storage_account = f\"ingest{lz_key}curated{env}\"\n",
    "curated_container = \"gold\"\n",
    "silver_curated_container = \"silver\"\n",
    "checkpoint_storage_account = f\"ingest{lz_key}xcutting{env}\"\n",
    "\n",
    "# --- Assign OAuth to storage accounts ---\n",
    "storage_accounts = [curated_storage_account, checkpoint_storage_account]\n",
    "\n",
    "for storage_account in storage_accounts:\n",
    "    try:\n",
    "        configs = {\n",
    "            f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\": \"OAuth\",\n",
    "            f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\":\n",
    "                \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "            f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\": client_id,\n",
    "            f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\": client_secret,\n",
    "            f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\":\n",
    "                f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "        }\n",
    "\n",
    "        for key, val in configs.items():\n",
    "            try:\n",
    "                spark.conf.set(key, val)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to set Spark config '{key}' for storage account '{storage_account}': {e}\", exc_info=True)\n",
    "                raise RuntimeError(f\"Failed to set Spark config '{key}' for storage account '{storage_account}': {e}\")\n",
    "\n",
    "        logger.info(f\"✅ Successfully configured OAuth for storage account: {storage_account}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error configuring OAuth for storage account '{storage_account}': {e}\", exc_info=True)\n",
    "        raise RuntimeError(f\"Error configuring OAuth for storage account '{storage_account}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86c6ca78-8d30-4ebc-a762-4adb3a11fdc6",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764156534467}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1764851051152}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": "From CCD Publish Payload to CCD Call Result"
    }
   },
   "outputs": [],
   "source": [
    "ccd_call_result = spark.read.format(\"delta\").load(f\"abfss://silver@ingest{lz_key}curated{env}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/AUDIT/APPEALS/all_active_states/ack_audit\")\n",
    "ccd_call_result.createOrReplaceTempView(\"ccd_call_result\")\n",
    "\n",
    "ccd_publish_payload_result = spark.read.format(\"delta\").load(f\"abfss://{silver_curated_container}@{curated_storage_account}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/publish_payload_audit\")\n",
    "ccd_publish_payload_result.createOrReplaceTempView(\"ccd_publish_payload_result\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT \n",
    "          \n",
    "        COALESCE(t2.runid, t1.runid) as RunID,\n",
    "        COALESCE(t2.state, t1.state) as State,\n",
    "        split_part(COALESCE(t2.CaseNo, t1.CaseNo), \".\", 1) as CaseNo,\n",
    "        t1.status as `Publish Payload Status`,\n",
    "        t2.status as `CCD Call Status`,\n",
    "        t1.PublishingDateTime as `CCD Publish Payload Publishing Date Time`,\n",
    "        t2.error as `CCD Call Function App Error`,\n",
    "        t2.StartDateTime as `CCD Call Function App Start Date Time`,\n",
    "        t2.EndDateTime as `CCD Call Function App End Date Time`,\n",
    "        t2.CCDCaseID as `CCD Case ID`,\n",
    "        t1.error as `CCD Publish Payload Error`\n",
    "          \n",
    "          FROM ccd_publish_payload_result t1\n",
    "          full outer join ccd_call_result t2 on t2.CaseNo = t1.Filename and t1.state = t2.state --and t1.RunID = t2.RunID\n",
    "\n",
    "          ORDER BY t1.PublishingDateTime DESC\n",
    "          \n",
    "          \"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6a073c1-bb35-499c-9497-24fe9d9be352",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764703506442}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1764851062365}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": "From Segmentation to CCD Call Result"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "WITH segmentation_state_cte AS (\n",
    "    SELECT \n",
    "        split_part(CaseNo, '.', 1) AS CaseNo,\n",
    "        ariaDesiredState AS State,\n",
    "        'Yes' AS `Segmentation State`\n",
    "        -- RunID\n",
    "    FROM hive_metastore.gold_payloads.dashboard_segmentation_state\n",
    "),\n",
    "\n",
    "valid_invalid_json_cte AS (\n",
    "    SELECT\n",
    "        CaseNo,\n",
    "        State AS State,\n",
    "        'Invalid' AS `Validation Status`\n",
    "        -- RunID\n",
    "    FROM hive_metastore.gold_payloads.dashboard_invalid_json_files\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT\n",
    "        CaseNo,\n",
    "        State AS State,\n",
    "        'Valid' AS `Validation Status`\n",
    "        -- RunID\n",
    "    FROM hive_metastore.gold_payloads.dashboard_valid_json_files\n",
    "),\n",
    "\n",
    "pub_payload_call_result_cte AS (\n",
    "    SELECT \n",
    "        COALESCE(t2.runid, t1.runid) AS RunID,\n",
    "        COALESCE(t2.state, t1.state) AS State,\n",
    "        split_part(COALESCE(t2.CaseNo, t1.CaseNo), \".\", 1) AS CaseNo,\n",
    "        t1.status AS `CCD Publish Payload Status`,\n",
    "        t2.status AS `CCD Call Status`,\n",
    "        t1.PublishingDateTime AS `CCD Publish Payload Publishing Date Time`,\n",
    "        t2.error AS `CCD Call Function App Error`,\n",
    "        t2.StartDateTime AS `CCD Call Function App Start Date Time`,\n",
    "        t2.EndDateTime AS `CCD Call Function App End Date Time`,\n",
    "        t2.CCDCaseID AS `CCD Case ID`,\n",
    "        t1.error AS `CCD Publish Payload Error`\n",
    "    FROM ccd_publish_payload_result t1\n",
    "    FULL OUTER JOIN ccd_call_result t2 \n",
    "        ON t2.CaseNo = t1.Filename \n",
    "       AND t1.state = t2.state \n",
    "    --    AND t1.RunID = t2.RunID\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    COALESCE(a.State, b.State) AS State, \n",
    "    COALESCE(a.CaseNo, b.CaseNo) AS CaseNo,\n",
    "    -- COALESCE(a.RunID, b.RunID, d.RunID) AS RunID,\n",
    "    d.RunID,\n",
    "    a.`Segmentation State`,\n",
    "    b.`Validation Status`,\n",
    "    d.`CCD Publish Payload Status`,\n",
    "    d.`CCD Call Status`,\n",
    "    d.`CCD Publish Payload Publishing Date Time`,\n",
    "    d.`CCD Publish Payload Error`,\n",
    "    d.`CCD Call Function App Start Date Time`,\n",
    "    d.`CCD Call Function App End Date Time`,\n",
    "    d.`CCD Case ID`,\n",
    "    d.`CCD Call Function App Error`\n",
    "FROM segmentation_state_cte a\n",
    "FULL OUTER JOIN valid_invalid_json_cte b\n",
    "    ON a.CaseNo = b.CaseNo \n",
    "    -- AND a.RunID = b.RunID\n",
    "FULL OUTER JOIN pub_payload_call_result_cte d\n",
    "    ON COALESCE(a.CaseNo, b.CaseNo) = d.CaseNo\n",
    "--    AND COALESCE(a.RunID, b.RunID) = d.RunID\n",
    "\n",
    "ORDER BY d.`CCD Call Function App Start Date Time` DESC;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0614573a-f693-44fb-bbe2-6abcd7883060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Notebook completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [
    {
     "elements": [
      {
       "dashboardResultIndex": 0,
       "elementNUID": "86c6ca78-8d30-4ebc-a762-4adb3a11fdc6",
       "elementType": "command",
       "guid": "06a416e4-707b-4bc4-b8d4-39453a39e79c",
       "options": null,
       "position": {
        "height": 14,
        "width": 24,
        "x": 0,
        "y": 0,
        "z": null
       },
       "resultIndex": null
      }
     ],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "460527ac-f6c6-44a4-bca1-ba672fcab2ae",
     "origId": 6787605121945691,
     "title": "Case Execution Tracking Dashboard",
     "version": "DashboardViewV1",
     "width": 2048
    }
   ],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6787605121945688,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Case_Execution_Tracking_Dashboard",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
