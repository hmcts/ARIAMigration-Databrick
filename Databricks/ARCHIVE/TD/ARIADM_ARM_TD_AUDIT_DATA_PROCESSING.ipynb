{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2522e663-16c8-479b-a5d0-cde07c2e4358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Audit Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cbb73ff-ea97-4d52-8cb2-bef1d6a7c843",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import json\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "from pyspark.sql.window import Window\n",
    "import uuid\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0d2f113-790d-4706-80c6-7b161ab5a259",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# audit_mnt = \"/mnt/ingest00curatedsboxsilver/ARIADM/ARM/AUDIT/TD\"\n",
    "audit_delta_path = \"/mnt/ingest00curatedsboxsilver/ARIADM/ARM/AUDIT/TD/td_cr_audit_table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21d6bc13-19f1-4d4f-93b1-3420aa291f45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def datetime_uuid():\n",
    "    dt_str = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return str(uuid.uuid5(uuid.NAMESPACE_DNS, dt_str))\n",
    "\n",
    "run_id_value = datetime_uuid()\n",
    "\n",
    "audit_schema = StructType([\n",
    "    StructField(\"Runid\", StringType(), True),\n",
    "    StructField(\"Unique_identifier_desc\", StringType(), True),\n",
    "    StructField(\"Unique_identifier\", StringType(), True),\n",
    "    StructField(\"Table_name\", StringType(), True),\n",
    "    StructField(\"Stage_name\", StringType(), True),\n",
    "    StructField(\"Record_count\", IntegerType(), True),\n",
    "    StructField(\"Run_dt\", TimestampType(), True),\n",
    "    StructField(\"Batch_id\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"File_name\", StringType(), True),\n",
    "    StructField(\"Status\", StringType(), True)\n",
    "])\n",
    "\n",
    "def create_audit_df(df: DataFrame, unique_identifier_desc: str, table_name: str, stage_name: str, description: str, additional_columns: list = None) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Creates an audit DataFrame and writes it to Delta format.\n",
    "\n",
    "    :param df: Input DataFrame from which unique identifiers are extracted.\n",
    "    :param unique_identifier_desc: Column name that acts as a unique identifier.\n",
    "    :param table_name: Name of the source table.\n",
    "    :param stage_name: Name of the data processing stage.\n",
    "    :param description: Description of the table.\n",
    "    :param additional_columns: List of additional columns to include in the audit DataFrame.\n",
    "    :return: DataFrame containing the audit information.\n",
    "    \"\"\"\n",
    "\n",
    "    dt_desc = datetime.utcnow()\n",
    "\n",
    "    additional_columns = additional_columns or []  # Default to an empty list if None   \n",
    "    additional_columns = [col(c) for c in additional_columns if c is not None]  # Filter out None values\n",
    "\n",
    "    audit_df = df.select(col(unique_identifier_desc).alias(\"unique_identifier\"), *additional_columns) \\\n",
    "        .withColumn(\"Runid\", lit(run_id_value)) \\\n",
    "        .withColumn(\"Unique_identifier_desc\", lit(unique_identifier_desc)) \\\n",
    "        .withColumn(\"Stage_name\", lit(stage_name)) \\\n",
    "        .withColumn(\"Table_name\", lit(table_name)) \\\n",
    "        .withColumn(\"Run_dt\", lit(dt_desc).cast(TimestampType())) \\\n",
    "        .withColumn(\"Description\", lit(description))\n",
    "\n",
    "    list_cols = audit_df.columns\n",
    "\n",
    "    final_audit_df = audit_df.groupBy(*list_cols).agg(count(\"*\").cast(IntegerType()).alias(\"Record_count\"))\n",
    "\n",
    "    # final_audit_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(audit_delta_path)\n",
    "    \n",
    "    return final_audit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf4da11e-d865-4a22-8ec7-ae1866b46213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Delta Table Path in Azure Storage\n",
    "\n",
    "\n",
    "if not DeltaTable.isDeltaTable(spark, audit_delta_path):\n",
    "    print(f\"ðŸ›‘ Delta table '{audit_delta_path}' does not exist. Creating an empty Delta table...\")\n",
    "\n",
    "    # Create an empty DataFrame\n",
    "    empty_df = spark.createDataFrame([], audit_schema)\n",
    "\n",
    "    # Write the empty DataFrame in Delta format to create the table\n",
    "    empty_df.write.format(\"delta\").mode(\"overwrite\").save(audit_delta_path)\n",
    "\n",
    "    print(\"âœ… Empty Delta table successfully created in Azure Storage.\")\n",
    "else:\n",
    "    print(f\"âš¡ Delta table '{audit_delta_path}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "484ab2a6-69b1-43bd-b6f1-3a0effd732c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "audit_params = [\n",
    "        {\n",
    "        \"unique_identifier_cols\": [\"CaseNo\", \"Forenames\", \"Name\"],\n",
    "        \"table_name\": \"bronze_ac_ca_ant_fl_dt_hc\",\n",
    "        \"stage_name\": \"bronze_stage\",\n",
    "        \"description\": \"The bronze_ac_ca_ant_fl_dt_hc table Delta Live Table combining Appeal Case data with Case Appellant, Appellant, File Location, Department, and Hearing Centre.\"\n",
    "    },\n",
    "    {\n",
    "        \"unique_identifier_cols\": [\"CaseNo\", \"Forenames\", \"Name\"],\n",
    "        \"table_name\": \"bronze_iris_extract\",\n",
    "        \"stage_name\": \"bronze_stage\",\n",
    "        \"description\": \"Delta Live Table extracted from the IRIS Tribunal decision file extract.\"\n",
    "    },\n",
    "    {\n",
    "        \"unique_identifier_cols\": [\"CaseNo\"],\n",
    "        \"table_name\": \"stg_td_filtered\",\n",
    "        \"stage_name\": \"segmentation_stage\",\n",
    "        \"description\": \"The stg_td_filtered - segmentation Table for appeal cases requiring tribunal decisions with unique list of CaseNo's\"\n",
    "    },\n",
    "    {\n",
    "        \"unique_identifier_cols\": [\"CaseNo\", \"Forenames\", \"Name\"],\n",
    "        \"table_name\": \"silver_tribunaldecision_detail\",\n",
    "        \"stage_name\": \"silver_stage\",\n",
    "        \"description\": \"The silver_tribunaldecision_detail - or Tribunal Decision information\"\n",
    "    },\n",
    "    {\n",
    "        \"unique_identifier_cols\": [\"client_identifier\", \"bf_002\", \"bf_003\"],\n",
    "        \"table_name\": \"silver_archive_metadata\",\n",
    "        \"stage_name\": \"silver_stage\",\n",
    "        \"description\": \"The silver_archive_metadata table consolidates keys metadata for Archive Metadata da\"\n",
    "    },\n",
    "    {\n",
    "        \"unique_identifier_cols\": [\"CaseNo\", \"Forenames\", \"Name\"],\n",
    "        \"table_name\": \"stg_create_td_iris_json_content\",\n",
    "        \"stage_name\": \"silver_stage\",\n",
    "        \"description\": \"The stg_create_td_iris_json_content table generates JSON content for TD cases\",\n",
    "        \"Extra_columns_mapping\": {\"File_name\": \"JSONFileName\", \"Status\": \"JSONStatus\"}\n",
    "    },\n",
    "    {\n",
    "        \"unique_identifier_cols\": [\"CaseNo\", \"Forenames\", \"Name\"],\n",
    "        \"table_name\": \"stg_create_td_iris_html_content\",\n",
    "        \"stage_name\": \"silver_stage\",\n",
    "        \"description\": \"The stg_create_td_iris_html_content table generates HTML content for TD cases\",\n",
    "        \"Extra_columns_mapping\": {\"File_name\": \"HTMLFileName\", \"Status\": \"HTMLStatus\"}\n",
    "    },\n",
    "    {\n",
    "        \"unique_identifier_cols\": [\"client_identifier\", \"bf_002\", \"bf_003\"],\n",
    "        \"table_name\": \"stg_create_td_iris_a360_content\",\n",
    "        \"stage_name\": \"silver_stage\",\n",
    "        \"description\": \"The stg_create_td_iris_a360_content table generates A360 content for TD cases\",\n",
    "        \"Extra_columns_mapping\": {\"File_name\": \"NotYetBatched\", \"Status\": \"A360Status\"}\n",
    "    },\n",
    "    {\n",
    "        \"unique_identifier_cols\": [\"CaseNo\", \"Forenames\", \"Name\"],\n",
    "        \"table_name\": \"stg_td_iris_unified\",\n",
    "        \"stage_name\": \"silver_stage\",\n",
    "        \"description\": \"The stg_td_iris_unified table generates A360 BatchId for TD cases\",\n",
    "        \"Extra_columns_mapping\": {\"File_name\": \"A360FileName\", \"Status\": \"A360Status\"}\n",
    "    },\n",
    "    {\n",
    "        \"unique_identifier_cols\": [\"CaseNo\", \"Forenames\", \"Name\"],\n",
    "        \"table_name\": \"gold_td_iris_with_html\",\n",
    "        \"stage_name\": \"silver_stage\",\n",
    "        \"description\": \"The gold_td_iris_with_html with HTML Outputs Uploded..\",\n",
    "        \"Extra_columns_mapping\": {\"File_name\": \"HTMLFileName\", \"Status\": \"UploadStatus\"}\n",
    "    },\n",
    "    {\n",
    "        \"unique_identifier_cols\": [\"CaseNo\", \"Forenames\", \"Name\"],\n",
    "        \"table_name\": \"gold_td_iris_with_json\",\n",
    "        \"stage_name\": \"silver_stage\",\n",
    "        \"description\": \"The gold_td_iris_with_json with HTML Outputs Uploded..\",\n",
    "        \"Extra_columns_mapping\": {\"File_name\": \"JSONFileName\", \"Status\": \"UploadStatus\"}\n",
    "    },\n",
    "    {\n",
    "        \"unique_identifier_cols\": [\"A360BatchId\"],\n",
    "        \"table_name\": \"gold_td_iris_with_a360\",\n",
    "        \"stage_name\": \"silver_stage\",\n",
    "        \"description\": \"The gold_td_iris_with_a360 with HTML Outputs Uploded..\",\n",
    "        \"Extra_columns_mapping\": {\"File_name\": \"A360FileName\", \"Status\": \"UploadStatus\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "audit_dataframes = []\n",
    "\n",
    "for params in audit_params:\n",
    "    table_name = params[\"table_name\"]\n",
    "    stage_name = params[\"stage_name\"]\n",
    "    unique_identifier_cols = params[\"unique_identifier_cols\"]\n",
    "    description = params[\"description\"]\n",
    "    extra_columns_mapping = params.get(\"Extra_columns_mapping\", {})\n",
    "    unique_identifier_desc = \"_\".join(unique_identifier_cols)\n",
    "\n",
    "    try:\n",
    "\n",
    "        df_logging = spark.read.table(f\"hive_metastore.ariadm_arm_td.{table_name}\")\n",
    "\n",
    "        df_audit = df_logging\n",
    "        if len(unique_identifier_cols) > 1:\n",
    "            df_audit = df_audit.withColumn(\n",
    "                unique_identifier_desc, \n",
    "                concat_ws(\"_\", *[col(c).cast(\"string\") for c in unique_identifier_cols])\n",
    "            )\n",
    "        else:\n",
    "            df_audit = df_audit.withColumn(unique_identifier_desc, col(unique_identifier_desc))\n",
    "\n",
    "        # Apply extra column mappings dynamically\n",
    "        for new_col, source_col in extra_columns_mapping.items():\n",
    "            if source_col == \"NotYetBatched\":\n",
    "                df_audit = df_audit.withColumn(new_col, lit(\"NotYetBatched\"))\n",
    "            else:\n",
    "                df_audit = df_audit.withColumn(new_col, col(source_col))\n",
    "\n",
    "        # Generate the audit DataFrame\n",
    "        df_audit_appended = create_audit_df(\n",
    "            df_audit,\n",
    "            unique_identifier_desc=unique_identifier_desc,\n",
    "            table_name=table_name,\n",
    "            stage_name=stage_name,\n",
    "            description=description\n",
    "        )\n",
    "\n",
    "        audit_dataframes.append(df_audit_appended)\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        # Table does not exist, create an audit entry for it\n",
    "        status = f\"Failed - Table {table_name} does not exist\"\n",
    "\n",
    "        row_data = {\n",
    "            \"Runid\": run_id_value,\n",
    "            \"Unique_identifier_desc\": unique_identifier_desc,\n",
    "            \"Unique_identifier\": None,\n",
    "            \"Table_name\": table_name,\n",
    "            \"Stage_name\": stage_name,\n",
    "            \"Record_count\": 0,\n",
    "            \"Run_dt\": datetime.now(),\n",
    "            \"Batch_id\": None,\n",
    "            \"Description\": description,\n",
    "            \"File_name\": None,\n",
    "            \"Status\": status\n",
    "        }\n",
    "\n",
    "        row_df = spark.createDataFrame([row_data], schema=audit_schema)\n",
    "        audit_dataframes.append(row_df)\n",
    "\n",
    "df_final_audit = audit_dataframes[0]\n",
    "for df in audit_dataframes[1:]:\n",
    "    df_final_audit = df_final_audit.unionByName(df, allowMissingColumns=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0392391e-e2c0-49dd-9854-59ba0fa9f1c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# audit_params = [\n",
    "#         {\n",
    "#         \"unique_identifier_cols\": [\"CaseNo\", \"Forenames\", \"Name\"],\n",
    "#         \"table_name\": \"bronze_ac_ca_ant_fl_dt_hc\",\n",
    "#         \"stage_name\": \"bronze_stage\",\n",
    "#         \"description\": \"The bronze_ac_ca_ant_fl_dt_hc table Delta Live Table combining Appeal Case data with Case Appellant, Appellant, File Location, Department, and Hearing Centre.\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"unique_identifier_cols\": [\"CaseNo\", \"Forenames\", \"Name\"],\n",
    "#         \"table_name\": \"bronze_iris_extract\",\n",
    "#         \"stage_name\": \"bronze_stage\",\n",
    "#         \"description\": \"Delta Live Table extracted from the IRIS Tribunal decision file extract.\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"unique_identifier_cols\": [\"CaseNo\"],\n",
    "#         \"table_name\": \"stg_td_filtered\",\n",
    "#         \"stage_name\": \"silver_stage\",\n",
    "#         \"description\": \"The stg_td_filtered - segmentation Table for appeal cases requiring tribunal decisions with unique list of CaseNo's\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"unique_identifier_cols\": [\"CaseNo\", \"Forenames\", \"Name\"],\n",
    "#         \"table_name\": \"silver_tribunaldecision_detail\",\n",
    "#         \"stage_name\": \"silver_stage\",\n",
    "#         \"description\": \"The silver_tribunaldecision_detail - or Tribunal Decision information\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"unique_identifier_cols\": [\"client_identifier\", \"bf_002\", \"bf_003\"],\n",
    "#         \"table_name\": \"silver_archive_metadata\",\n",
    "#         \"stage_name\": \"silver_stage\",\n",
    "#         \"description\": \"The silver_archive_metadata table consolidates keys metadata for Archive Metadata da\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"unique_identifier_cols\": [\"CaseNo\", \"Forenames\", \"Name\"],\n",
    "#         \"table_name\": \"stg_create_td_iris_json_content\",\n",
    "#         \"stage_name\": \"silver_stage\",\n",
    "#         \"description\": \"The stg_create_td_iris_json_content table generates JSON content for TD cases\",\n",
    "#         \"Extra_columns_mapping\": {\"File_name\": \"JSONFileName\", \"Status\": \"JSONStatus\"}\n",
    "#     },\n",
    "#     {\n",
    "#         \"unique_identifier_cols\": [\"CaseNo\", \"Forenames\", \"Name\"],\n",
    "#         \"table_name\": \"stg_create_td_iris_html_content\",\n",
    "#         \"stage_name\": \"silver_stage\",\n",
    "#         \"description\": \"The stg_create_td_iris_html_content table generates HTML content for TD cases\",\n",
    "#         \"Extra_columns_mapping\": {\"File_name\": \"HTMLFileName\", \"Status\": \"HTMLStatus\"}\n",
    "#     },\n",
    "#     {\n",
    "#         \"unique_identifier_cols\": [\"client_identifier\", \"bf_002\", \"bf_003\"],\n",
    "#         \"table_name\": \"stg_create_td_iris_a360_content\",\n",
    "#         \"stage_name\": \"silver_stage\",\n",
    "#         \"description\": \"The stg_create_td_iris_a360_content table generates A360 content for TD cases\",\n",
    "#         \"Extra_columns_mapping\": {\"File_name\": \"NotYetBatched\", \"Status\": \"A360Status\"}\n",
    "#     },\n",
    "#     {\n",
    "#         \"unique_identifier_cols\": [\"CaseNo\", \"Forenames\", \"Name\"],\n",
    "#         \"table_name\": \"stg_td_iris_unified\",\n",
    "#         \"stage_name\": \"silver_stage\",\n",
    "#         \"description\": \"The stg_td_iris_unified table generates A360 BatchId for TD cases\",\n",
    "#         \"Extra_columns_mapping\": {\"File_name\": \"A360FileName\", \"Status\": \"A360Status\"}\n",
    "#     },\n",
    "#     {\n",
    "#         \"unique_identifier_cols\": [\"CaseNo\", \"Forenames\", \"Name\"],\n",
    "#         \"table_name\": \"gold_td_iris_with_html\",\n",
    "#         \"stage_name\": \"silver_stage\",\n",
    "#         \"description\": \"The gold_td_iris_with_html with HTML Outputs Uploded..\",\n",
    "#         \"Extra_columns_mapping\": {\"File_name\": \"HTMLFileName\", \"Status\": \"UploadStatus\"}\n",
    "#     },\n",
    "#     {\n",
    "#         \"unique_identifier_cols\": [\"CaseNo\", \"Forenames\", \"Name\"],\n",
    "#         \"table_name\": \"gold_td_iris_with_json\",\n",
    "#         \"stage_name\": \"silver_stage\",\n",
    "#         \"description\": \"The gold_td_iris_with_json with HTML Outputs Uploded..\",\n",
    "#         \"Extra_columns_mapping\": {\"File_name\": \"JSONFileName\", \"Status\": \"UploadStatus\"}\n",
    "#     },\n",
    "#     {\n",
    "#         \"unique_identifier_cols\": [\"A360BatchId\"],\n",
    "#         \"table_name\": \"gold_td_iris_with_a360\",\n",
    "#         \"stage_name\": \"silver_stage\",\n",
    "#         \"description\": \"The gold_td_iris_with_a360 with HTML Outputs Uploded..\",\n",
    "#         \"Extra_columns_mapping\": {\"File_name\": \"A360FileName\", \"Status\": \"UploadStatus\"}\n",
    "#     },\n",
    "#     {\n",
    "#         \"unique_identifier_cols\": [\"A360BatchId\"],\n",
    "#         \"table_name\": \"temp\",\n",
    "#         \"stage_name\": \"silver_stage\",\n",
    "#         \"description\": \"The gold_td_iris_with_a360 with HTML Outputs Uploded..\",\n",
    "#         \"Extra_columns_mapping\": {\"File_name\": \"A360FileName\", \"Status\": \"UploadStatus\"}\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# audit_dataframes = []\n",
    "\n",
    "# for params in audit_params:\n",
    "#     table_name = params[\"table_name\"]\n",
    "#     stage_name = params[\"stage_name\"]\n",
    "#     unique_identifier_cols = params[\"unique_identifier_cols\"]\n",
    "#     description = params[\"description\"]\n",
    "#     extra_columns_mapping = params.get(\"Extra_columns_mapping\", {})\n",
    "#     unique_identifier_desc = \"_\".join(unique_identifier_cols)\n",
    "\n",
    "#     try:\n",
    "\n",
    "#         df_logging = spark.read.table(f\"hive_metastore.ariadm_arm_td.{table_name}\")\n",
    "\n",
    "#         df_audit = df_logging\n",
    "#         if len(unique_identifier_cols) > 1:\n",
    "#             df_audit = df_audit.withColumn(\n",
    "#                 unique_identifier_desc, \n",
    "#                 concat_ws(\"_\", *[col(c).cast(\"string\") for c in unique_identifier_cols])\n",
    "#             )\n",
    "#         else:\n",
    "#             df_audit = df_audit.withColumn(unique_identifier_desc, col(unique_identifier_desc))\n",
    "\n",
    "#         # Apply extra column mappings dynamically\n",
    "#         for new_col, source_col in extra_columns_mapping.items():\n",
    "#             if source_col == \"NotYetBatched\":\n",
    "#                 df_audit = df_audit.withColumn(new_col, lit(\"NotYetBatched\"))\n",
    "#             else:\n",
    "#                 df_audit = df_audit.withColumn(new_col, col(source_col))\n",
    "\n",
    "#         # Generate the audit DataFrame\n",
    "#         df_audit_appended = create_audit_df(\n",
    "#             df_audit,\n",
    "#             unique_identifier_desc=unique_identifier_desc,\n",
    "#             table_name=table_name,\n",
    "#             stage_name=stage_name,\n",
    "#             description=description\n",
    "#         )\n",
    "\n",
    "#         audit_dataframes.append(df_audit_appended)\n",
    "\n",
    "#     except Exception as e:\n",
    "\n",
    "#         # Table does not exist, create an audit entry for it\n",
    "#         status = f\"Failed - Table {table_name} does not exist\"\n",
    "\n",
    "#         row_data = {\n",
    "#             \"Runid\": run_id_value,\n",
    "#             \"Unique_identifier_desc\": unique_identifier_desc,\n",
    "#             \"Unique_identifier\": None,\n",
    "#             \"Table_name\": table_name,\n",
    "#             \"Stage_name\": stage_name,\n",
    "#             \"Record_count\": 0,\n",
    "#             \"Run_dt\": datetime.now(),\n",
    "#             \"Batch_id\": None,\n",
    "#             \"Description\": description,\n",
    "#             \"File_name\": None,\n",
    "#             \"Status\": status\n",
    "#         }\n",
    "\n",
    "#         row_df = spark.createDataFrame([row_data], schema=audit_schema)\n",
    "#         audit_dataframes.append(row_df)\n",
    "\n",
    "# df_final_audit = audit_dataframes[0]\n",
    "# for df in audit_dataframes[1:]:\n",
    "#     df_final_audit = df_final_audit.unionByName(df, allowMissingColumns=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed5ad2c6-0905-4d60-9030-eeead054f5f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_final_audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb545dd2-e117-4f63-b601-44f8cacc2b2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final_audit.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(audit_delta_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ARIADM_ARM_TD_AUDIT_DATA_PROCESSING",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
