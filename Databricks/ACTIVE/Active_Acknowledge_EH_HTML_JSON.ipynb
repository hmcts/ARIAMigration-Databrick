{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca145479-3655-4bf2-bafa-8c2894258a3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ARM Acknowledgment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81ef4be3-3c60-450c-91c7-9d09d193aea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Autoloader set up**  \n",
    "This Notebook sets up an Autoloader job that runs on a manual trigger to collect ack messages from the ack eventhubs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ab6f686-9bdf-4e7c-8537-52a7097d6569",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, LongType\n",
    "from pyspark.sql.functions import col,from_json\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c41be175-1d89-4a6f-9135-82e7f162705e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ack_schema = StructType([\n",
    "    StructField(\"file_name\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"error_message\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bf5e79f-62a2-48ec-a216-5c36b44a1a8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Set up configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4e82801-8912-4ce4-8444-98d5e636341d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/configs/config.json\")\n",
    "env = config.first()[\"env\"].strip().lower()\n",
    "lz_key = config.first()[\"lz_key\"].strip().lower()\n",
    "\n",
    "keyvault_name = f\"ingest{lz_key}-meta002-{env}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0c451d4-fa8a-448d-80bb-1e8e9c49cd5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Access the Service Principle secrets from keyvaults\n",
    "client_secret = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-SECRET')\n",
    "tenant_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-TENANT-ID')\n",
    "client_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13a173df-b554-4f68-a341-23a44c35e2f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "EH_NAMESPACE = f\"ingest{lz_key}-integration-eventHubNamespace001-{env}\"\n",
    "EH_NAME = f\"evh-active-pub-{env}-{lz_key}-uks-dlrm-01\" #To create this Eventhub in the UI\n",
    "\n",
    "connection_string = dbutils.secrets.get(keyvault_name, \"RootManageSharedAccessKey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5db00bd6-0302-4a5b-b55c-fb2f2a0cb89c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "KAFKA_OPTIONS = {\n",
    "    \"kafka.bootstrap.servers\": f\"{EH_NAMESPACE}.servicebus.windows.net:9093\",\n",
    "    \"subscribe\": EH_NAME,\n",
    "    \"consumer.group.id\": \"active\",\n",
    "    # \"startingOffsets\": \"earliest\",\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"failOnDataLoss\": \"false\",\n",
    "    \"startingOffsets\": \"latest\",\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.sasl.jaas.config\": f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{connection_string}\";'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9927a08-6a4a-40b6-b0dd-bb8e731dc0de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lets loop here \n",
    "curated_storage_account = f\"ingest{lz_key}curated{env}\"\n",
    "checkpoint_storage_account = f\"ingest{lz_key}xcutting{env}\"\n",
    "\n",
    "##Assign OAuth to curated storage account\n",
    "storage_accounts = [curated_storage_account, checkpoint_storage_account]\n",
    "\n",
    "for storage_account in storage_accounts:\n",
    "    configs = {\n",
    "            f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\": \"OAuth\",\n",
    "            f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\":\n",
    "                \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "            f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\": client_id,\n",
    "            f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\": client_secret,\n",
    "            f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\":\n",
    "                f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "        }\n",
    "    for key,val in configs.items():\n",
    "        spark.conf.set(key,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a1c150e-a9ab-4fda-afad-2f1c06f82424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, from_json\n",
    "# import time\n",
    "\n",
    "# states = [\n",
    "#     \"paymentPending\", \n",
    "#     \"appealSubmitted\", \n",
    "#     \"awaitingRespondentEvidence(a)\", \n",
    "#     \"awaitingRespondentEvidence(b)\", \n",
    "#     \"caseUnderReview\", \n",
    "#     \"reasonForAppealSubmitted\", \n",
    "#     \"listing\",\n",
    "#     \"PrepareForHearing\",\n",
    "#     \"Decision\",\n",
    "#     \"FTPA Submitted (a)\",\n",
    "#     \"FTPA Submitted (b)\",\n",
    "#     \"Decided (b)\",\n",
    "#     \"Decided (a)\",\n",
    "#     \"FTPA Decided\",\n",
    "#     \"Ended\",\n",
    "#     \"Remitted\"\n",
    "# ]\n",
    "\n",
    "# ## Map each state to a Kafka partition (if needed downstream)\n",
    "# state_partition_map = {state: i for i, state in enumerate(states)}\n",
    "\n",
    "# for state in states:\n",
    "#     print(f\"Starting stream for state: {state}\")\n",
    "#     try:\n",
    "#         data_path = f\"abfss://silver@ingest{lz_key}curated{env}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/{state}/publish_audit_db_eh/\"\n",
    "#         checkpoint_path = f\"abfss://db-ack-checkpoint@ingest{lz_key}xcutting{env}.dfs.core.windows.net/{state}/ACK/\"\n",
    "\n",
    "#         KAFKA_OPTIONS = {\n",
    "#             \"kafka.bootstrap.servers\": f\"{EH_NAMESPACE}.servicebus.windows.net:9093\",\n",
    "#             \"subscribe\": EH_NAME,\n",
    "#             \"kafka.group.id\": state,   # consumer group tied to this state\n",
    "#             \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "#             \"failOnDataLoss\": \"false\",\n",
    "#             \"startingOffsets\": \"latest\",\n",
    "#             \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "#             \"kafka.sasl.jaas.config\": f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{connection_string}\";'\n",
    "#         }\n",
    "\n",
    "#         # Start Kafka job\n",
    "#         eventhubdf = spark.readStream.format(\"kafka\")\\\n",
    "#             .options(**KAFKA_OPTIONS)\\\n",
    "#             .load()\n",
    "\n",
    "#         parsed_df = (\n",
    "#             eventhubdf\n",
    "#             .select(col(\"value\").cast(\"string\").alias(\"json_str\"))\n",
    "#             .select(from_json(col(\"json_str\"), ack_schema).alias(\"json_obj\"))\n",
    "#             .select(\"json_obj.*\")\n",
    "#         )\n",
    "\n",
    "#         query = parsed_df.writeStream \\\n",
    "#             .format(\"delta\") \\\n",
    "#             .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "#             .outputMode(\"append\") \\\n",
    "#             .start(data_path)\n",
    "\n",
    "#         # Wait some time for this state to ingest data\n",
    "#         time.sleep(30)\n",
    "#         query.stop()\n",
    "\n",
    "#         df = spark.read.format(\"delta\").load(data_path)\n",
    "#         print(f\"Completed writing for {state}, total rows: {df.count()}\")\n",
    "#         display(df)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Skipping state {state} due to error: {str(e)}\")\n",
    "#         continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef36d9d4-10f9-499a-86fe-167a0fe8f4eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json\n",
    "import time\n",
    "\n",
    "states = [\n",
    "    \"paymentPending\", \n",
    "    \"appealSubmitted\", \n",
    "    \"awaitingRespondentEvidence(a)\", \n",
    "    \"awaitingRespondentEvidence(b)\", \n",
    "    \"caseUnderReview\", \n",
    "    \"reasonForAppealSubmitted\", \n",
    "    \"listing\",\n",
    "    \"PrepareForHearing\",\n",
    "    \"Decision\",\n",
    "    \"FTPA Submitted (a)\",\n",
    "    \"FTPA Submitted (b)\",\n",
    "    \"Decided (b)\",\n",
    "    \"Decided (a)\",\n",
    "    \"FTPA Decided\",\n",
    "    \"Ended\",\n",
    "    \"Remitted\"\n",
    "]\n",
    "\n",
    "##Create a 1:1 map between partition and consumer group\n",
    "# state_partition_map = {state: i for i, state in enumerate(states)}\n",
    "\n",
    "KAFKA_OPTIONS = {\n",
    "\"kafka.bootstrap.servers\": f\"{EH_NAMESPACE}.servicebus.windows.net:9093\",\n",
    "\"subscribe\": EH_NAME,\n",
    "\"kafka.group.id\": state,\n",
    "\"kafka.security.protocol\": \"SASL_SSL\",\n",
    "\"failOnDataLoss\": \"false\",\n",
    "\"startingOffsets\": \"latest\",\n",
    "\"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "\"kafka.sasl.jaas.config\": f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{connection_string}\";'\n",
    "}\n",
    "\n",
    "\n",
    "state_paths = []\n",
    "checkpoint_paths = []\n",
    "\n",
    "for state in states:\n",
    "\n",
    "    if state == \"paymentPending\":\n",
    "\n",
    "        data_path = f\"abfss://silver@ingest{lz_key}curated{env}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/{state}/publish_audit_db_eh/\"\n",
    "        checkpoint_path = f\"abfss://db-ack-checkpoint@ingest{lz_key}xcutting{env}.dfs.core.windows.net/{state}/ACK/\"\n",
    "\n",
    "        state_paths.append(data_path)\n",
    "        checkpoint_paths.append(checkpoint_path)\n",
    "\n",
    "        ## Start Kafka job to consume/read rows from the EventHub\n",
    "        eventhubdf = spark.readStream.format(\"kafka\")\\\n",
    "            .options(**KAFKA_OPTIONS)\\\n",
    "            .load()\n",
    "\n",
    "        ## Select columns of interest from ack_schema that is streamed from the EventHub\n",
    "        parsed_df = (\n",
    "            eventhubdf\n",
    "            # 'body' is binary, so we cast to string (assuming UTF-8)\n",
    "            .select(col(\"value\").cast(\"string\").alias(\"json_str\"))\n",
    "            .select(from_json(col(\"json_str\"), ack_schema).alias(\"json_obj\"))\n",
    "            .select(\"json_obj.*\")\n",
    "        )\n",
    "\n",
    "        ## Write the stream to the relevant state checkpoint/data path one state at a time\n",
    "        query = parsed_df.writeStream \\\n",
    "            .format(\"delta\") \\\n",
    "            .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "            .outputMode(\"append\") \\\n",
    "            .start(data_path)\n",
    "\n",
    "        ## Wait 30 seconds to write all data to relevant location then stop the stream\n",
    "        time.sleep(30)\n",
    "        query.stop()\n",
    "\n",
    "        df = spark.read.format(\"delta\") \\\n",
    "        .load(data_path) \n",
    "        # .filter(col(\"status\").isNotNull()) #86 non null. 1720 total records. to query\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "902afeeb-686d-4375-8363-0f6491e509e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Notebook completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f9a8f91-0c54-4957-bc09-7d3ce45a7ac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Delete the entire APPEALS directory structure\n",
    "dbutils.fs.rm(\"abfss://silver@ingest00curatedsbox.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/\", True)\n",
    "\n",
    "# Verify it's gone\n",
    "try:\n",
    "    dbutils.fs.ls(\"abfss://silver@ingest00curatedsbox.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/\")\n",
    "    print(\"APPEALS directory still exists\")\n",
    "except:\n",
    "    print(\"APPEALS directory successfully deleted - clean slate!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84948942-51c2-4205-b0ca-ee81706faffd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Active_Acknowledge_EH_HTML_JSON",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
