{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "78f8041a-0f60-4a0c-b858-eb3d5975bf99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Load Config and Setup Enviorment Variables\n",
    "config = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/configs/config.json\")\n",
    "env_name = config.first()[\"env\"].strip().lower()\n",
    "lz_key = config.first()[\"lz_key\"].strip().lower()\n",
    " \n",
    "# print(f\"env_code: {lz_key}\")  # This won't be redacted\n",
    "# print(f\"env_name: {env_name}\")  # This won't be redacted\n",
    " \n",
    "KeyVault_name = f\"ingest{lz_key}-meta002-{env_name}\"\n",
    "# print(f\"KeyVault_name: {KeyVault_name}\")\n",
    " \n",
    "# Service principal credentials\n",
    "client_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-ID\")\n",
    "client_secret = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-CLIENT-SECRET\")\n",
    "tenant_id = dbutils.secrets.get(KeyVault_name, \"SERVICE-PRINCIPLE-TENANT-ID\")\n",
    " \n",
    "# Storage account names\n",
    "curated_storage = f\"ingest{lz_key}curated{env_name}\"\n",
    "checkpoint_storage = f\"ingest{lz_key}xcutting{env_name}\"\n",
    "raw_storage = f\"ingest{lz_key}raw{env_name}\"\n",
    "landing_storage = f\"ingest{lz_key}landing{env_name}\"\n",
    "external_storage = f\"ingest{lz_key}external{env_name}\"\n",
    "  \n",
    "# Spark config for curated storage (Delta table)\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{curated_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{curated_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{curated_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{curated_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{curated_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{checkpoint_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{checkpoint_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{checkpoint_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{checkpoint_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{checkpoint_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{raw_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{raw_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{raw_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{raw_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{raw_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{landing_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{landing_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{landing_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{landing_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{landing_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    " \n",
    " \n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{external_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{external_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{external_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{external_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{external_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "  \n",
    "# Setting variables for use in subsequent cells\n",
    "bronze_path = f\"abfss://bronze@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/\"\n",
    "silver_path = f\"abfss://silver@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/\"\n",
    "# audit_path = f\"abfss://silver@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/AUDIT/{state_under_test}\"\n",
    "# gold_path = f\"abfss://gold@ingest{lz_key}curated{env_name}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/{state_under_test}\"\n",
    " \n",
    " \n",
    "# Print all variables\n",
    "# variables = {\n",
    "#     # \"read_hive\": read_hive,\n",
    "    \n",
    "#     \"bronze_path\": bronze_path,\n",
    "#     \"silver_path\": silver_path,\n",
    "#     \"audit_path\": audit_path,\n",
    "#     \"gold_path\": gold_path,\n",
    "#     \"key_vault\": KeyVault_name,\n",
    "#     \"AppealState\": state_under_test\n",
    " \n",
    "# }\n",
    " \n",
    "# display(variables)\n",
    "\n",
    "import json\n",
    "\n",
    "#Get Latest Json Folder\n",
    "# json_location = dbutils.fs.ls(f\"{gold_path}/\")[-1]\n",
    "# latest_json_location = json_location.name\n",
    "# dbutils.fs.ls(f\"{gold_path}/{latest_json_location}\")\n",
    "\n",
    "#Set Paths\n",
    "try: \n",
    "    #json_path = f\"{gold_path}/{latest_json_location}/JSON/\"\n",
    "    # json_path = f\"{gold_path}/{latest_json_location}/INVALID_JSON/\"\n",
    "    M1_silver = f\"{silver_path}/silver_appealcase_detail\"\n",
    "    M1_bronze = f\"{bronze_path}/bronze_appealcase_crep_rep_floc_cspon_cfs\"\n",
    "    M2_silver = f\"{silver_path}/silver_caseapplicant_detail\"\n",
    "    M3_silver = f\"{silver_path}/silver_status_detail\"\n",
    "    C = f\"{silver_path}/silver_appealcategory_detail\"\n",
    "    bhc = f\"{bronze_path}/bronze_hearing_centres\"\n",
    "    bat = f\"{bronze_path}/bronze_appealtype\" \n",
    "    docsr = f\"{bronze_path}/bronze_documentsreceived\"   \n",
    "except:\n",
    "    print(f\"Error during fetch: {str(e)}\")\n",
    "\n",
    "#Create and Load Dataframes\n",
    "# json_data = spark.read.format(\"json\").load(json_path)\n",
    "M1_silver = spark.read.format(\"delta\").load(M1_silver)\n",
    "M1_bronze = spark.read.format(\"delta\").load(M1_bronze)\n",
    "M2_silver = spark.read.format(\"delta\").load(M2_silver)\n",
    "M3_silver = spark.read.format(\"delta\").load(M3_silver)\n",
    "C = spark.read.format(\"delta\").load(C)\n",
    "bhc = spark.read.format(\"delta\").load(bhc)\n",
    "bat = spark.read.format(\"delta\").load(bat)\n",
    "docsr = spark.read.format(\"delta\").load(docsr)\n",
    "\n",
    "#Can be removed later, added to allow developing of code in this notebook to begin with before moving to func files\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, lit, array, struct, collect_list, \n",
    "    max as spark_max, date_format, row_number, expr, \n",
    "    size, udf, coalesce, concat_ws, concat, trim, year, split, datediff,\n",
    "    collect_set, current_timestamp,transform, first, array_contains\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96cb78db-fd13-4734-9d46-e096cfdada79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#Code to quickly reload Libraries during Development\n",
    "######################\n",
    "# import importlib\n",
    "# import functions.compare_data_helper as datahelper\n",
    "# import functions.sql_helper as sqlhelper\n",
    "# import functions.parquet_helper as pqhelper\n",
    "# import functions.test_run_helper as run_test\n",
    "# import reporting.parquet_to_bronze_html_report as html\n",
    "# import parquet_to_bronze_tests.Bronze_M1_Test as m1test\n",
    "# import parquet_to_bronze_tests.Bronze_M2_Test as m2test\n",
    "# import parquet_to_bronze_tests.Bronze_M3_Test as m3test\n",
    "# import parquet_to_bronze_tests.Bronze_M4_Test as m4test\n",
    "# import parquet_to_bronze_tests.Bronze_M5_Test as m5test\n",
    "# import parquet_to_bronze_tests.Bronze_M6_Test as m6test\n",
    "# import parquet_to_bronze_tests.Bronze_C_Test as ctest\n",
    "# import parquet_to_bronze_tests.Bronze_D_Test as dtest\n",
    "# import parquet_to_bronze_tests.Bronze_H_Test as htest\n",
    "\n",
    "\n",
    "# importlib.reload(datahelper)\n",
    "# importlib.reload(sqlhelper)\n",
    "# importlib.reload(pqhelper)\n",
    "# importlib.reload(run_test)\n",
    "# importlib.reload(html)\n",
    "# importlib.reload(m1test)\n",
    "# importlib.reload(m2test)\n",
    "# importlib.reload(m3test)\n",
    "# importlib.reload(m4test)\n",
    "# importlib.reload(m5test)\n",
    "# importlib.reload(ctest)\n",
    "# importlib.reload(dtest)\n",
    "# importlib.reload(htest)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bd32d2d-1fcb-44d8-ba03-7e613660bab1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768561443495}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#################\n",
    "#PARQUET TO BRONZE ARIA TEST\n",
    "\n",
    "#Status - 9 Tables - 8 passing, 1 WIP (M3 table)\n",
    "##################\n",
    "\n",
    "test_results_path= \"/Workspace/Users/peter.gresty@hmcts.net/Results/Bronze_To_Parquet\"\n",
    "\n",
    "##########################\n",
    "#MAIN SETUP FOR ALL TESTS\n",
    "##########################\n",
    "import functions.compare_data_helper as datahelper\n",
    "import functions.sql_helper as sqlhelper\n",
    "import functions.parquet_helper as pqhelper\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "import os\n",
    "import reporting.csv_report as csv\n",
    "import reporting.parquet_to_bronze_html_report as html\n",
    "import functions.test_run_helper as run_test\n",
    "##Import Tests\n",
    "import parquet_to_bronze_tests.Bronze_M1_Test as m1test\n",
    "import parquet_to_bronze_tests.Bronze_M2_Test as m2test\n",
    "import parquet_to_bronze_tests.Bronze_M3_Test as m3test\n",
    "import parquet_to_bronze_tests.Bronze_M4_Test as m4test\n",
    "import parquet_to_bronze_tests.Bronze_M5_Test as m5test\n",
    "import parquet_to_bronze_tests.Bronze_M6_Test as m6test\n",
    "import parquet_to_bronze_tests.Bronze_C_Test as ctest\n",
    "import parquet_to_bronze_tests.Bronze_D_Test as dtest\n",
    "import parquet_to_bronze_tests.Bronze_H_Test as htest\n",
    "\n",
    "PARQUET_BASE_PATH = \"abfss://landing@ingest00landingstg.dfs.core.windows.net/SQLServer/Sales/IRIS/dbo/\"\n",
    "results = []\n",
    "\n",
    "##############\n",
    "# Test Config\n",
    "##############\n",
    "results = []\n",
    "config = {\n",
    "    \"pqhelper\":pqhelper,\n",
    "    \"PARQUET_BASE_PATH\": PARQUET_BASE_PATH ,\n",
    "    \"dbutils\": dbutils,\n",
    "    \"spark\":spark,\n",
    "    \"run_tests\":run_test,\n",
    "}\n",
    "#######################\n",
    "#Tests\n",
    "#######################\n",
    "all_results = []\n",
    "\n",
    "#Passing\n",
    "#Parquet To M1 Bronze\n",
    "results, result_text,sql_h,pq_h  = m1test.test_m1_bronze_table(config)\n",
    "all_results.extend(results)\n",
    "print(result_text)\n",
    "# display(sql_h.orderBy(\"_row_hash\"))\n",
    "# display(pq_h.orderBy(\"_row_hash\"))\n",
    "\n",
    "#Passing\n",
    "#Parquet To M2 Bronze\n",
    "# results, result_text,sql_h,pq_h  = m2test.test_m2_bronze_table(config)\n",
    "# all_results.extend(results)\n",
    "# print(result_text)\n",
    "\n",
    "#WIP ? - Not quite finished\n",
    "#Parquet To M3 Bronze\n",
    "# results, result_text,sql_h,pq_h  = m3test.test_m3_bronze_table(config)\n",
    "# all_results.extend(results)\n",
    "# print(result_text)\n",
    "\n",
    "#Passing\n",
    "#Parquet To M4 Bronze\n",
    "# results, result_text,sql_h,pq_h  = m4test.test_m4_bronze_table(config)\n",
    "# all_results.extend(results)\n",
    "# print(result_text)\n",
    "\n",
    "#Passing\n",
    "#Parquet To M5 Bronze\n",
    "# results, result_text,sql_h,pq_h  = m5test.test_m5_bronze_table(config)\n",
    "# all_results.extend(results)\n",
    "# print(result_text)\n",
    "\n",
    "#Passing\n",
    "#Parquet To M6 Bronze\n",
    "# results, result_text,sql_h,pq_h  = m6test.test_m6_bronze_table(config)\n",
    "# all_results.extend(results)\n",
    "# print(result_text)\n",
    "\n",
    "#Passing\n",
    "#Parquet To C Bronze\n",
    "# results, result_text,sql_h,pq_h  = ctest.test_c_bronze_table(config)\n",
    "# all_results.extend(results)\n",
    "# print(result_text)\n",
    "\n",
    "#Passing\n",
    "# #Parquet To D Bronze\n",
    "# results, result_text,sql_h,pq_h  = dtest.test_d_bronze_table(config)\n",
    "# all_results.extend(results)\n",
    "# print(result_text)\n",
    "# display(sql_h.orderBy(\"_row_hash\"))\n",
    "# display(pq_h.orderBy(\"_row_hash\"))\n",
    "\n",
    "# #Parquet To H Bronze\n",
    "# results, result_text,sql_h,pq_h  = htest.test_h_bronze_table(config)\n",
    "# all_results.extend(results)\n",
    "# print(result_text)\n",
    "# display(sql_h.orderBy(\"_row_hash\"))\n",
    "# display(pq_h.orderBy(\"_row_hash\"))\n",
    "\n",
    "\n",
    "\n",
    "##############\n",
    "#Display Results\n",
    "##############\n",
    "ordered_cols = [\n",
    "    \"table_name\",\n",
    "    \"test_type\",\n",
    "    \"status\",\n",
    "    \"message\",    \n",
    "]\n",
    "results_df = spark.createDataFrame(all_results)\n",
    "results_ordered_df = results_df.select(*ordered_cols)\n",
    "display(results_ordered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a56426e-b189-4155-ba14-3fa53ac911f9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Different ways to get CaseNo values"
    }
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "#REPORTS AND RESULTS\n",
    "#####################\n",
    "#display Results\n",
    "ordered_cols = [\n",
    "    \"table_name\",\n",
    "    \"test_type\",\n",
    "    \"status\",\n",
    "    \"message\",    \n",
    "]\n",
    "results_df = spark.createDataFrame(all_results)\n",
    "results_ordered_df = results_df.select(*ordered_cols)\n",
    "display(results_ordered_df)\n",
    "\n",
    "#generate Results Folder\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = f\"{test_results_path}/{timestamp}/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#TODO - Choose output format\n",
    "# Generate HTML report\n",
    "html_report_path = html.generate_html_report(all_results,output_dir,timestamp)\n",
    "\n",
    "#Generate CSV Report\n",
    "csv_report_path = csv.generate_csv_report(all_results,output_dir)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "parquet_to_bronze_testing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
