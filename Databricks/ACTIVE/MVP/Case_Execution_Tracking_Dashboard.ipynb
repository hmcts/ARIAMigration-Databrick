{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "282a1963-cd5d-4076-a466-330fd4eed639",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install confluent_kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4609d39-8e6c-41d5-9abf-392a9ee56455",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Libraries"
    }
   },
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "import json\n",
    "from  itertools import islice\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import col, decode, split, element_at, udf, lit, reduce, from_json, regexp_replace, concat\n",
    "import logging\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DateType\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark import SparkContext\n",
    "import os\n",
    "from functools import reduce\n",
    "import time\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64cf17e7-1a72-4e5d-b50f-835f5f169017",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initialise logging"
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"DatabricksWorkflow\")\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "if not logger.hasHandlers():\n",
    "    logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3010f725-1597-452d-89a1-d3636edff2ad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set-up cnfigs"
    }
   },
   "outputs": [],
   "source": [
    "config_path = \"dbfs:/configs/config.json\"\n",
    "try:\n",
    "    config = spark.read.option(\"multiline\", \"true\").json(config_path)\n",
    "    logger.info(f\"Successfully read config file from {config_path}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not read config file at {config_path}: {e}\", exc_info=True)\n",
    "    raise FileNotFoundError(f\"Could not read config file at {config_path}: {e}\")\n",
    "\n",
    "try:\n",
    "    first_row = config.first()\n",
    "    env = first_row[\"env\"].strip().lower()\n",
    "    lz_key = first_row[\"lz_key\"].strip().lower()\n",
    "    logger.info(f\"Extracted configs: env={env}, lz_key={lz_key}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Missing expected keys 'env' or 'lz_key' in config file: {e}\", exc_info=True)\n",
    "    raise KeyError(f\"Missing expected keys 'env' or 'lz_key' in config file: {e}\")\n",
    "\n",
    "try:\n",
    "    keyvault_name = f\"ingest{lz_key}-meta002-{env}\"\n",
    "    logger.info(f\"Constructed keyvault name: {keyvault_name}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error constructing keyvault name: {e}\", exc_info=True)\n",
    "    raise ValueError(f\"Error constructing keyvault name: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daf71926-d55f-4e3c-be2a-8ae9808b5390",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set-up OAuth"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    client_secret = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-SECRET')\n",
    "    logger.info(\"Successfully retrieved SERVICE-PRINCIPLE-CLIENT-SECRET from Key Vault\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not retrieve 'SERVICE-PRINCIPLE-CLIENT-SECRET' from Key Vault '{keyvault_name}': {e}\", exc_info=True)\n",
    "    raise KeyError(f\"Could not retrieve 'SERVICE-PRINCIPLE-CLIENT-SECRET' from Key Vault '{keyvault_name}': {e}\")\n",
    "\n",
    "try:\n",
    "    tenant_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-TENANT-ID')\n",
    "    logger.info(\"Successfully retrieved SERVICE-PRINCIPLE-TENANT-ID from Key Vault\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not retrieve 'SERVICE-PRINCIPLE-TENANT-ID' from Key Vault '{keyvault_name}': {e}\", exc_info=True)\n",
    "    raise KeyError(f\"Could not retrieve 'SERVICE-PRINCIPLE-TENANT-ID' from Key Vault '{keyvault_name}': {e}\")\n",
    "\n",
    "try:\n",
    "    client_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-ID')\n",
    "    logger.info(\"Successfully retrieved SERVICE-PRINCIPLE-CLIENT-ID from Key Vault\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not retrieve 'SERVICE-PRINCIPLE-CLIENT-ID' from Key Vault '{keyvault_name}': {e}\", exc_info=True)\n",
    "    raise KeyError(f\"Could not retrieve 'SERVICE-PRINCIPLE-CLIENT-ID' from Key Vault '{keyvault_name}': {e}\")\n",
    "\n",
    "logger.info(\"✅ Successfully retrieved all Service Principal secrets from Key Vault\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "881b1297-abd6-47b9-83f3-0111b4a47cd8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Assign OAuth"
    }
   },
   "outputs": [],
   "source": [
    "# --- Parameterise containers ---\n",
    "curated_storage_account = f\"ingest{lz_key}curated{env}\"\n",
    "curated_container = \"gold\"\n",
    "silver_curated_container = \"silver\"\n",
    "checkpoint_storage_account = f\"ingest{lz_key}xcutting{env}\"\n",
    "\n",
    "# --- Assign OAuth to storage accounts ---\n",
    "storage_accounts = [curated_storage_account, checkpoint_storage_account]\n",
    "\n",
    "for storage_account in storage_accounts:\n",
    "    try:\n",
    "        configs = {\n",
    "            f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\": \"OAuth\",\n",
    "            f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\":\n",
    "                \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "            f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\": client_id,\n",
    "            f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\": client_secret,\n",
    "            f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\":\n",
    "                f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "        }\n",
    "\n",
    "        for key, val in configs.items():\n",
    "            try:\n",
    "                spark.conf.set(key, val)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to set Spark config '{key}' for storage account '{storage_account}': {e}\", exc_info=True)\n",
    "                raise RuntimeError(f\"Failed to set Spark config '{key}' for storage account '{storage_account}': {e}\")\n",
    "\n",
    "        logger.info(f\"✅ Successfully configured OAuth for storage account: {storage_account}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error configuring OAuth for storage account '{storage_account}': {e}\", exc_info=True)\n",
    "        raise RuntimeError(f\"Error configuring OAuth for storage account '{storage_account}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86c6ca78-8d30-4ebc-a762-4adb3a11fdc6",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764156534467}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1764156634553}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": "From CCD Publish Payload to CCD Call Result"
    }
   },
   "outputs": [],
   "source": [
    "ccd_call_result = spark.read.format(\"delta\").load(f\"abfss://silver@ingest{lz_key}curated{env}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/AUDIT/APPEALS/all_active_states/ack_audit\")\n",
    "ccd_call_result.createOrReplaceTempView(\"ccd_call_result\")\n",
    "\n",
    "ccd_publish_payload_result = spark.read.format(\"delta\").load(f\"abfss://{silver_curated_container}@{curated_storage_account}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/publish_layload_audit\")\n",
    "ccd_publish_payload_result.createOrReplaceTempView(\"ccd_publish_payload_result\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT \n",
    "          \n",
    "        COALESCE(t2.runid, t1.runid) as RunID,\n",
    "        COALESCE(t2.state, t1.state) as State,\n",
    "        split_part(COALESCE(t2.CaseNo, t1.CaseNo), \".\", 1) as CaseNo,\n",
    "        t1.status as `Publish Payload Status`,\n",
    "        t2.status as `CCD Call Status`,\n",
    "        t1.PublishingDateTime as `CCD Publish Payload Publishing Date Time`,\n",
    "        t2.error as `CCD Call Function App Error`,\n",
    "        t2.StartDateTime as `CCD Call Function App Start Date Time`,\n",
    "        t2.EndDateTime as `CCD Call Function App End Date Time`,\n",
    "        t2.CCDCaseID as `CCD Case ID`,\n",
    "        t1.error as `CCD Publish Payload Error`\n",
    "          \n",
    "          FROM ccd_publish_payload_result t1\n",
    "          full outer join ccd_call_result t2 on t2.CaseNo = t1.Filename and t1.state = t2.state\n",
    "\n",
    "          -- WHERE t2.StartDateTime >= '2025-11-28 11:40:00' and t1.PublishingDateTime >= '2025-11-28 11:40:00'\n",
    "          -- ORDER BY t1.PublishingDateTime DESC\n",
    "          \n",
    "          \"\"\").display()\n",
    "        #   and t1.RunID = t2.RunID uncomment once we have working runs and add to join conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce182cab-b0b0-4922-bb86-543d0a44875b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Retrieve latest Valid/Invalid JSON per state"
    }
   },
   "outputs": [],
   "source": [
    "valid_json_list = []\n",
    "invalid_json_list = []\n",
    "\n",
    "#, \"appealSubmitted\", \"awaitingRespondentEvidence(a)\"\n",
    "# \"awaitingRespondentEvidence(b)\", \"reasonsForAppealSubmitted\", \"caseUnderReview\"\n",
    "states = [\"paymentPending\"]\n",
    "\n",
    "#For each state, retrieve the most recent gold output from DLT invalid + valid files (folder level)\n",
    "for state in states:\n",
    "    gold_files_base_path = (\n",
    "        f\"abfss://{curated_container}@{curated_storage_account}.dfs.core.windows.net/ARIADM/ACTIVE/CCD/APPEALS/{state}/\")\n",
    "    folders = dbutils.fs.ls(gold_files_base_path)\n",
    "    if not folders:\n",
    "        logger.warning(f\"No folders found for {state}\")\n",
    "        continue\n",
    "    latest_folder = folders[-1]\n",
    "    valid_path = latest_folder.path + \"JSON/\"\n",
    "    invalid_path = latest_folder.path + \"INVALID_JSON/\"\n",
    "\n",
    "    #Query if any data exists in the file path\n",
    "    try:\n",
    "        valid_files = dbutils.fs.ls(valid_path)\n",
    "        if valid_files:\n",
    "            valid_json_list.append((valid_path))\n",
    "        else:\n",
    "            logger.warning(f\"Empty valid JSON folder for {state}\")\n",
    "    except Exception:\n",
    "        logger.warning(f\"No valid JSON folder found for {state}\")\n",
    "\n",
    "    #Query if any data exists in the file path\n",
    "    try:\n",
    "        invalid_files = dbutils.fs.ls(invalid_path)\n",
    "        if invalid_files:\n",
    "            invalid_json_list.append((invalid_path))\n",
    "        else:\n",
    "            logger.warning(f\"Empty invalid JSON folder for {state}\")\n",
    "    except Exception:\n",
    "        logger.warning(f\"No invalid JSON folder found for {state}\")\n",
    "\n",
    "valid_df_list = []\n",
    "invalid_df_list = []\n",
    "\n",
    "#Retrieve the path for each file in the valid/invalid folders and create a dataframe\n",
    "for path in valid_json_list:\n",
    "    valid_files = dbutils.fs.ls(path)\n",
    "\n",
    "    for f in valid_files:\n",
    "        valid_df_list.append({\"file_name\": f.name, \"file_path\": f.path})\n",
    "\n",
    "for path in invalid_json_list:\n",
    "    invalid_files = dbutils.fs.ls(path)\n",
    "\n",
    "    for f in invalid_files:\n",
    "        invalid_df_list.append({\"file_name\": f.name, \"file_path\": f.path})\n",
    "\n",
    "#Pull out the state from the file_path\n",
    "valid_df = spark.createDataFrame(valid_df_list)\n",
    "valid_df = valid_df.withColumn(\"State\", element_at(split(col(\"file_path\"), \"/\"), -4))\n",
    "\n",
    "invalid_df = spark.createDataFrame(invalid_df_list)\n",
    "invalid_df = invalid_df.withColumn(\"State\", element_at(split(col(\"file_path\"), \"/\"), -4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "108ccb38-2f2f-4d02-ba8d-d9746c430fe5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Segmentation Table Creation"
    }
   },
   "outputs": [],
   "source": [
    "segmentation_df = spark.read.table('hive_metastore.paymentpending_gold.stg_main_payment_pending_validation')\n",
    "# segmentation_df.display()\n",
    "segmentation_df = segmentation_df.withColumn(\"CaseNo\", concat(lit(\"APPEALS_\"), regexp_replace(col(\"CaseNo\"), \"/\", \"_\"), lit(\".json\"))).select(col(\"CaseNo\"), col(\"ariaDesiredState\"))\n",
    "segmentation_df.createOrReplaceTempView(\"segmentation_state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07c0b8d9-554f-4828-aa14-acec9823bddb",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"CaseNo\":215},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764334969245}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "From Segmentation to CCD Call Result"
    }
   },
   "outputs": [],
   "source": [
    "#Create views that we can use to query the data\n",
    "valid_df.createOrReplaceTempView(\"valid_json_files\")\n",
    "invalid_df.createOrReplaceTempView(\"invalid_json_files\")\n",
    "\n",
    "## Query to trace data for each state from segmentation (gold layer) through valid/invalid\n",
    "## Through each layer of the CCD pipeline (publish, case creation, validation, submission, results)\n",
    "spark.sql('''\n",
    "        with segmentation_state_cte as (\n",
    "        SELECT \n",
    "        split_part(CaseNo, '.', 1) AS CaseNo,\n",
    "        ariaDesiredState as State,\n",
    "        'Yes' as `Segmentation State`\n",
    "        FROM segmentation_state),\n",
    "\n",
    "        valid_invalid_json_cte as (\n",
    "        SELECT\n",
    "        split_part(file_name, '.', 1) AS CaseNo,\n",
    "        state AS State,\n",
    "        'Invalid' AS `Validation Status`\n",
    "        FROM invalid_json_files\n",
    "\n",
    "        UNION ALL\n",
    "\n",
    "        SELECT\n",
    "        split_part(file_name, '.', 1) AS CaseNo,\n",
    "        state AS State,\n",
    "        'Valid' AS `Validation Status`\n",
    "        FROM valid_json_files),\n",
    "\n",
    "        pub_payload_call_result_cte as (\n",
    "        SELECT \n",
    "        COALESCE(t2.runid, t1.runid) as RunID,\n",
    "        COALESCE(t2.state, t1.state) as State,\n",
    "        split_part(COALESCE(t2.CaseNo, t1.CaseNo), \".\", 1) as CaseNo,\n",
    "        t1.status as `CCD Publish Payload Status`,\n",
    "        t2.status as `CCD Call Status`,\n",
    "        t1.PublishingDateTime as `CCD Publish Payload Publishing Date Time`,\n",
    "        t2.error as `CCD Call Function App Error`,\n",
    "        t2.StartDateTime as `CCD Call Function App Start Date Time`,\n",
    "        t2.EndDateTime as `CCD Call Function App End Date Time`,\n",
    "        t2.CCDCaseID as `CCD Case ID`,\n",
    "        t1.error as `CCD Publish Payload Error`\n",
    "\n",
    "        FROM ccd_publish_payload_result t1\n",
    "        full outer join ccd_call_result t2 on t2.CaseNo = t1.Filename and t1.state = t2.state)\n",
    "\n",
    "        SELECT\n",
    "        COALESCE(a.State, b.State) as State, \n",
    "        COALESCE(a.CaseNo, b.CaseNo) as CaseNo,\n",
    "        a.`Segmentation State`,\n",
    "        b.`Validation Status`,\n",
    "        d.`CCD Publish Payload Status`,\n",
    "        d.`CCD Call Status`,\n",
    "        d.`CCD Publish Payload Publishing Date Time`,\n",
    "        d.`CCD Publish Payload Error`,\n",
    "        d.`CCD Call Function App Start Date Time`,\n",
    "        d.`CCD Call Function App End Date Time`,\n",
    "        d.`CCD Case ID`,\n",
    "        d.`CCD Call Function App Error`\n",
    "        \n",
    "        FROM segmentation_state_cte a FULL OUTER JOIN valid_invalid_json_cte b on a.CaseNo = b.CaseNo\n",
    "                                      FULL OUTER JOIN pub_payload_call_result_cte d on a.CaseNo = d.CaseNo\n",
    "\n",
    "        -- WHERE (d.`CCD Call Function App Start Date Time` >= '2025-11-28 11:40:00' OR d.`CCD Call Function App Start Date Time` IS NULL)\n",
    "        -- and (d.`CCD Publish Payload Publishing Date Time` >= '2025-11-28 11:40:00' OR d.`CCD Publish Payload Publishing Date Time` IS NULL)\n",
    "\n",
    "        -- ORDER BY d.`CCD Publish Payload Publishing Date Time` DESC\n",
    "          \n",
    "          ''').display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0614573a-f693-44fb-bbe2-6abcd7883060",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Notebook completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [
    {
     "elements": [
      {
       "dashboardResultIndex": 0,
       "elementNUID": "07c0b8d9-554f-4828-aa14-acec9823bddb",
       "elementType": "command",
       "guid": "04f027ef-114b-45dc-9a56-e3e62eebb3ab",
       "options": null,
       "position": {
        "height": 13,
        "width": 24,
        "x": 0,
        "y": 14,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": 0,
       "elementNUID": "86c6ca78-8d30-4ebc-a762-4adb3a11fdc6",
       "elementType": "command",
       "guid": "06a416e4-707b-4bc4-b8d4-39453a39e79c",
       "options": null,
       "position": {
        "height": 14,
        "width": 24,
        "x": 0,
        "y": 0,
        "z": null
       },
       "resultIndex": null
      }
     ],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "460527ac-f6c6-44a4-bca1-ba672fcab2ae",
     "origId": 7260245976989881,
     "title": "Case Execution Tracking Dashboard",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7260245976989892,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Case_Execution_Tracking_Dashboard",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
