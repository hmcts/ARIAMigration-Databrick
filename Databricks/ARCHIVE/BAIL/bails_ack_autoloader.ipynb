{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca145479-3655-4bf2-bafa-8c2894258a3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ARM Acknowledgment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81ef4be3-3c60-450c-91c7-9d09d193aea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Autoloader set up**  \n",
    "This Notebook sets up an Autoloader job that runs on a manual trigger to collect ack messages from the ack eventhubs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ab6f686-9bdf-4e7c-8537-52a7097d6569",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, LongType\n",
    "from pyspark.sql.functions import col,from_json\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c41be175-1d89-4a6f-9135-82e7f162705e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ack_schema = StructType([\n",
    "    StructField(\"filename\", StringType(), True),\n",
    "    StructField(\"http_response\", IntegerType(),True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"http_message\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bf5e79f-62a2-48ec-a216-5c36b44a1a8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Set up configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4e82801-8912-4ce4-8444-98d5e636341d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/configs/config.json\")\n",
    "env = config.first()[\"env\"].strip().lower()\n",
    "lz_key = config.first()[\"lz_key\"].strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba776ce4-cc9b-4473-92d3-ad6e679a432b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "keyvault_name = f\"ingest{lz_key}-meta002-{env}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0c451d4-fa8a-448d-80bb-1e8e9c49cd5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Access the Service Principle secrets from keyvaults\n",
    "client_secret = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-SECRET')\n",
    "tenant_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-TENANT-ID')\n",
    "client_id = dbutils.secrets.get(scope=keyvault_name, key='SERVICE-PRINCIPLE-CLIENT-ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13a173df-b554-4f68-a341-23a44c35e2f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "EH_NAMESPACE = f\"ingest{lz_key}-integration-eventHubNamespace001-{env}\"\n",
    "EH_NAME = f\"evh-bl-ack-{lz_key}-uks-dlrm-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb341325-5340-4b6a-89d0-9aafdd47b1c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## set up the configuration to allow the autoloader to connect to the source system\n",
    "\n",
    "## Eventhub details\n",
    "connection_string = dbutils.secrets.get(keyvault_name, \"RootManageSharedAccessKey\")\n",
    "\n",
    "# Encrypt the connection string using the EventHubsUtils.encrypt method\n",
    "#encrypted_conn_str = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connection_string)\n",
    "\n",
    "\n",
    "# ehConf = {\n",
    "#     \"eventhubs.connectionString\": encrypted_conn_str,\n",
    "#     \"eventhubs.consumerGroup\": consumer_group,\n",
    "#     # \"eventhubs.startingPositions\": starting_positions_json\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# eventhubdf = spark.readStream.format(\"eventhubs\")\\\n",
    "#     .options(**ehConf)\\\n",
    "#         .load()\n",
    "\n",
    "# eventhubdf.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5db00bd6-0302-4a5b-b55c-fb2f2a0cb89c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "KAFKA_OPTIONS = {\n",
    "    \"kafka.bootstrap.servers\": f\"{EH_NAMESPACE}.servicebus.windows.net:9093\",\n",
    "    \"subscribe\": EH_NAME,\n",
    "    \"startingOffsets\": \"earliest\",\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"failOnDataLoss\": \"false\",\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.sasl.jaas.config\": f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{connection_string}\";'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9927a08-6a4a-40b6-b0dd-bb8e731dc0de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Service principal credentials\n",
    "# client_id = dbutils.secrets.get(\"ingest00-meta002-sbox\", \"SERVICE-PRINCIPLE-CLIENT-ID\")\n",
    "# client_secret = dbutils.secrets.get(\"ingest00-meta002-sbox\", \"SERVICE-PRINCIPLE-CLIENT-SECRET\")\n",
    "# tenant_id = dbutils.secrets.get(\"ingest00-meta002-sbox\", \"SERVICE-PRINCIPLE-TENANT-ID\")\n",
    "\n",
    "# Storage account names\n",
    "curated_storage = f\"ingest{lz_key}curated{env}\"\n",
    "checkpoint_storage = f\"ingest{lz_key}xcutting{env}\"\n",
    "\n",
    "# Spark config for curated storage (Delta table)\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{curated_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{curated_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{curated_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{curated_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{curated_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "# Spark config for checkpoint storage\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{checkpoint_storage}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{checkpoint_storage}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{checkpoint_storage}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{checkpoint_storage}.dfs.core.windows.net\", client_secret)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{checkpoint_storage}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87d93d13-e894-4a99-b37f-96de6adc989b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Container and path for storing Delta table (in curated storage account)\n",
    "data_path = f\"abfss://silver@ingest{lz_key}curated{env}.dfs.core.windows.net/ARIADM/ARM/AUDIT/BAILS/bl_ack_audit_table\"\n",
    "\n",
    "# Container and path for checkpoint (in xcuttings storage account)\n",
    "checkpoint_path = f\"abfss://db-ack-checkpoint@ingest{lz_key}xcutting{env}.dfs.core.windows.net/BAILS/ACK/ack\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cb50870-e84d-4d05-8d76-64d844e6a4c2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reconciliation figures"
    }
   },
   "outputs": [],
   "source": [
    "gold_data_path = f\"abfss://gold@ingest{lz_key}curated{env}.dfs.core.windows.net/ARIADM/ARM/BAILS/\"\n",
    "# HTML Count\n",
    "html_path = f\"{gold_data_path}HTML/*.html\"\n",
    "html_df = spark.read.format(\"binaryFile\").load(html_path)\n",
    "\n",
    "json_path = f\"{gold_data_path}JSON/*.json\"\n",
    "json_df = spark.read.format(\"binaryFile\").load(json_path)\n",
    "\n",
    "a360_path = f\"{gold_data_path}A360/*.a360\"\n",
    "a360_df = spark.read.format(\"binaryFile\").load(a360_path)\n",
    "\n",
    "\n",
    "expected_html = html_df.count()\n",
    "expected_json = json_df.count()\n",
    "expected_a360 = a360_df.count()\n",
    "\n",
    "print(f\"Expected HTML: {expected_html}\")\n",
    "print(f\"Expected JSON: {expected_json}\")\n",
    "print(f\"Expected A360: {expected_a360}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60c1ade3-6716-41e6-9f8f-ed97a60a88b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eventhubdf = spark.readStream.format(\"kafka\")\\\n",
    "    .options(**KAFKA_OPTIONS)\\\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4cb5a90-4074-47c1-8c93-bbe76876bc0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parsed_df = (\n",
    "    eventhubdf\n",
    "    # 'body' is binary, so we cast to string (assuming UTF-8)\n",
    "    .select(col(\"value\").cast(\"string\").alias(\"json_str\"))\n",
    "    .select(from_json(col(\"json_str\"), ack_schema).alias(\"json_obj\"))\n",
    "    .select(\"json_obj.*\")\n",
    ")\n",
    "\n",
    "\n",
    "# parsed_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c2a3647-c3c2-4748-8b7f-be677ef5bf58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_processed_counts():\n",
    "    df = spark.read.format(\"delta\").load(data_path)\n",
    "    \n",
    "    html_count = df.filter(col(\"filename\").endswith(\".html\")).select(\"filename\").distinct().count()\n",
    "    json_count = df.filter((col(\"filename\")).endswith(\".json\")).select(\"filename\").distinct().count()\n",
    "    a360_count = df.filter((col(\"filename\")).endswith(\".a360\")).select(\"filename\").distinct().count()\n",
    "\n",
    "    return html_count, json_count, a360_count\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ea47fa2-e4ee-4f46-921e-dc56b46f4b64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = parsed_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start(data_path)\n",
    "\n",
    "\n",
    "\n",
    "# wait 60 seconds before starting the read stream\n",
    "time.sleep(60)\n",
    "\n",
    "\n",
    "# query.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f549011b-a689-4517-9665-b604f7829acd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "max_attempt = 5\n",
    "delay = 60 # seconds\n",
    "\n",
    "for attempt in range(1,max_attempt):\n",
    "    try:\n",
    "        html_count, json_count, a360_count = get_processed_counts()\n",
    "    except Exception as e:\n",
    "        if attempt < max_attempt:\n",
    "            print(f\"Attempt {attempt} failed: {e}. Retrying in {delay} seconds... \")\n",
    "            time.sleep(60)\n",
    "        else:\n",
    "            print(\"Failed to get processed counts after {max_attempt} attempts: {e}\")\n",
    "            raise\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad84b62b-f3bd-4616-98a0-036413f1c65b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "while query.isActive:\n",
    "    html_count, json_count, a360_count = get_processed_counts()\n",
    "    print(f\"Status HTML: {html_count}/{expected_html} ---- Status JSON: {json_count}/{expected_json} ---- Status A360: {a360_count}/{expected_a360}\")\n",
    "\n",
    "    if (\n",
    "        html_count >= expected_html\n",
    "        and json_count >= expected_json\n",
    "        and a360_count >= expected_a360\n",
    "    ):\n",
    "        print(\"All files processed\")\n",
    "        query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21636e85-ef38-432a-a1f6-e26ce0ed9646",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(data_path)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1640307-70bd-4699-8108-d07195bd4954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"Notebook completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bails_ack_autoloader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
