trigger:
  branches:
    include:
      - master  

pool:
  vmImage: 'ubuntu-latest' 

variables:
  - name: env
    value: 'sandbox'
  - name: DATABRICKS_HOST_WORKSPACE_MAIN00
    value: 'https://adb-3635282203417052.12.azuredatabricks.net'
  - name: DATABRICKS_HOST_WORKSPACE_MAIN01
    value: 'https://adb-376876256300083.3.azuredatabricks.net/'
  - name: DATABRICKS_HOST_WORKSPACE_MAIN02
    value: 'https://adb-1879076228317698.18.azuredatabricks.net'

stages:
- stage: 'Build'
  displayName: 'Build Stage'
  jobs:
  - job: BuildJob
    displayName: 'Run Build and deploy'
    steps:
    - task: UsePythonVersion@0
      displayName: 'Use Python 3.x'
      inputs:
        versionSpec: '3.x'

    - script: |
        python -m pip install --upgrade pip
        pip install databricks-cli
      displayName: 'Install Databricks CLI'

    # Configure and deploy to Workspace MAIN00
    - script: |
        echo "[DEFAULT]" > $(HOME)/.databrickscfg
        echo "host = $(DATABRICKS_HOST_WORKSPACE_MAIN00)" >> $(HOME)/.databrickscfg
        echo "token = $(DATABRICKS_TOKEN_WORKSPACE_MAIN00)" >> $(HOME)/.databrickscfg
      displayName: 'Configure Databricks CLI for Workspace MAIN00'

    - script: |
        databricks workspace delete /live --recursive 
        databricks workspace mkdirs /live
        databricks workspace import_dir Databricks /live --overwrite
      displayName: 'Publish Databricks folder to /live in Databricks MAIN00 Workspace'

    # Configure and deploy to Workspace MAIN01
    - script: |
        echo "[DEFAULT]" > $(HOME)/.databrickscfg
        echo "host = $(DATABRICKS_HOST_WORKSPACE_MAIN01)" >> $(HOME)/.databrickscfg
        echo "token = $(DATABRICKS_TOKEN_WORKSPACE_MAIN01)" >> $(HOME)/.databrickscfg
      displayName: 'Configure Databricks CLI for Workspace MAIN01'

    - script: |
        databricks workspace delete /live --recursive
        databricks workspace mkdirs /live
        databricks workspace import_dir Databricks /live --overwrite
      displayName: 'Publish Databricks folder to /live in Databricks MAIN01 Workspace'

    # Configure and deploy to Workspace MAIN02
    - script: |
        echo "[DEFAULT]" > $(HOME)/.databrickscfg
        echo "host = $(DATABRICKS_HOST_WORKSPACE_MAIN02)" >> $(HOME)/.databrickscfg
        echo "token = $(DATABRICKS_TOKEN_WORKSPACE_MAIN02)" >> $(HOME)/.databrickscfg
      displayName: 'Configure Databricks CLI for Workspace MAIN02'

    - script: |
        databricks workspace delete /live --recursive
        databricks workspace mkdirs /live
        databricks workspace import_dir Databricks /live --overwrite
      displayName: 'Publish Databricks folder to /live in Databricks MAIN02 Workspace'


- stage: 'Deploy'
  displayName: 'Deploy Stage'
  jobs:
  - job: DeployJob
    displayName: 'Deploy Databricks Asset Bundles to $(env)-ingest02-databricks-product'
    steps:
    - task: UsePythonVersion@0
      displayName: 'Use Python 3.x'
      inputs:
        versionSpec: '3.x'

    - script: |
         /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
         eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"
         
         brew install databricks/tap/databricks
         databricks --version
      displayName: 'Install Databricks CLI'

      #Ensure databrcks CLI is available in current session (loading homebrew shell)
    - script: |
        eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)"  
        databricks --version  
        echo "[DEFAULT]" > $(HOME)/.databrickscfg
        echo "host = $(DATABRICKS_HOST_WORKSPACE_MAIN02)" >> $(HOME)/.databrickscfg
        echo "token = $(DATABRICKS_TOKEN_WORKSPACE_MAIN02)" >> $(HOME)/.databrickscfg
        cat ~/.databrickscfg  # Print the contents for debugging
      displayName: 'Configure Databricks CLI for Workspace MAIN02'
    
    - script: |
       eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)" 

        # List the files and subdirectories
        databricks workspace list /Users/andrew.mcdevitt@hmcts.net/.bundle

        # For each item in the directory, delete it
        for item in $(databricks workspace list /Users/andrew.mcdevitt@hmcts.net/.bundle); do
          # Check if it's a directory or a file, and delete accordingly
          if [[ "$item" == */ ]]; then
            # It's a directory, so we recursively delete it
            databricks workspace delete "/Users/andrew.mcdevitt@hmcts.net/.bundle/$item" --recursive
          else
            # It's a file, so delete it
            databricks workspace delete "/Users/andrew.mcdevitt@hmcts.net/.bundle/$item"
          fi
        done

        # Finally, delete the empty parent directory
        databricks workspace delete /Users/andrew.mcdevitt@hmcts.net/.bundle

      displayName: "Delete all files in bundle directory"

    - script: |
        databricks workspace delete /live --recursive
        echo "Current working directory: $(pwd)"
        ls -l /home/vsts/work/1/s
        eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)" 

        cd ARIA_DABs
        databricks bundle validate -t sandbox 
        databricks bundle deploy -t sandbox --debug

        echo "Deploy exit code: $DEPLOY_EXIT"
        echo "Checking .databricks directory for state file..."
        echo "Listing file in .databricks/bundle/sandbox/" ls -la .databricks/bundle/
        cat .databricks/bundle.state || echo "No bundle state file found"

#        if [ $? -eq 0 ]; then
#          echo "Deploy successful, waiting 10 seconds before running..."
#          sleep 10
#          databricks bundle run resources/pipelines/ARIA_ARCHIVE_BAILS.yml -t sandbox
#        else
#          echo "Deploy failed"
#          exit 1
#        fi
      displayName: "Deploy Databricks resources to ingest02 workspace"

#    - script: |
#       eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)" 
#       cd ARIA_DABs
#       databricks jobs create /resources/workflows/db_wf_apl_fta_send.yml
#       databricks jobs create /resources/workflows/db_wf_bail_receive.yml
#       databricks jobs create /resources/workflows/db_wf_bail_reprocessing.yml
#       databricks jobs create /resources/workflows/db_wf_bail_send.yml
#       databricks jobs create /resources/workflows/db_wf_joh_recieve.yml
#       databricks jobs create /resources/workflows/db_wf_joh_send.yml
#       databricks jobs create /resources/workflows/db_wf_td_receive.yml
#       databricks jobs create /resources/workflows/db_wf_td_send.yml
#      displayName: "Create Databricks workflows"

#    - script: |
#       eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)" 
#       cd ARIA_DABs
#       databricks pipelines create /resources/pipelines/ARIA_ARCHIVE_BAILS.yml
#       databricks pipelines create /resources/pipelines/PL_ARIADM_ARM_APPEALS.yml
#       databricks pipelines create /resources/pipelines/PL_ARIADM_ARM_FTA.yml
#       databricks pipelines create /resources/pipelines/PL_ARIADM_ARM_JOH.yml
#       databricks pipelines create /resources/pipelines/PL_ARIADM_ARM_TD.yml
#      displayName: "Create Databricks pipelines"
    
#    - script: |
#       eval "$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)" 
#       cd ARIA_DABs
#       databricks jobs list
#       databricks pipelines list
#      displayName: "Query Databricks Pipelines & Workflows"